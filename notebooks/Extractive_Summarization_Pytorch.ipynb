{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extractive Summarization - Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "324475a18c884452bb9af4cf073fe0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b5b6ae63794348998d456ebdd5923662",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2b7662cde49c444b8c9baea49f05cb2f",
              "IPY_MODEL_de939b5f8bdc42509f9cd28f8d7cfa22",
              "IPY_MODEL_b0209d7557474b8e8934ac9e10245b99"
            ]
          }
        },
        "b5b6ae63794348998d456ebdd5923662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b7662cde49c444b8c9baea49f05cb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0bb1476f15f94c658acf245d5086154d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66b9f73a59b84baca8f59312e0cf3ba4"
          }
        },
        "de939b5f8bdc42509f9cd28f8d7cfa22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3727ac6784834860ac5055447529cc64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2298,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2298,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72c9c044c28340a2b6c0f4771a28d61f"
          }
        },
        "b0209d7557474b8e8934ac9e10245b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d24e2f778bf64464920fe1252c493c6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2298/2298 [03:48&lt;00:00, 10.75it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1ddc0c15e314702bbc90e4aa8787fe4"
          }
        },
        "0bb1476f15f94c658acf245d5086154d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66b9f73a59b84baca8f59312e0cf3ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3727ac6784834860ac5055447529cc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72c9c044c28340a2b6c0f4771a28d61f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d24e2f778bf64464920fe1252c493c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1ddc0c15e314702bbc90e4aa8787fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f89e12102924e7c9cc37a79c5affd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4652aba4b50746f9a796f2c6019a1ab8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c3eb0d8e1f4c45d7929cc6f846d5ae7c",
              "IPY_MODEL_41565a27fc1c4c08b38a9820eea106d5",
              "IPY_MODEL_33d437eda9dd4d2a922f6a8533ac7124"
            ]
          }
        },
        "4652aba4b50746f9a796f2c6019a1ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3eb0d8e1f4c45d7929cc6f846d5ae7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6309829d05174115ab603854a118a507",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef437f44befb494e9a3184ddf84958e8"
          }
        },
        "41565a27fc1c4c08b38a9820eea106d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5ed14bfe75964dc590e5f61c45ec4143",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9192,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9192,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcf504dfb22c477b92728993fc10c5f2"
          }
        },
        "33d437eda9dd4d2a922f6a8533ac7124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a1aa6f22b02040439f4503b3fc13677e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9192/9192 [14:57&lt;00:00, 11.41it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3054ea4c25624837b61cf665f8eaa7b8"
          }
        },
        "6309829d05174115ab603854a118a507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef437f44befb494e9a3184ddf84958e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ed14bfe75964dc590e5f61c45ec4143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcf504dfb22c477b92728993fc10c5f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1aa6f22b02040439f4503b3fc13677e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3054ea4c25624837b61cf665f8eaa7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8411ce7430f445b68ce478ba9326ef5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_989a7f3721cd4337bf741ed991a474b7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fa8c4eb3aac4238babdfeee0e2b99c0",
              "IPY_MODEL_4a955f131937447c8e862c8d4891d85d",
              "IPY_MODEL_65595478b48d4a7fbc6734b91ee30300"
            ]
          }
        },
        "989a7f3721cd4337bf741ed991a474b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fa8c4eb3aac4238babdfeee0e2b99c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8246e3c9143b4ab18302eafa85ed62ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 69%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe2aea2667b945e38cb507567766236e"
          }
        },
        "4a955f131937447c8e862c8d4891d85d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7c6932829f824f58b7c0aee38d371b84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3770,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2592,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c95c31d824dc48c78fde6c88910809fc"
          }
        },
        "65595478b48d4a7fbc6734b91ee30300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e8eab30af014c5c94f86e1c236aa3ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2592/3770 [14:29:28&lt;6:32:31, 19.99s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a127d4e49fb84000aa551ed90e71eca6"
          }
        },
        "8246e3c9143b4ab18302eafa85ed62ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe2aea2667b945e38cb507567766236e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c6932829f824f58b7c0aee38d371b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c95c31d824dc48c78fde6c88910809fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e8eab30af014c5c94f86e1c236aa3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a127d4e49fb84000aa551ed90e71eca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-th8OKdY4mx",
        "outputId": "3ee30c64-d4dc-4dff-e082-1fb77881ff1c"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=252647ccbaa19c6514ffdae4d0018eb6066258971a6f5378a00344b424715e58\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-onbzgx18/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqq1Pf-fZAWL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXUYA-cBZm4Y"
      },
      "source": [
        "!pip install -q transformers  rouge-score sentence-transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvTweWCpAYB2"
      },
      "source": [
        "import tensorflow_datasets as tfds \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf\n",
        "from IPython.display import Image \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os \n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "sum_dir = \"drive/MyDrive/mlexperiments/extractivesummarization/\"\n",
        "cnn_df = pd.read_json(sum_dir + \"data/test/test.json\") "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs98qEJVDPs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g4ZwxMXDRkf"
      },
      "source": [
        "## Model Architecture \n",
        "\n",
        "- Get representations for documents and sentences\n",
        "- Train a classification head to predict if each sentence belongs in a summary. \n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPWB-9WgEWhR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT_WVAkmEYjZ"
      },
      "source": [
        "## Extracting Sentence Embeddigns via Mean Pooling\n",
        "\n",
        "- We use sentence embedding models hosted in HuggingFace and use mean pooling of each token sentence representation. \n",
        "\n",
        "Source: https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBtmCZP2Zl6W"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "#Load AutoModel from huggingface model repository\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
        "sentence_model = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "def get_sentence_embedding(sentences):\n",
        "    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "      model_output = sentence_model(**encoded_input)\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    return sentence_embeddings\n",
        "\n",
        "#Sentences we want sentence embeddings for\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "             'Sentences are passed as a list of string.',\n",
        "             'Twitter is rolling out changes to its newly rebuilt API that will allow third-party developers to build tools and other solutions specifically for its audio chatroom product, Twitter Spaces. The company today announced it’s shipping new endpoints to support Spaces on the Twitter API v2, with the initial focus on enabling discovery of live or scheduled Spaces. This may later be followed by an API update that will make it possible for developers to build out more tools for Spaces’ hosts.With the current API update, Twitter hopes developers will build new products that enable users — both on and off Twitter — to find Twitter Spaces more easily, the company says. This could potentially broaden the reach of Spaces and introduce its audio chats to more people, which could give Twitter a leg up in the increasingly competitive landscape for audio-based social networking. Today, Twitter Spaces isn’t only taking on Clubhouse, but also the audio chat experiences being offered by Facebook, Discord, Reddit, Public.com, Spotify and smaller social apps.']\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhqWeP7Gae9-",
        "outputId": "cfe22c2b-c3d9-4547-f727-f6e7238b43ab"
      },
      "source": [
        "%time\n",
        "semmb = get_sentence_embedding(sentences)\n",
        "semmb.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300, 384])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98iE04vl_iCh"
      },
      "source": [
        "## Create Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_7r8CoJA9xc"
      },
      "source": [
        "def get_dicts(df, folder=\"test\"):   \n",
        "  sents_dict = {}\n",
        "  doc_dict = { i: {\"article\": df.article[i], \"highlight\": df.highlights[i]} for i in df.index }\n",
        "  raw_docs = [ doc_dict[k][\"article\"] for k in doc_dict.keys()]\n",
        "\n",
        "  doc_sents = {}\n",
        "  sents_list = []\n",
        "  raw_sents = [] \n",
        "  i = 0\n",
        "  min_sent_length = 14\n",
        "  for k in tqdm(doc_dict.keys()):\n",
        "    article = doc_dict[k][\"article\"]  \n",
        "    sents = nlp(article).sents\n",
        "    doc_sent_ids = [] \n",
        "    for sent in sents:\n",
        "      if (len(sent)) > min_sent_length:\n",
        "        sents_dict[i] = {\"docid\":k, \"text\": str(sent)} \n",
        "        sents_list.append({\"sentid\":i, \"docid\":k, \"text\": str(sent) }) \n",
        "        raw_sents.append(str(sent))\n",
        "        i += 1  \n",
        "  return doc_dict, sents_list\n",
        "\n",
        "  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "324475a18c884452bb9af4cf073fe0ed",
            "b5b6ae63794348998d456ebdd5923662",
            "2b7662cde49c444b8c9baea49f05cb2f",
            "de939b5f8bdc42509f9cd28f8d7cfa22",
            "b0209d7557474b8e8934ac9e10245b99",
            "0bb1476f15f94c658acf245d5086154d",
            "66b9f73a59b84baca8f59312e0cf3ba4",
            "3727ac6784834860ac5055447529cc64",
            "72c9c044c28340a2b6c0f4771a28d61f",
            "d24e2f778bf64464920fe1252c493c6d",
            "e1ddc0c15e314702bbc90e4aa8787fe4",
            "6f89e12102924e7c9cc37a79c5affd2b",
            "4652aba4b50746f9a796f2c6019a1ab8",
            "c3eb0d8e1f4c45d7929cc6f846d5ae7c",
            "41565a27fc1c4c08b38a9820eea106d5",
            "33d437eda9dd4d2a922f6a8533ac7124",
            "6309829d05174115ab603854a118a507",
            "ef437f44befb494e9a3184ddf84958e8",
            "5ed14bfe75964dc590e5f61c45ec4143",
            "bcf504dfb22c477b92728993fc10c5f2",
            "a1aa6f22b02040439f4503b3fc13677e",
            "3054ea4c25624837b61cf665f8eaa7b8"
          ]
        },
        "id": "oNerfOEJBE0V",
        "outputId": "3e80988b-e38f-4d93-edab-c090b75201b8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(cnn_df, test_size=0.2, random_state=42)\n",
        "print(train_df.shape, test_df.shape)\n",
        "\n",
        "test_doc_dict, test_sents_list = get_dicts(test_df)\n",
        "train_doc_dict, train_sents_list = get_dicts(train_df)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9192, 2) (2298, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "324475a18c884452bb9af4cf073fe0ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2298 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f89e12102924e7c9cc37a79c5affd2b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/9192 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niQiYsTgCARC"
      },
      "source": [
        "from rouge_score import rouge_scorer \n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def get_rougue_score(text, highlights, metric=\"rougeL\"):\n",
        "  max_score = 0\n",
        "  for h_text in highlights:\n",
        "    score =  scorer.score(text, h_text)[metric].fmeasure\n",
        "    # print(score, text, \"\\n \\t\" , h_text)\n",
        "    if score > max_score:\n",
        "      max_score = score \n",
        "  return max_score \n",
        "\n",
        "\n",
        "def get_label(sent, doc_dict,  score_threshold = 0.55):\n",
        "  sent_id, doc_id, sentence = sent[\"sentid\"], sent[\"docid\"], sent[\"text\"]  \n",
        "  highlights = doc_dict[doc_id][\"highlight\"].split(\"\\n\")\n",
        "  doc = doc_dict[doc_id][\"article\"]\n",
        "\n",
        "  label_score = get_rougue_score(sentence, highlights) \n",
        "  # Normalize label to 0/1 based on rogue score threshold\n",
        "  label_score = 0 if label_score < score_threshold else 1 \n",
        "  return (sentence, doc, label_score)\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F5mn3dVDqli"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, doc_dict, sents_list):\n",
        "        'Initialization'\n",
        "        self.sents = sents_list\n",
        "        self.doc_dict = doc_dict \n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.sents)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        x1, x2, y = get_label( self.sents[index], self.doc_dict)\n",
        "\n",
        "        embs =  get_sentence_embedding([x1,x2])\n",
        "        x1,x2 = embs[0], embs[1]\n",
        "        X=(x1,x2)\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJOqPYGQEZAU"
      },
      "source": [
        "def get_batched_vals(sents_batch):\n",
        "    vals = [get_label(x, train_doc_dict)  for x in sents_batch] \n",
        "    \n",
        "    sents, docs, y = [], [], [] \n",
        "    for row in vals:\n",
        "      sents.append(row[0])\n",
        "      docs.append(row[1])\n",
        "      y.append(row[2])\n",
        "       \n",
        "    \n",
        "    return sents, docs, y"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6t-UKqexJk2"
      },
      "source": [
        "## Get Class Weights \n",
        "\n",
        "There are multiple approaches to handing class imbalance. \n",
        "\n",
        "- Undersample majority class\n",
        "- SMOTE \n",
        "- Weighted loss function. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWBXSLN4j9UM",
        "outputId": "741db947-2fd3-4317-aa13-bf5d14989ade"
      },
      "source": [
        "s,d, y = get_batched_vals(train_sents_list)\n",
        "\n",
        "train_labels_ohe = keras.utils.to_categorical(y)\n",
        "class_totals = train_labels_ohe.sum(axis=0)\n",
        "class_weight = dict()\n",
        "\n",
        "for i in range(0, len(class_totals)):\n",
        "\tclass_weight[i] = class_totals.max() / class_totals[i]\n",
        "class_weight"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0, 1: 25.604918}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSdGho86aorq"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features=384):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # classification head that accepts text features\n",
        "\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features*3, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Dropout(p=0.5),\n",
        "\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, doc_feats, sentence_feats):\n",
        "        \n",
        "        # Multiply (element-wise) the feature vectors of the two images together, \n",
        "        # to generate a combined feature vector representing the similarity between the two.\n",
        "        combined_features = doc_feats * sentence_feats  \n",
        "\n",
        "        # get concat of both features and elementwise product\n",
        "        x = torch.cat((doc_feats, sentence_feats, combined_features), dim=1) \n",
        "\n",
        "        output = self.cls_head(x)\n",
        "\n",
        "        # print(output.shape)\n",
        "        return output"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt1d41VbUWg_"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8411ce7430f445b68ce478ba9326ef5c",
            "989a7f3721cd4337bf741ed991a474b7",
            "0fa8c4eb3aac4238babdfeee0e2b99c0",
            "4a955f131937447c8e862c8d4891d85d",
            "65595478b48d4a7fbc6734b91ee30300",
            "8246e3c9143b4ab18302eafa85ed62ed",
            "fe2aea2667b945e38cb507567766236e",
            "7c6932829f824f58b7c0aee38d371b84",
            "c95c31d824dc48c78fde6c88910809fc",
            "6e8eab30af014c5c94f86e1c236aa3ca",
            "a127d4e49fb84000aa551ed90e71eca6"
          ]
        },
        "id": "qmmL6WiBgi0Q",
        "outputId": "7514d212-11d8-4fef-a3ad-81026b38ddcc"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "model = Net() \n",
        "\n",
        "losses = []\n",
        "correct = 0\n",
        "total = 0 \n",
        "\n",
        "\n",
        "# Set device to CUDA if a CUDA device is available, else CPU. Copy model to selected device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(\"Cuda is available?\", torch.cuda.is_available())\n",
        "learning_rate = 1e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# criterion = nn.CrossEntropyLoss() # \n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "writer = SummaryWriter(os.path.join(\"\", \"summary\"))\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 2\n",
        "n_batches = int( len(train_sents_list) / batch_size)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(\"Epoch ... [{} / {}]\".format(epoch, n_epochs))\n",
        "  model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for i in tqdm(range(n_batches)):\n",
        "    # sentence, document, y = get_label(sent, train_doc_dict)\n",
        "    batch_start = i*batch_size\n",
        "    batch_end = (i+1)*batch_size\n",
        "\n",
        "    sent_batch = train_sents_list[batch_start:batch_end]\n",
        "    sentences, docs, y = get_batched_vals(sent_batch)\n",
        "\n",
        "    loss_weights = [ class_weight[label] for label in y] \n",
        "    # print(y, \"\\n\", loss_weights)\n",
        "    loss_weights = torch.FloatTensor(loss_weights)\n",
        "\n",
        "    # get embeddings for docss and sentences \n",
        "    sentences = get_sentence_embedding(sentences)\n",
        "    docs = get_sentence_embedding(docs)\n",
        "    y = torch.reshape(torch.FloatTensor(y), (-1,1))\n",
        "    # y =  torch.LongTensor(y)\n",
        "   \n",
        "\n",
        "    sentences, docs, y, loss_weights = map(lambda x: x.to(device), [sentences, docs, y, loss_weights])\n",
        "\n",
        "    # get model prediction\n",
        "    prob = model(sentences, docs) \n",
        "    loss = criterion(prob, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    correct += torch.count_nonzero(y == (prob > 0.5)).item()\n",
        "    total += len(y)\n",
        "    print(\"Batch | \", str(i), \"of\", str(n_batches), \"loss -> \", loss.item() , \"acc ->\", round(correct/total,4) )\n",
        "\n",
        "  writer.add_scalar('train_loss', sum(losses)/len(losses), epoch)\n",
        "  writer.add_scalar('train_acc', correct / total, epoch)\n",
        "\n",
        "  print(\"\\tTraining: Loss={:.2f}\\t Accuracy={:.2f}\\t\".format(sum(losses)/len(losses), correct / total))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda is available? True\n",
            "Epoch ... [0 / 2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8411ce7430f445b68ce478ba9326ef5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3770 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch |  0 of 3770 loss ->  0.6237705945968628 acc -> 0.67\n",
            "Batch |  1 of 3770 loss ->  0.6322818994522095 acc -> 0.68\n",
            "Batch |  2 of 3770 loss ->  0.6256148219108582 acc -> 0.67\n",
            "Batch |  3 of 3770 loss ->  0.6349412202835083 acc -> 0.67\n",
            "Batch |  4 of 3770 loss ->  0.6356877088546753 acc -> 0.67\n",
            "Batch |  5 of 3770 loss ->  0.6163499355316162 acc -> 0.67\n",
            "Batch |  6 of 3770 loss ->  0.6182960271835327 acc -> 0.68\n",
            "Batch |  7 of 3770 loss ->  0.588424801826477 acc -> 0.69\n",
            "Batch |  8 of 3770 loss ->  0.6172498464584351 acc -> 0.69\n",
            "Batch |  9 of 3770 loss ->  0.5891910195350647 acc -> 0.69\n",
            "Batch |  10 of 3770 loss ->  0.6133273839950562 acc -> 0.7\n",
            "Batch |  11 of 3770 loss ->  0.6018751859664917 acc -> 0.7\n",
            "Batch |  12 of 3770 loss ->  0.5888773202896118 acc -> 0.71\n",
            "Batch |  13 of 3770 loss ->  0.5913211107254028 acc -> 0.71\n",
            "Batch |  14 of 3770 loss ->  0.6169883012771606 acc -> 0.71\n",
            "Batch |  15 of 3770 loss ->  0.6284613013267517 acc -> 0.71\n",
            "Batch |  16 of 3770 loss ->  0.6180523633956909 acc -> 0.71\n",
            "Batch |  17 of 3770 loss ->  0.6306808590888977 acc -> 0.7\n",
            "Batch |  18 of 3770 loss ->  0.5935579538345337 acc -> 0.7\n",
            "Batch |  19 of 3770 loss ->  0.6052165031433105 acc -> 0.7\n",
            "Batch |  20 of 3770 loss ->  0.6241340637207031 acc -> 0.7\n",
            "Batch |  21 of 3770 loss ->  0.6357587575912476 acc -> 0.7\n",
            "Batch |  22 of 3770 loss ->  0.6186088919639587 acc -> 0.7\n",
            "Batch |  23 of 3770 loss ->  0.632209062576294 acc -> 0.7\n",
            "Batch |  24 of 3770 loss ->  0.6544709205627441 acc -> 0.7\n",
            "Batch |  25 of 3770 loss ->  0.6343811750411987 acc -> 0.7\n",
            "Batch |  26 of 3770 loss ->  0.595165491104126 acc -> 0.7\n",
            "Batch |  27 of 3770 loss ->  0.6057896614074707 acc -> 0.7\n",
            "Batch |  28 of 3770 loss ->  0.616364598274231 acc -> 0.7\n",
            "Batch |  29 of 3770 loss ->  0.6295220851898193 acc -> 0.7\n",
            "Batch |  30 of 3770 loss ->  0.6235694885253906 acc -> 0.7\n",
            "Batch |  31 of 3770 loss ->  0.5960661172866821 acc -> 0.7\n",
            "Batch |  32 of 3770 loss ->  0.6141449809074402 acc -> 0.7\n",
            "Batch |  33 of 3770 loss ->  0.6012672185897827 acc -> 0.7\n",
            "Batch |  34 of 3770 loss ->  0.5784604549407959 acc -> 0.7\n",
            "Batch |  35 of 3770 loss ->  0.6007465124130249 acc -> 0.7\n",
            "Batch |  36 of 3770 loss ->  0.5884743928909302 acc -> 0.7\n",
            "Batch |  37 of 3770 loss ->  0.592530369758606 acc -> 0.71\n",
            "Batch |  38 of 3770 loss ->  0.6185218095779419 acc -> 0.71\n",
            "Batch |  39 of 3770 loss ->  0.6242023706436157 acc -> 0.71\n",
            "Batch |  40 of 3770 loss ->  0.570026159286499 acc -> 0.71\n",
            "Batch |  41 of 3770 loss ->  0.5862938165664673 acc -> 0.71\n",
            "Batch |  42 of 3770 loss ->  0.5834593772888184 acc -> 0.71\n",
            "Batch |  43 of 3770 loss ->  0.5998823642730713 acc -> 0.71\n",
            "Batch |  44 of 3770 loss ->  0.623712956905365 acc -> 0.71\n",
            "Batch |  45 of 3770 loss ->  0.6139302849769592 acc -> 0.71\n",
            "Batch |  46 of 3770 loss ->  0.584467887878418 acc -> 0.71\n",
            "Batch |  47 of 3770 loss ->  0.5814372301101685 acc -> 0.71\n",
            "Batch |  48 of 3770 loss ->  0.6001836061477661 acc -> 0.71\n",
            "Batch |  49 of 3770 loss ->  0.6191083192825317 acc -> 0.71\n",
            "Batch |  50 of 3770 loss ->  0.6013706922531128 acc -> 0.71\n",
            "Batch |  51 of 3770 loss ->  0.6258844137191772 acc -> 0.71\n",
            "Batch |  52 of 3770 loss ->  0.6148186922073364 acc -> 0.71\n",
            "Batch |  53 of 3770 loss ->  0.5982294082641602 acc -> 0.71\n",
            "Batch |  54 of 3770 loss ->  0.6200652122497559 acc -> 0.71\n",
            "Batch |  55 of 3770 loss ->  0.6080868244171143 acc -> 0.71\n",
            "Batch |  56 of 3770 loss ->  0.6021865606307983 acc -> 0.71\n",
            "Batch |  57 of 3770 loss ->  0.6306778192520142 acc -> 0.71\n",
            "Batch |  58 of 3770 loss ->  0.5844624638557434 acc -> 0.71\n",
            "Batch |  59 of 3770 loss ->  0.5861009359359741 acc -> 0.71\n",
            "Batch |  60 of 3770 loss ->  0.5984234809875488 acc -> 0.71\n",
            "Batch |  61 of 3770 loss ->  0.6023040413856506 acc -> 0.72\n",
            "Batch |  62 of 3770 loss ->  0.6040497422218323 acc -> 0.72\n",
            "Batch |  63 of 3770 loss ->  0.6003727912902832 acc -> 0.71\n",
            "Batch |  64 of 3770 loss ->  0.6015461683273315 acc -> 0.71\n",
            "Batch |  65 of 3770 loss ->  0.6182252764701843 acc -> 0.71\n",
            "Batch |  66 of 3770 loss ->  0.6303972601890564 acc -> 0.71\n",
            "Batch |  67 of 3770 loss ->  0.635630190372467 acc -> 0.71\n",
            "Batch |  68 of 3770 loss ->  0.6142078638076782 acc -> 0.71\n",
            "Batch |  69 of 3770 loss ->  0.6409595012664795 acc -> 0.71\n",
            "Batch |  70 of 3770 loss ->  0.5997365117073059 acc -> 0.71\n",
            "Batch |  71 of 3770 loss ->  0.6261922121047974 acc -> 0.71\n",
            "Batch |  72 of 3770 loss ->  0.5977510213851929 acc -> 0.71\n",
            "Batch |  73 of 3770 loss ->  0.6082525253295898 acc -> 0.71\n",
            "Batch |  74 of 3770 loss ->  0.5703331828117371 acc -> 0.71\n",
            "Batch |  75 of 3770 loss ->  0.6184364557266235 acc -> 0.71\n",
            "Batch |  76 of 3770 loss ->  0.5980380773544312 acc -> 0.71\n",
            "Batch |  77 of 3770 loss ->  0.619451642036438 acc -> 0.71\n",
            "Batch |  78 of 3770 loss ->  0.5938931107521057 acc -> 0.71\n",
            "Batch |  79 of 3770 loss ->  0.6113348603248596 acc -> 0.71\n",
            "Batch |  80 of 3770 loss ->  0.5843931436538696 acc -> 0.71\n",
            "Batch |  81 of 3770 loss ->  0.6365329027175903 acc -> 0.71\n",
            "Batch |  82 of 3770 loss ->  0.6507682800292969 acc -> 0.71\n",
            "Batch |  83 of 3770 loss ->  0.5819858312606812 acc -> 0.71\n",
            "Batch |  84 of 3770 loss ->  0.628921627998352 acc -> 0.71\n",
            "Batch |  85 of 3770 loss ->  0.5785382986068726 acc -> 0.71\n",
            "Batch |  86 of 3770 loss ->  0.5798437595367432 acc -> 0.71\n",
            "Batch |  87 of 3770 loss ->  0.6046249866485596 acc -> 0.71\n",
            "Batch |  88 of 3770 loss ->  0.5434132814407349 acc -> 0.71\n",
            "Batch |  89 of 3770 loss ->  0.5980021953582764 acc -> 0.71\n",
            "Batch |  90 of 3770 loss ->  0.5822811126708984 acc -> 0.72\n",
            "Batch |  91 of 3770 loss ->  0.5852059125900269 acc -> 0.72\n",
            "Batch |  92 of 3770 loss ->  0.5934261083602905 acc -> 0.72\n",
            "Batch |  93 of 3770 loss ->  0.6282784342765808 acc -> 0.72\n",
            "Batch |  94 of 3770 loss ->  0.6446816921234131 acc -> 0.72\n",
            "Batch |  95 of 3770 loss ->  0.6153878569602966 acc -> 0.72\n",
            "Batch |  96 of 3770 loss ->  0.5938218832015991 acc -> 0.72\n",
            "Batch |  97 of 3770 loss ->  0.6133551597595215 acc -> 0.71\n",
            "Batch |  98 of 3770 loss ->  0.6256726980209351 acc -> 0.71\n",
            "Batch |  99 of 3770 loss ->  0.6034416556358337 acc -> 0.71\n",
            "Batch |  100 of 3770 loss ->  0.6082157492637634 acc -> 0.71\n",
            "Batch |  101 of 3770 loss ->  0.5548073053359985 acc -> 0.72\n",
            "Batch |  102 of 3770 loss ->  0.6136786937713623 acc -> 0.72\n",
            "Batch |  103 of 3770 loss ->  0.587854266166687 acc -> 0.72\n",
            "Batch |  104 of 3770 loss ->  0.5886087417602539 acc -> 0.72\n",
            "Batch |  105 of 3770 loss ->  0.5900124311447144 acc -> 0.72\n",
            "Batch |  106 of 3770 loss ->  0.6247358322143555 acc -> 0.72\n",
            "Batch |  107 of 3770 loss ->  0.5670700073242188 acc -> 0.72\n",
            "Batch |  108 of 3770 loss ->  0.6150416135787964 acc -> 0.72\n",
            "Batch |  109 of 3770 loss ->  0.5785531997680664 acc -> 0.72\n",
            "Batch |  110 of 3770 loss ->  0.570576548576355 acc -> 0.72\n",
            "Batch |  111 of 3770 loss ->  0.6173376441001892 acc -> 0.72\n",
            "Batch |  112 of 3770 loss ->  0.6344156861305237 acc -> 0.72\n",
            "Batch |  113 of 3770 loss ->  0.6348346471786499 acc -> 0.72\n",
            "Batch |  114 of 3770 loss ->  0.5772889256477356 acc -> 0.72\n",
            "Batch |  115 of 3770 loss ->  0.6410897374153137 acc -> 0.72\n",
            "Batch |  116 of 3770 loss ->  0.5837357640266418 acc -> 0.72\n",
            "Batch |  117 of 3770 loss ->  0.5964584946632385 acc -> 0.72\n",
            "Batch |  118 of 3770 loss ->  0.5963853001594543 acc -> 0.72\n",
            "Batch |  119 of 3770 loss ->  0.6030741930007935 acc -> 0.72\n",
            "Batch |  120 of 3770 loss ->  0.6278870701789856 acc -> 0.72\n",
            "Batch |  121 of 3770 loss ->  0.5769602060317993 acc -> 0.72\n",
            "Batch |  122 of 3770 loss ->  0.6799788475036621 acc -> 0.72\n",
            "Batch |  123 of 3770 loss ->  0.5871434211730957 acc -> 0.72\n",
            "Batch |  124 of 3770 loss ->  0.6168800592422485 acc -> 0.72\n",
            "Batch |  125 of 3770 loss ->  0.579267144203186 acc -> 0.72\n",
            "Batch |  126 of 3770 loss ->  0.6221152544021606 acc -> 0.72\n",
            "Batch |  127 of 3770 loss ->  0.6171168088912964 acc -> 0.72\n",
            "Batch |  128 of 3770 loss ->  0.6227062940597534 acc -> 0.72\n",
            "Batch |  129 of 3770 loss ->  0.604358971118927 acc -> 0.72\n",
            "Batch |  130 of 3770 loss ->  0.6112985014915466 acc -> 0.72\n",
            "Batch |  131 of 3770 loss ->  0.553555965423584 acc -> 0.72\n",
            "Batch |  132 of 3770 loss ->  0.629150390625 acc -> 0.72\n",
            "Batch |  133 of 3770 loss ->  0.5863209962844849 acc -> 0.72\n",
            "Batch |  134 of 3770 loss ->  0.5873942971229553 acc -> 0.72\n",
            "Batch |  135 of 3770 loss ->  0.5947765111923218 acc -> 0.72\n",
            "Batch |  136 of 3770 loss ->  0.5795632004737854 acc -> 0.72\n",
            "Batch |  137 of 3770 loss ->  0.589118480682373 acc -> 0.72\n",
            "Batch |  138 of 3770 loss ->  0.5769104957580566 acc -> 0.72\n",
            "Batch |  139 of 3770 loss ->  0.617956280708313 acc -> 0.72\n",
            "Batch |  140 of 3770 loss ->  0.5816034078598022 acc -> 0.72\n",
            "Batch |  141 of 3770 loss ->  0.599111795425415 acc -> 0.72\n",
            "Batch |  142 of 3770 loss ->  0.6092870831489563 acc -> 0.72\n",
            "Batch |  143 of 3770 loss ->  0.603040337562561 acc -> 0.72\n",
            "Batch |  144 of 3770 loss ->  0.5429005026817322 acc -> 0.72\n",
            "Batch |  145 of 3770 loss ->  0.5905089974403381 acc -> 0.72\n",
            "Batch |  146 of 3770 loss ->  0.5757976174354553 acc -> 0.72\n",
            "Batch |  147 of 3770 loss ->  0.6141464710235596 acc -> 0.72\n",
            "Batch |  148 of 3770 loss ->  0.5954705476760864 acc -> 0.72\n",
            "Batch |  149 of 3770 loss ->  0.6044367551803589 acc -> 0.72\n",
            "Batch |  150 of 3770 loss ->  0.5994262099266052 acc -> 0.72\n",
            "Batch |  151 of 3770 loss ->  0.610612154006958 acc -> 0.72\n",
            "Batch |  152 of 3770 loss ->  0.6337440013885498 acc -> 0.72\n",
            "Batch |  153 of 3770 loss ->  0.5966833829879761 acc -> 0.72\n",
            "Batch |  154 of 3770 loss ->  0.605404257774353 acc -> 0.72\n",
            "Batch |  155 of 3770 loss ->  0.5879720449447632 acc -> 0.72\n",
            "Batch |  156 of 3770 loss ->  0.6007342338562012 acc -> 0.72\n",
            "Batch |  157 of 3770 loss ->  0.5670995712280273 acc -> 0.72\n",
            "Batch |  158 of 3770 loss ->  0.5932067632675171 acc -> 0.73\n",
            "Batch |  159 of 3770 loss ->  0.5895469188690186 acc -> 0.73\n",
            "Batch |  160 of 3770 loss ->  0.6175292134284973 acc -> 0.73\n",
            "Batch |  161 of 3770 loss ->  0.6112729907035828 acc -> 0.72\n",
            "Batch |  162 of 3770 loss ->  0.5932291746139526 acc -> 0.72\n",
            "Batch |  163 of 3770 loss ->  0.6244261264801025 acc -> 0.72\n",
            "Batch |  164 of 3770 loss ->  0.6050205230712891 acc -> 0.72\n",
            "Batch |  165 of 3770 loss ->  0.6114165186882019 acc -> 0.72\n",
            "Batch |  166 of 3770 loss ->  0.6143757700920105 acc -> 0.72\n",
            "Batch |  167 of 3770 loss ->  0.5599976181983948 acc -> 0.73\n",
            "Batch |  168 of 3770 loss ->  0.5917253494262695 acc -> 0.73\n",
            "Batch |  169 of 3770 loss ->  0.590472400188446 acc -> 0.73\n",
            "Batch |  170 of 3770 loss ->  0.5676344633102417 acc -> 0.73\n",
            "Batch |  171 of 3770 loss ->  0.6066001653671265 acc -> 0.73\n",
            "Batch |  172 of 3770 loss ->  0.6038669943809509 acc -> 0.73\n",
            "Batch |  173 of 3770 loss ->  0.5967703461647034 acc -> 0.73\n",
            "Batch |  174 of 3770 loss ->  0.586911678314209 acc -> 0.73\n",
            "Batch |  175 of 3770 loss ->  0.6124684810638428 acc -> 0.73\n",
            "Batch |  176 of 3770 loss ->  0.5624659061431885 acc -> 0.73\n",
            "Batch |  177 of 3770 loss ->  0.6372678279876709 acc -> 0.73\n",
            "Batch |  178 of 3770 loss ->  0.5967305302619934 acc -> 0.73\n",
            "Batch |  179 of 3770 loss ->  0.5879817008972168 acc -> 0.73\n",
            "Batch |  180 of 3770 loss ->  0.6322948932647705 acc -> 0.73\n",
            "Batch |  181 of 3770 loss ->  0.6139625310897827 acc -> 0.73\n",
            "Batch |  182 of 3770 loss ->  0.5774732232093811 acc -> 0.73\n",
            "Batch |  183 of 3770 loss ->  0.5607531666755676 acc -> 0.73\n",
            "Batch |  184 of 3770 loss ->  0.5890339612960815 acc -> 0.73\n",
            "Batch |  185 of 3770 loss ->  0.608096182346344 acc -> 0.73\n",
            "Batch |  186 of 3770 loss ->  0.5897653698921204 acc -> 0.73\n",
            "Batch |  187 of 3770 loss ->  0.618638277053833 acc -> 0.73\n",
            "Batch |  188 of 3770 loss ->  0.5667051076889038 acc -> 0.73\n",
            "Batch |  189 of 3770 loss ->  0.6068273782730103 acc -> 0.73\n",
            "Batch |  190 of 3770 loss ->  0.5853635668754578 acc -> 0.73\n",
            "Batch |  191 of 3770 loss ->  0.5579982995986938 acc -> 0.73\n",
            "Batch |  192 of 3770 loss ->  0.5512988567352295 acc -> 0.73\n",
            "Batch |  193 of 3770 loss ->  0.6041194200515747 acc -> 0.73\n",
            "Batch |  194 of 3770 loss ->  0.5880666971206665 acc -> 0.73\n",
            "Batch |  195 of 3770 loss ->  0.5579752922058105 acc -> 0.73\n",
            "Batch |  196 of 3770 loss ->  0.5819491147994995 acc -> 0.73\n",
            "Batch |  197 of 3770 loss ->  0.6355072855949402 acc -> 0.73\n",
            "Batch |  198 of 3770 loss ->  0.5709526538848877 acc -> 0.73\n",
            "Batch |  199 of 3770 loss ->  0.5633158087730408 acc -> 0.73\n",
            "Batch |  200 of 3770 loss ->  0.5599802732467651 acc -> 0.73\n",
            "Batch |  201 of 3770 loss ->  0.5745495557785034 acc -> 0.73\n",
            "Batch |  202 of 3770 loss ->  0.5784209370613098 acc -> 0.73\n",
            "Batch |  203 of 3770 loss ->  0.5942256450653076 acc -> 0.73\n",
            "Batch |  204 of 3770 loss ->  0.5589814186096191 acc -> 0.73\n",
            "Batch |  205 of 3770 loss ->  0.5841031074523926 acc -> 0.73\n",
            "Batch |  206 of 3770 loss ->  0.5564943552017212 acc -> 0.73\n",
            "Batch |  207 of 3770 loss ->  0.5749808549880981 acc -> 0.73\n",
            "Batch |  208 of 3770 loss ->  0.60164874792099 acc -> 0.73\n",
            "Batch |  209 of 3770 loss ->  0.6025128364562988 acc -> 0.73\n",
            "Batch |  210 of 3770 loss ->  0.6090772151947021 acc -> 0.73\n",
            "Batch |  211 of 3770 loss ->  0.5422255992889404 acc -> 0.73\n",
            "Batch |  212 of 3770 loss ->  0.5658597946166992 acc -> 0.73\n",
            "Batch |  213 of 3770 loss ->  0.5939658284187317 acc -> 0.73\n",
            "Batch |  214 of 3770 loss ->  0.5969945192337036 acc -> 0.73\n",
            "Batch |  215 of 3770 loss ->  0.6069885492324829 acc -> 0.73\n",
            "Batch |  216 of 3770 loss ->  0.6035590767860413 acc -> 0.73\n",
            "Batch |  217 of 3770 loss ->  0.5600968599319458 acc -> 0.73\n",
            "Batch |  218 of 3770 loss ->  0.5745824575424194 acc -> 0.73\n",
            "Batch |  219 of 3770 loss ->  0.6216099858283997 acc -> 0.73\n",
            "Batch |  220 of 3770 loss ->  0.6124027967453003 acc -> 0.73\n",
            "Batch |  221 of 3770 loss ->  0.5912396907806396 acc -> 0.73\n",
            "Batch |  222 of 3770 loss ->  0.5784897804260254 acc -> 0.73\n",
            "Batch |  223 of 3770 loss ->  0.5772728323936462 acc -> 0.73\n",
            "Batch |  224 of 3770 loss ->  0.5912891030311584 acc -> 0.73\n",
            "Batch |  225 of 3770 loss ->  0.572350263595581 acc -> 0.73\n",
            "Batch |  226 of 3770 loss ->  0.5987472534179688 acc -> 0.73\n",
            "Batch |  227 of 3770 loss ->  0.5772750973701477 acc -> 0.74\n",
            "Batch |  228 of 3770 loss ->  0.5798816680908203 acc -> 0.74\n",
            "Batch |  229 of 3770 loss ->  0.5916850566864014 acc -> 0.74\n",
            "Batch |  230 of 3770 loss ->  0.5664962530136108 acc -> 0.74\n",
            "Batch |  231 of 3770 loss ->  0.5943313837051392 acc -> 0.74\n",
            "Batch |  232 of 3770 loss ->  0.582356870174408 acc -> 0.74\n",
            "Batch |  233 of 3770 loss ->  0.547902524471283 acc -> 0.74\n",
            "Batch |  234 of 3770 loss ->  0.586853563785553 acc -> 0.74\n",
            "Batch |  235 of 3770 loss ->  0.5654939413070679 acc -> 0.74\n",
            "Batch |  236 of 3770 loss ->  0.6190219521522522 acc -> 0.74\n",
            "Batch |  237 of 3770 loss ->  0.5939531326293945 acc -> 0.74\n",
            "Batch |  238 of 3770 loss ->  0.5759936571121216 acc -> 0.74\n",
            "Batch |  239 of 3770 loss ->  0.6165789365768433 acc -> 0.74\n",
            "Batch |  240 of 3770 loss ->  0.5866422057151794 acc -> 0.74\n",
            "Batch |  241 of 3770 loss ->  0.5613047480583191 acc -> 0.74\n",
            "Batch |  242 of 3770 loss ->  0.6058858036994934 acc -> 0.74\n",
            "Batch |  243 of 3770 loss ->  0.5881630182266235 acc -> 0.74\n",
            "Batch |  244 of 3770 loss ->  0.5446677207946777 acc -> 0.74\n",
            "Batch |  245 of 3770 loss ->  0.5899816751480103 acc -> 0.74\n",
            "Batch |  246 of 3770 loss ->  0.6532479524612427 acc -> 0.74\n",
            "Batch |  247 of 3770 loss ->  0.5815553665161133 acc -> 0.74\n",
            "Batch |  248 of 3770 loss ->  0.5607777833938599 acc -> 0.74\n",
            "Batch |  249 of 3770 loss ->  0.5798785090446472 acc -> 0.74\n",
            "Batch |  250 of 3770 loss ->  0.5875599384307861 acc -> 0.74\n",
            "Batch |  251 of 3770 loss ->  0.6033620834350586 acc -> 0.74\n",
            "Batch |  252 of 3770 loss ->  0.5597707033157349 acc -> 0.74\n",
            "Batch |  253 of 3770 loss ->  0.5358785390853882 acc -> 0.74\n",
            "Batch |  254 of 3770 loss ->  0.6140431761741638 acc -> 0.74\n",
            "Batch |  255 of 3770 loss ->  0.6025247573852539 acc -> 0.74\n",
            "Batch |  256 of 3770 loss ->  0.5826200246810913 acc -> 0.74\n",
            "Batch |  257 of 3770 loss ->  0.5797286033630371 acc -> 0.74\n",
            "Batch |  258 of 3770 loss ->  0.5874313116073608 acc -> 0.74\n",
            "Batch |  259 of 3770 loss ->  0.5651317834854126 acc -> 0.74\n",
            "Batch |  260 of 3770 loss ->  0.5926234722137451 acc -> 0.74\n",
            "Batch |  261 of 3770 loss ->  0.5697788000106812 acc -> 0.74\n",
            "Batch |  262 of 3770 loss ->  0.5806509256362915 acc -> 0.74\n",
            "Batch |  263 of 3770 loss ->  0.5720011591911316 acc -> 0.74\n",
            "Batch |  264 of 3770 loss ->  0.6039282083511353 acc -> 0.74\n",
            "Batch |  265 of 3770 loss ->  0.5815579891204834 acc -> 0.74\n",
            "Batch |  266 of 3770 loss ->  0.5757244825363159 acc -> 0.74\n",
            "Batch |  267 of 3770 loss ->  0.5808199644088745 acc -> 0.74\n",
            "Batch |  268 of 3770 loss ->  0.562175452709198 acc -> 0.74\n",
            "Batch |  269 of 3770 loss ->  0.5798494815826416 acc -> 0.74\n",
            "Batch |  270 of 3770 loss ->  0.5961149334907532 acc -> 0.74\n",
            "Batch |  271 of 3770 loss ->  0.5666753053665161 acc -> 0.74\n",
            "Batch |  272 of 3770 loss ->  0.5792093873023987 acc -> 0.74\n",
            "Batch |  273 of 3770 loss ->  0.5603299140930176 acc -> 0.74\n",
            "Batch |  274 of 3770 loss ->  0.5690010786056519 acc -> 0.74\n",
            "Batch |  275 of 3770 loss ->  0.5992974042892456 acc -> 0.74\n",
            "Batch |  276 of 3770 loss ->  0.6044396162033081 acc -> 0.74\n",
            "Batch |  277 of 3770 loss ->  0.6119838953018188 acc -> 0.74\n",
            "Batch |  278 of 3770 loss ->  0.6312934756278992 acc -> 0.74\n",
            "Batch |  279 of 3770 loss ->  0.6079990267753601 acc -> 0.74\n",
            "Batch |  280 of 3770 loss ->  0.5651636123657227 acc -> 0.74\n",
            "Batch |  281 of 3770 loss ->  0.5830920338630676 acc -> 0.74\n",
            "Batch |  282 of 3770 loss ->  0.5342816710472107 acc -> 0.74\n",
            "Batch |  283 of 3770 loss ->  0.568407416343689 acc -> 0.74\n",
            "Batch |  284 of 3770 loss ->  0.6131162643432617 acc -> 0.74\n",
            "Batch |  285 of 3770 loss ->  0.6246523261070251 acc -> 0.74\n",
            "Batch |  286 of 3770 loss ->  0.5885865092277527 acc -> 0.74\n",
            "Batch |  287 of 3770 loss ->  0.5795988440513611 acc -> 0.74\n",
            "Batch |  288 of 3770 loss ->  0.5797469615936279 acc -> 0.74\n",
            "Batch |  289 of 3770 loss ->  0.607627809047699 acc -> 0.74\n",
            "Batch |  290 of 3770 loss ->  0.5681780576705933 acc -> 0.74\n",
            "Batch |  291 of 3770 loss ->  0.5402582287788391 acc -> 0.74\n",
            "Batch |  292 of 3770 loss ->  0.5895532369613647 acc -> 0.74\n",
            "Batch |  293 of 3770 loss ->  0.5695914626121521 acc -> 0.74\n",
            "Batch |  294 of 3770 loss ->  0.571496844291687 acc -> 0.74\n",
            "Batch |  295 of 3770 loss ->  0.5526826977729797 acc -> 0.74\n",
            "Batch |  296 of 3770 loss ->  0.5813745260238647 acc -> 0.74\n",
            "Batch |  297 of 3770 loss ->  0.5761227607727051 acc -> 0.74\n",
            "Batch |  298 of 3770 loss ->  0.5365031361579895 acc -> 0.74\n",
            "Batch |  299 of 3770 loss ->  0.5851521492004395 acc -> 0.74\n",
            "Batch |  300 of 3770 loss ->  0.5756168961524963 acc -> 0.74\n",
            "Batch |  301 of 3770 loss ->  0.5302209854125977 acc -> 0.74\n",
            "Batch |  302 of 3770 loss ->  0.5740433931350708 acc -> 0.74\n",
            "Batch |  303 of 3770 loss ->  0.5533804893493652 acc -> 0.74\n",
            "Batch |  304 of 3770 loss ->  0.586775541305542 acc -> 0.74\n",
            "Batch |  305 of 3770 loss ->  0.5725486874580383 acc -> 0.74\n",
            "Batch |  306 of 3770 loss ->  0.5544803142547607 acc -> 0.74\n",
            "Batch |  307 of 3770 loss ->  0.6163218021392822 acc -> 0.74\n",
            "Batch |  308 of 3770 loss ->  0.5855671167373657 acc -> 0.74\n",
            "Batch |  309 of 3770 loss ->  0.5999585390090942 acc -> 0.74\n",
            "Batch |  310 of 3770 loss ->  0.5753706693649292 acc -> 0.74\n",
            "Batch |  311 of 3770 loss ->  0.571283221244812 acc -> 0.74\n",
            "Batch |  312 of 3770 loss ->  0.5579266548156738 acc -> 0.75\n",
            "Batch |  313 of 3770 loss ->  0.5959050059318542 acc -> 0.75\n",
            "Batch |  314 of 3770 loss ->  0.5835168361663818 acc -> 0.75\n",
            "Batch |  315 of 3770 loss ->  0.5319685935974121 acc -> 0.75\n",
            "Batch |  316 of 3770 loss ->  0.591100811958313 acc -> 0.75\n",
            "Batch |  317 of 3770 loss ->  0.5816391706466675 acc -> 0.75\n",
            "Batch |  318 of 3770 loss ->  0.5808530449867249 acc -> 0.75\n",
            "Batch |  319 of 3770 loss ->  0.5637447834014893 acc -> 0.75\n",
            "Batch |  320 of 3770 loss ->  0.5670897960662842 acc -> 0.75\n",
            "Batch |  321 of 3770 loss ->  0.5816975235939026 acc -> 0.75\n",
            "Batch |  322 of 3770 loss ->  0.5735195875167847 acc -> 0.75\n",
            "Batch |  323 of 3770 loss ->  0.6016068458557129 acc -> 0.75\n",
            "Batch |  324 of 3770 loss ->  0.559359073638916 acc -> 0.75\n",
            "Batch |  325 of 3770 loss ->  0.5999897718429565 acc -> 0.75\n",
            "Batch |  326 of 3770 loss ->  0.5884279012680054 acc -> 0.75\n",
            "Batch |  327 of 3770 loss ->  0.5991220474243164 acc -> 0.75\n",
            "Batch |  328 of 3770 loss ->  0.5546791553497314 acc -> 0.75\n",
            "Batch |  329 of 3770 loss ->  0.5696508884429932 acc -> 0.75\n",
            "Batch |  330 of 3770 loss ->  0.5464694499969482 acc -> 0.75\n",
            "Batch |  331 of 3770 loss ->  0.5756060481071472 acc -> 0.75\n",
            "Batch |  332 of 3770 loss ->  0.5768624544143677 acc -> 0.75\n",
            "Batch |  333 of 3770 loss ->  0.5515740513801575 acc -> 0.75\n",
            "Batch |  334 of 3770 loss ->  0.5550783276557922 acc -> 0.75\n",
            "Batch |  335 of 3770 loss ->  0.5713462829589844 acc -> 0.75\n",
            "Batch |  336 of 3770 loss ->  0.5351717472076416 acc -> 0.75\n",
            "Batch |  337 of 3770 loss ->  0.5658801794052124 acc -> 0.75\n",
            "Batch |  338 of 3770 loss ->  0.5483142137527466 acc -> 0.75\n",
            "Batch |  339 of 3770 loss ->  0.561803936958313 acc -> 0.75\n",
            "Batch |  340 of 3770 loss ->  0.5916305780410767 acc -> 0.75\n",
            "Batch |  341 of 3770 loss ->  0.5554251074790955 acc -> 0.75\n",
            "Batch |  342 of 3770 loss ->  0.5686880946159363 acc -> 0.75\n",
            "Batch |  343 of 3770 loss ->  0.6132619380950928 acc -> 0.75\n",
            "Batch |  344 of 3770 loss ->  0.5747884511947632 acc -> 0.75\n",
            "Batch |  345 of 3770 loss ->  0.5319429636001587 acc -> 0.75\n",
            "Batch |  346 of 3770 loss ->  0.5542984008789062 acc -> 0.75\n",
            "Batch |  347 of 3770 loss ->  0.5546081066131592 acc -> 0.75\n",
            "Batch |  348 of 3770 loss ->  0.5945935249328613 acc -> 0.75\n",
            "Batch |  349 of 3770 loss ->  0.5815359354019165 acc -> 0.75\n",
            "Batch |  350 of 3770 loss ->  0.584755003452301 acc -> 0.75\n",
            "Batch |  351 of 3770 loss ->  0.5606765747070312 acc -> 0.75\n",
            "Batch |  352 of 3770 loss ->  0.5720292329788208 acc -> 0.75\n",
            "Batch |  353 of 3770 loss ->  0.5668268799781799 acc -> 0.75\n",
            "Batch |  354 of 3770 loss ->  0.5934292078018188 acc -> 0.75\n",
            "Batch |  355 of 3770 loss ->  0.5970922708511353 acc -> 0.75\n",
            "Batch |  356 of 3770 loss ->  0.5903058052062988 acc -> 0.75\n",
            "Batch |  357 of 3770 loss ->  0.5937610268592834 acc -> 0.75\n",
            "Batch |  358 of 3770 loss ->  0.5559678673744202 acc -> 0.75\n",
            "Batch |  359 of 3770 loss ->  0.5670405626296997 acc -> 0.75\n",
            "Batch |  360 of 3770 loss ->  0.5878303647041321 acc -> 0.75\n",
            "Batch |  361 of 3770 loss ->  0.6049209833145142 acc -> 0.75\n",
            "Batch |  362 of 3770 loss ->  0.5731523036956787 acc -> 0.75\n",
            "Batch |  363 of 3770 loss ->  0.5452947020530701 acc -> 0.75\n",
            "Batch |  364 of 3770 loss ->  0.5443171262741089 acc -> 0.75\n",
            "Batch |  365 of 3770 loss ->  0.5372660160064697 acc -> 0.75\n",
            "Batch |  366 of 3770 loss ->  0.5412440299987793 acc -> 0.75\n",
            "Batch |  367 of 3770 loss ->  0.5360493659973145 acc -> 0.75\n",
            "Batch |  368 of 3770 loss ->  0.5441855788230896 acc -> 0.75\n",
            "Batch |  369 of 3770 loss ->  0.5758764743804932 acc -> 0.75\n",
            "Batch |  370 of 3770 loss ->  0.5430678129196167 acc -> 0.75\n",
            "Batch |  371 of 3770 loss ->  0.5568600296974182 acc -> 0.75\n",
            "Batch |  372 of 3770 loss ->  0.5296923518180847 acc -> 0.75\n",
            "Batch |  373 of 3770 loss ->  0.5725058913230896 acc -> 0.75\n",
            "Batch |  374 of 3770 loss ->  0.5438092947006226 acc -> 0.75\n",
            "Batch |  375 of 3770 loss ->  0.5630891919136047 acc -> 0.75\n",
            "Batch |  376 of 3770 loss ->  0.555904746055603 acc -> 0.76\n",
            "Batch |  377 of 3770 loss ->  0.5804098844528198 acc -> 0.76\n",
            "Batch |  378 of 3770 loss ->  0.590850830078125 acc -> 0.76\n",
            "Batch |  379 of 3770 loss ->  0.532588005065918 acc -> 0.76\n",
            "Batch |  380 of 3770 loss ->  0.548346996307373 acc -> 0.76\n",
            "Batch |  381 of 3770 loss ->  0.5733556747436523 acc -> 0.76\n",
            "Batch |  382 of 3770 loss ->  0.5677236914634705 acc -> 0.76\n",
            "Batch |  383 of 3770 loss ->  0.5750119090080261 acc -> 0.76\n",
            "Batch |  384 of 3770 loss ->  0.5563607215881348 acc -> 0.76\n",
            "Batch |  385 of 3770 loss ->  0.5757029056549072 acc -> 0.76\n",
            "Batch |  386 of 3770 loss ->  0.5682027339935303 acc -> 0.76\n",
            "Batch |  387 of 3770 loss ->  0.5188394784927368 acc -> 0.76\n",
            "Batch |  388 of 3770 loss ->  0.5482723712921143 acc -> 0.76\n",
            "Batch |  389 of 3770 loss ->  0.5345198512077332 acc -> 0.76\n",
            "Batch |  390 of 3770 loss ->  0.5674127340316772 acc -> 0.76\n",
            "Batch |  391 of 3770 loss ->  0.5713275671005249 acc -> 0.76\n",
            "Batch |  392 of 3770 loss ->  0.5995917916297913 acc -> 0.76\n",
            "Batch |  393 of 3770 loss ->  0.558423638343811 acc -> 0.76\n",
            "Batch |  394 of 3770 loss ->  0.5402517318725586 acc -> 0.76\n",
            "Batch |  395 of 3770 loss ->  0.5679163932800293 acc -> 0.76\n",
            "Batch |  396 of 3770 loss ->  0.5400451421737671 acc -> 0.76\n",
            "Batch |  397 of 3770 loss ->  0.5483259558677673 acc -> 0.76\n",
            "Batch |  398 of 3770 loss ->  0.548658013343811 acc -> 0.76\n",
            "Batch |  399 of 3770 loss ->  0.5954049229621887 acc -> 0.76\n",
            "Batch |  400 of 3770 loss ->  0.5454983711242676 acc -> 0.76\n",
            "Batch |  401 of 3770 loss ->  0.5698311924934387 acc -> 0.76\n",
            "Batch |  402 of 3770 loss ->  0.5564735531806946 acc -> 0.76\n",
            "Batch |  403 of 3770 loss ->  0.5505820512771606 acc -> 0.76\n",
            "Batch |  404 of 3770 loss ->  0.514423131942749 acc -> 0.76\n",
            "Batch |  405 of 3770 loss ->  0.5545309782028198 acc -> 0.76\n",
            "Batch |  406 of 3770 loss ->  0.575648844242096 acc -> 0.76\n",
            "Batch |  407 of 3770 loss ->  0.586254358291626 acc -> 0.76\n",
            "Batch |  408 of 3770 loss ->  0.5743663311004639 acc -> 0.76\n",
            "Batch |  409 of 3770 loss ->  0.5335670113563538 acc -> 0.76\n",
            "Batch |  410 of 3770 loss ->  0.5648037195205688 acc -> 0.76\n",
            "Batch |  411 of 3770 loss ->  0.5621980428695679 acc -> 0.76\n",
            "Batch |  412 of 3770 loss ->  0.5533483028411865 acc -> 0.76\n",
            "Batch |  413 of 3770 loss ->  0.581568717956543 acc -> 0.76\n",
            "Batch |  414 of 3770 loss ->  0.5325174331665039 acc -> 0.76\n",
            "Batch |  415 of 3770 loss ->  0.5474247932434082 acc -> 0.76\n",
            "Batch |  416 of 3770 loss ->  0.5481482148170471 acc -> 0.76\n",
            "Batch |  417 of 3770 loss ->  0.5701708793640137 acc -> 0.76\n",
            "Batch |  418 of 3770 loss ->  0.539898157119751 acc -> 0.76\n",
            "Batch |  419 of 3770 loss ->  0.5686843395233154 acc -> 0.76\n",
            "Batch |  420 of 3770 loss ->  0.5478401184082031 acc -> 0.76\n",
            "Batch |  421 of 3770 loss ->  0.608512282371521 acc -> 0.76\n",
            "Batch |  422 of 3770 loss ->  0.537928581237793 acc -> 0.76\n",
            "Batch |  423 of 3770 loss ->  0.5742173194885254 acc -> 0.76\n",
            "Batch |  424 of 3770 loss ->  0.5445091724395752 acc -> 0.76\n",
            "Batch |  425 of 3770 loss ->  0.5702418088912964 acc -> 0.76\n",
            "Batch |  426 of 3770 loss ->  0.6239414811134338 acc -> 0.76\n",
            "Batch |  427 of 3770 loss ->  0.5639167428016663 acc -> 0.76\n",
            "Batch |  428 of 3770 loss ->  0.5599634051322937 acc -> 0.76\n",
            "Batch |  429 of 3770 loss ->  0.5683531761169434 acc -> 0.76\n",
            "Batch |  430 of 3770 loss ->  0.5534067153930664 acc -> 0.76\n",
            "Batch |  431 of 3770 loss ->  0.5896679162979126 acc -> 0.76\n",
            "Batch |  432 of 3770 loss ->  0.5519888997077942 acc -> 0.76\n",
            "Batch |  433 of 3770 loss ->  0.5339747667312622 acc -> 0.76\n",
            "Batch |  434 of 3770 loss ->  0.5261974930763245 acc -> 0.76\n",
            "Batch |  435 of 3770 loss ->  0.5314477682113647 acc -> 0.76\n",
            "Batch |  436 of 3770 loss ->  0.5859092473983765 acc -> 0.76\n",
            "Batch |  437 of 3770 loss ->  0.5379875898361206 acc -> 0.76\n",
            "Batch |  438 of 3770 loss ->  0.6084049940109253 acc -> 0.76\n",
            "Batch |  439 of 3770 loss ->  0.553504467010498 acc -> 0.76\n",
            "Batch |  440 of 3770 loss ->  0.5572254657745361 acc -> 0.76\n",
            "Batch |  441 of 3770 loss ->  0.5488508343696594 acc -> 0.76\n",
            "Batch |  442 of 3770 loss ->  0.5618228912353516 acc -> 0.76\n",
            "Batch |  443 of 3770 loss ->  0.5459729433059692 acc -> 0.76\n",
            "Batch |  444 of 3770 loss ->  0.569312572479248 acc -> 0.76\n",
            "Batch |  445 of 3770 loss ->  0.5649653673171997 acc -> 0.76\n",
            "Batch |  446 of 3770 loss ->  0.5441063642501831 acc -> 0.76\n",
            "Batch |  447 of 3770 loss ->  0.5432931780815125 acc -> 0.76\n",
            "Batch |  448 of 3770 loss ->  0.5970810651779175 acc -> 0.76\n",
            "Batch |  449 of 3770 loss ->  0.5437657833099365 acc -> 0.77\n",
            "Batch |  450 of 3770 loss ->  0.5599933862686157 acc -> 0.76\n",
            "Batch |  451 of 3770 loss ->  0.5762284994125366 acc -> 0.77\n",
            "Batch |  452 of 3770 loss ->  0.5405347347259521 acc -> 0.77\n",
            "Batch |  453 of 3770 loss ->  0.5356874465942383 acc -> 0.77\n",
            "Batch |  454 of 3770 loss ->  0.5625226497650146 acc -> 0.77\n",
            "Batch |  455 of 3770 loss ->  0.5938717126846313 acc -> 0.77\n",
            "Batch |  456 of 3770 loss ->  0.5770925283432007 acc -> 0.77\n",
            "Batch |  457 of 3770 loss ->  0.5746428966522217 acc -> 0.77\n",
            "Batch |  458 of 3770 loss ->  0.5344548225402832 acc -> 0.77\n",
            "Batch |  459 of 3770 loss ->  0.5564122200012207 acc -> 0.77\n",
            "Batch |  460 of 3770 loss ->  0.6129381656646729 acc -> 0.77\n",
            "Batch |  461 of 3770 loss ->  0.5749794244766235 acc -> 0.77\n",
            "Batch |  462 of 3770 loss ->  0.5433226823806763 acc -> 0.77\n",
            "Batch |  463 of 3770 loss ->  0.5747864842414856 acc -> 0.77\n",
            "Batch |  464 of 3770 loss ->  0.5311406254768372 acc -> 0.77\n",
            "Batch |  465 of 3770 loss ->  0.5665340423583984 acc -> 0.77\n",
            "Batch |  466 of 3770 loss ->  0.5461316108703613 acc -> 0.77\n",
            "Batch |  467 of 3770 loss ->  0.5548447966575623 acc -> 0.77\n",
            "Batch |  468 of 3770 loss ->  0.551237165927887 acc -> 0.77\n",
            "Batch |  469 of 3770 loss ->  0.542539119720459 acc -> 0.77\n",
            "Batch |  470 of 3770 loss ->  0.5428931713104248 acc -> 0.77\n",
            "Batch |  471 of 3770 loss ->  0.5283308029174805 acc -> 0.77\n",
            "Batch |  472 of 3770 loss ->  0.521776556968689 acc -> 0.77\n",
            "Batch |  473 of 3770 loss ->  0.5560969710350037 acc -> 0.77\n",
            "Batch |  474 of 3770 loss ->  0.5562968850135803 acc -> 0.77\n",
            "Batch |  475 of 3770 loss ->  0.5393240451812744 acc -> 0.77\n",
            "Batch |  476 of 3770 loss ->  0.550395667552948 acc -> 0.77\n",
            "Batch |  477 of 3770 loss ->  0.5645077228546143 acc -> 0.77\n",
            "Batch |  478 of 3770 loss ->  0.5291886329650879 acc -> 0.77\n",
            "Batch |  479 of 3770 loss ->  0.5638027787208557 acc -> 0.77\n",
            "Batch |  480 of 3770 loss ->  0.574271559715271 acc -> 0.77\n",
            "Batch |  481 of 3770 loss ->  0.5282992124557495 acc -> 0.77\n",
            "Batch |  482 of 3770 loss ->  0.575509786605835 acc -> 0.77\n",
            "Batch |  483 of 3770 loss ->  0.5518745183944702 acc -> 0.77\n",
            "Batch |  484 of 3770 loss ->  0.5395358800888062 acc -> 0.77\n",
            "Batch |  485 of 3770 loss ->  0.5496792793273926 acc -> 0.77\n",
            "Batch |  486 of 3770 loss ->  0.5486180782318115 acc -> 0.77\n",
            "Batch |  487 of 3770 loss ->  0.5667842626571655 acc -> 0.77\n",
            "Batch |  488 of 3770 loss ->  0.5429768562316895 acc -> 0.77\n",
            "Batch |  489 of 3770 loss ->  0.5576741099357605 acc -> 0.77\n",
            "Batch |  490 of 3770 loss ->  0.5216759443283081 acc -> 0.77\n",
            "Batch |  491 of 3770 loss ->  0.5689989924430847 acc -> 0.77\n",
            "Batch |  492 of 3770 loss ->  0.5557872653007507 acc -> 0.77\n",
            "Batch |  493 of 3770 loss ->  0.5850777626037598 acc -> 0.77\n",
            "Batch |  494 of 3770 loss ->  0.5960069894790649 acc -> 0.77\n",
            "Batch |  495 of 3770 loss ->  0.5759174823760986 acc -> 0.77\n",
            "Batch |  496 of 3770 loss ->  0.5141990780830383 acc -> 0.77\n",
            "Batch |  497 of 3770 loss ->  0.5471374988555908 acc -> 0.77\n",
            "Batch |  498 of 3770 loss ->  0.5637615919113159 acc -> 0.77\n",
            "Batch |  499 of 3770 loss ->  0.5387086868286133 acc -> 0.77\n",
            "Batch |  500 of 3770 loss ->  0.5266258716583252 acc -> 0.77\n",
            "Batch |  501 of 3770 loss ->  0.5467725992202759 acc -> 0.77\n",
            "Batch |  502 of 3770 loss ->  0.5794773101806641 acc -> 0.77\n",
            "Batch |  503 of 3770 loss ->  0.5568205118179321 acc -> 0.77\n",
            "Batch |  504 of 3770 loss ->  0.5335449576377869 acc -> 0.77\n",
            "Batch |  505 of 3770 loss ->  0.5689257383346558 acc -> 0.77\n",
            "Batch |  506 of 3770 loss ->  0.5253546833992004 acc -> 0.77\n",
            "Batch |  507 of 3770 loss ->  0.5649591684341431 acc -> 0.77\n",
            "Batch |  508 of 3770 loss ->  0.5697908401489258 acc -> 0.77\n",
            "Batch |  509 of 3770 loss ->  0.5657109618186951 acc -> 0.77\n",
            "Batch |  510 of 3770 loss ->  0.5384193658828735 acc -> 0.77\n",
            "Batch |  511 of 3770 loss ->  0.5426018238067627 acc -> 0.77\n",
            "Batch |  512 of 3770 loss ->  0.533176064491272 acc -> 0.77\n",
            "Batch |  513 of 3770 loss ->  0.5414248108863831 acc -> 0.77\n",
            "Batch |  514 of 3770 loss ->  0.5744774341583252 acc -> 0.77\n",
            "Batch |  515 of 3770 loss ->  0.5141319036483765 acc -> 0.77\n",
            "Batch |  516 of 3770 loss ->  0.5491634607315063 acc -> 0.77\n",
            "Batch |  517 of 3770 loss ->  0.5596924424171448 acc -> 0.77\n",
            "Batch |  518 of 3770 loss ->  0.5666348338127136 acc -> 0.77\n",
            "Batch |  519 of 3770 loss ->  0.5492221117019653 acc -> 0.77\n",
            "Batch |  520 of 3770 loss ->  0.5549703240394592 acc -> 0.77\n",
            "Batch |  521 of 3770 loss ->  0.5528461933135986 acc -> 0.77\n",
            "Batch |  522 of 3770 loss ->  0.5407317876815796 acc -> 0.77\n",
            "Batch |  523 of 3770 loss ->  0.5323693752288818 acc -> 0.77\n",
            "Batch |  524 of 3770 loss ->  0.5582733154296875 acc -> 0.77\n",
            "Batch |  525 of 3770 loss ->  0.49210238456726074 acc -> 0.77\n",
            "Batch |  526 of 3770 loss ->  0.5288879871368408 acc -> 0.77\n",
            "Batch |  527 of 3770 loss ->  0.5597320199012756 acc -> 0.77\n",
            "Batch |  528 of 3770 loss ->  0.5512243509292603 acc -> 0.77\n",
            "Batch |  529 of 3770 loss ->  0.550589919090271 acc -> 0.77\n",
            "Batch |  530 of 3770 loss ->  0.5810061693191528 acc -> 0.77\n",
            "Batch |  531 of 3770 loss ->  0.5617613792419434 acc -> 0.77\n",
            "Batch |  532 of 3770 loss ->  0.5547763109207153 acc -> 0.77\n",
            "Batch |  533 of 3770 loss ->  0.5766929984092712 acc -> 0.77\n",
            "Batch |  534 of 3770 loss ->  0.53190016746521 acc -> 0.77\n",
            "Batch |  535 of 3770 loss ->  0.512602686882019 acc -> 0.77\n",
            "Batch |  536 of 3770 loss ->  0.5476672649383545 acc -> 0.77\n",
            "Batch |  537 of 3770 loss ->  0.5507123470306396 acc -> 0.77\n",
            "Batch |  538 of 3770 loss ->  0.544513463973999 acc -> 0.77\n",
            "Batch |  539 of 3770 loss ->  0.5457430481910706 acc -> 0.77\n",
            "Batch |  540 of 3770 loss ->  0.5277805924415588 acc -> 0.77\n",
            "Batch |  541 of 3770 loss ->  0.5443097352981567 acc -> 0.77\n",
            "Batch |  542 of 3770 loss ->  0.5342198610305786 acc -> 0.78\n",
            "Batch |  543 of 3770 loss ->  0.5499407052993774 acc -> 0.78\n",
            "Batch |  544 of 3770 loss ->  0.570785403251648 acc -> 0.78\n",
            "Batch |  545 of 3770 loss ->  0.5639086365699768 acc -> 0.78\n",
            "Batch |  546 of 3770 loss ->  0.5438172817230225 acc -> 0.78\n",
            "Batch |  547 of 3770 loss ->  0.5146249532699585 acc -> 0.78\n",
            "Batch |  548 of 3770 loss ->  0.5369912385940552 acc -> 0.78\n",
            "Batch |  549 of 3770 loss ->  0.5511496067047119 acc -> 0.78\n",
            "Batch |  550 of 3770 loss ->  0.5306100845336914 acc -> 0.78\n",
            "Batch |  551 of 3770 loss ->  0.5480066537857056 acc -> 0.78\n",
            "Batch |  552 of 3770 loss ->  0.5421110391616821 acc -> 0.78\n",
            "Batch |  553 of 3770 loss ->  0.5485536456108093 acc -> 0.78\n",
            "Batch |  554 of 3770 loss ->  0.5804563760757446 acc -> 0.78\n",
            "Batch |  555 of 3770 loss ->  0.5414400100708008 acc -> 0.78\n",
            "Batch |  556 of 3770 loss ->  0.5378915071487427 acc -> 0.78\n",
            "Batch |  557 of 3770 loss ->  0.5368536710739136 acc -> 0.78\n",
            "Batch |  558 of 3770 loss ->  0.5425961017608643 acc -> 0.78\n",
            "Batch |  559 of 3770 loss ->  0.5786439776420593 acc -> 0.78\n",
            "Batch |  560 of 3770 loss ->  0.567272961139679 acc -> 0.78\n",
            "Batch |  561 of 3770 loss ->  0.5320019721984863 acc -> 0.78\n",
            "Batch |  562 of 3770 loss ->  0.5120571851730347 acc -> 0.78\n",
            "Batch |  563 of 3770 loss ->  0.6111852526664734 acc -> 0.78\n",
            "Batch |  564 of 3770 loss ->  0.5863775014877319 acc -> 0.78\n",
            "Batch |  565 of 3770 loss ->  0.5135608911514282 acc -> 0.78\n",
            "Batch |  566 of 3770 loss ->  0.5610094666481018 acc -> 0.78\n",
            "Batch |  567 of 3770 loss ->  0.5495625138282776 acc -> 0.78\n",
            "Batch |  568 of 3770 loss ->  0.5452713966369629 acc -> 0.78\n",
            "Batch |  569 of 3770 loss ->  0.5510622262954712 acc -> 0.78\n",
            "Batch |  570 of 3770 loss ->  0.5584798455238342 acc -> 0.78\n",
            "Batch |  571 of 3770 loss ->  0.5211755037307739 acc -> 0.78\n",
            "Batch |  572 of 3770 loss ->  0.50505530834198 acc -> 0.78\n",
            "Batch |  573 of 3770 loss ->  0.5703060626983643 acc -> 0.78\n",
            "Batch |  574 of 3770 loss ->  0.5871322751045227 acc -> 0.78\n",
            "Batch |  575 of 3770 loss ->  0.5297801494598389 acc -> 0.78\n",
            "Batch |  576 of 3770 loss ->  0.4981292486190796 acc -> 0.78\n",
            "Batch |  577 of 3770 loss ->  0.5402865409851074 acc -> 0.78\n",
            "Batch |  578 of 3770 loss ->  0.5255258083343506 acc -> 0.78\n",
            "Batch |  579 of 3770 loss ->  0.5405640006065369 acc -> 0.78\n",
            "Batch |  580 of 3770 loss ->  0.5984370112419128 acc -> 0.78\n",
            "Batch |  581 of 3770 loss ->  0.5503870248794556 acc -> 0.78\n",
            "Batch |  582 of 3770 loss ->  0.5275589823722839 acc -> 0.78\n",
            "Batch |  583 of 3770 loss ->  0.5029994249343872 acc -> 0.78\n",
            "Batch |  584 of 3770 loss ->  0.5602695941925049 acc -> 0.78\n",
            "Batch |  585 of 3770 loss ->  0.5649310946464539 acc -> 0.78\n",
            "Batch |  586 of 3770 loss ->  0.5374264717102051 acc -> 0.78\n",
            "Batch |  587 of 3770 loss ->  0.5771569013595581 acc -> 0.78\n",
            "Batch |  588 of 3770 loss ->  0.5765466094017029 acc -> 0.78\n",
            "Batch |  589 of 3770 loss ->  0.5303926467895508 acc -> 0.78\n",
            "Batch |  590 of 3770 loss ->  0.5384480357170105 acc -> 0.78\n",
            "Batch |  591 of 3770 loss ->  0.5215960144996643 acc -> 0.78\n",
            "Batch |  592 of 3770 loss ->  0.550050675868988 acc -> 0.78\n",
            "Batch |  593 of 3770 loss ->  0.5253663063049316 acc -> 0.78\n",
            "Batch |  594 of 3770 loss ->  0.5221240520477295 acc -> 0.78\n",
            "Batch |  595 of 3770 loss ->  0.48573198914527893 acc -> 0.78\n",
            "Batch |  596 of 3770 loss ->  0.5392105579376221 acc -> 0.78\n",
            "Batch |  597 of 3770 loss ->  0.5528792142868042 acc -> 0.78\n",
            "Batch |  598 of 3770 loss ->  0.5622837543487549 acc -> 0.78\n",
            "Batch |  599 of 3770 loss ->  0.560386598110199 acc -> 0.78\n",
            "Batch |  600 of 3770 loss ->  0.5375187397003174 acc -> 0.78\n",
            "Batch |  601 of 3770 loss ->  0.5796714425086975 acc -> 0.78\n",
            "Batch |  602 of 3770 loss ->  0.5336555242538452 acc -> 0.78\n",
            "Batch |  603 of 3770 loss ->  0.5342714786529541 acc -> 0.78\n",
            "Batch |  604 of 3770 loss ->  0.5283226370811462 acc -> 0.78\n",
            "Batch |  605 of 3770 loss ->  0.5563919544219971 acc -> 0.78\n",
            "Batch |  606 of 3770 loss ->  0.5377134680747986 acc -> 0.78\n",
            "Batch |  607 of 3770 loss ->  0.49992960691452026 acc -> 0.78\n",
            "Batch |  608 of 3770 loss ->  0.5405395030975342 acc -> 0.78\n",
            "Batch |  609 of 3770 loss ->  0.5516594648361206 acc -> 0.78\n",
            "Batch |  610 of 3770 loss ->  0.5014539957046509 acc -> 0.78\n",
            "Batch |  611 of 3770 loss ->  0.5222353935241699 acc -> 0.78\n",
            "Batch |  612 of 3770 loss ->  0.5363137722015381 acc -> 0.78\n",
            "Batch |  613 of 3770 loss ->  0.51826012134552 acc -> 0.78\n",
            "Batch |  614 of 3770 loss ->  0.5567975640296936 acc -> 0.78\n",
            "Batch |  615 of 3770 loss ->  0.5613099932670593 acc -> 0.78\n",
            "Batch |  616 of 3770 loss ->  0.5159226655960083 acc -> 0.78\n",
            "Batch |  617 of 3770 loss ->  0.5415511727333069 acc -> 0.78\n",
            "Batch |  618 of 3770 loss ->  0.5532423257827759 acc -> 0.78\n",
            "Batch |  619 of 3770 loss ->  0.5377832055091858 acc -> 0.78\n",
            "Batch |  620 of 3770 loss ->  0.5667572021484375 acc -> 0.78\n",
            "Batch |  621 of 3770 loss ->  0.5523108243942261 acc -> 0.78\n",
            "Batch |  622 of 3770 loss ->  0.5255496501922607 acc -> 0.78\n",
            "Batch |  623 of 3770 loss ->  0.5175045728683472 acc -> 0.78\n",
            "Batch |  624 of 3770 loss ->  0.5659842491149902 acc -> 0.78\n",
            "Batch |  625 of 3770 loss ->  0.546202540397644 acc -> 0.78\n",
            "Batch |  626 of 3770 loss ->  0.5374010801315308 acc -> 0.78\n",
            "Batch |  627 of 3770 loss ->  0.5322388410568237 acc -> 0.78\n",
            "Batch |  628 of 3770 loss ->  0.5464363098144531 acc -> 0.78\n",
            "Batch |  629 of 3770 loss ->  0.50192791223526 acc -> 0.78\n",
            "Batch |  630 of 3770 loss ->  0.5145037174224854 acc -> 0.78\n",
            "Batch |  631 of 3770 loss ->  0.6040701866149902 acc -> 0.78\n",
            "Batch |  632 of 3770 loss ->  0.5084725022315979 acc -> 0.78\n",
            "Batch |  633 of 3770 loss ->  0.558181643486023 acc -> 0.78\n",
            "Batch |  634 of 3770 loss ->  0.55536949634552 acc -> 0.78\n",
            "Batch |  635 of 3770 loss ->  0.5339523553848267 acc -> 0.78\n",
            "Batch |  636 of 3770 loss ->  0.5417619943618774 acc -> 0.78\n",
            "Batch |  637 of 3770 loss ->  0.5317801237106323 acc -> 0.78\n",
            "Batch |  638 of 3770 loss ->  0.5798453092575073 acc -> 0.78\n",
            "Batch |  639 of 3770 loss ->  0.5144325494766235 acc -> 0.78\n",
            "Batch |  640 of 3770 loss ->  0.5429142117500305 acc -> 0.78\n",
            "Batch |  641 of 3770 loss ->  0.518862247467041 acc -> 0.78\n",
            "Batch |  642 of 3770 loss ->  0.5082202553749084 acc -> 0.79\n",
            "Batch |  643 of 3770 loss ->  0.5138860940933228 acc -> 0.79\n",
            "Batch |  644 of 3770 loss ->  0.4895634353160858 acc -> 0.79\n",
            "Batch |  645 of 3770 loss ->  0.5066971182823181 acc -> 0.79\n",
            "Batch |  646 of 3770 loss ->  0.5000041127204895 acc -> 0.79\n",
            "Batch |  647 of 3770 loss ->  0.5244026780128479 acc -> 0.79\n",
            "Batch |  648 of 3770 loss ->  0.524826169013977 acc -> 0.79\n",
            "Batch |  649 of 3770 loss ->  0.536331057548523 acc -> 0.79\n",
            "Batch |  650 of 3770 loss ->  0.5318899750709534 acc -> 0.79\n",
            "Batch |  651 of 3770 loss ->  0.5494968891143799 acc -> 0.79\n",
            "Batch |  652 of 3770 loss ->  0.5686414241790771 acc -> 0.79\n",
            "Batch |  653 of 3770 loss ->  0.5700826644897461 acc -> 0.79\n",
            "Batch |  654 of 3770 loss ->  0.5230598449707031 acc -> 0.79\n",
            "Batch |  655 of 3770 loss ->  0.5170100927352905 acc -> 0.79\n",
            "Batch |  656 of 3770 loss ->  0.530933678150177 acc -> 0.79\n",
            "Batch |  657 of 3770 loss ->  0.48146045207977295 acc -> 0.79\n",
            "Batch |  658 of 3770 loss ->  0.5366873741149902 acc -> 0.79\n",
            "Batch |  659 of 3770 loss ->  0.5084302425384521 acc -> 0.79\n",
            "Batch |  660 of 3770 loss ->  0.575469970703125 acc -> 0.79\n",
            "Batch |  661 of 3770 loss ->  0.5197290182113647 acc -> 0.79\n",
            "Batch |  662 of 3770 loss ->  0.534109890460968 acc -> 0.79\n",
            "Batch |  663 of 3770 loss ->  0.504422664642334 acc -> 0.79\n",
            "Batch |  664 of 3770 loss ->  0.5514864921569824 acc -> 0.79\n",
            "Batch |  665 of 3770 loss ->  0.5600139498710632 acc -> 0.79\n",
            "Batch |  666 of 3770 loss ->  0.5462040901184082 acc -> 0.79\n",
            "Batch |  667 of 3770 loss ->  0.5360837578773499 acc -> 0.79\n",
            "Batch |  668 of 3770 loss ->  0.5167689919471741 acc -> 0.79\n",
            "Batch |  669 of 3770 loss ->  0.5659908056259155 acc -> 0.79\n",
            "Batch |  670 of 3770 loss ->  0.5474262237548828 acc -> 0.79\n",
            "Batch |  671 of 3770 loss ->  0.5178754329681396 acc -> 0.79\n",
            "Batch |  672 of 3770 loss ->  0.5434994101524353 acc -> 0.79\n",
            "Batch |  673 of 3770 loss ->  0.5232234001159668 acc -> 0.79\n",
            "Batch |  674 of 3770 loss ->  0.5883697271347046 acc -> 0.79\n",
            "Batch |  675 of 3770 loss ->  0.5201775431632996 acc -> 0.79\n",
            "Batch |  676 of 3770 loss ->  0.5322008728981018 acc -> 0.79\n",
            "Batch |  677 of 3770 loss ->  0.5196800231933594 acc -> 0.79\n",
            "Batch |  678 of 3770 loss ->  0.5184899568557739 acc -> 0.79\n",
            "Batch |  679 of 3770 loss ->  0.494772344827652 acc -> 0.79\n",
            "Batch |  680 of 3770 loss ->  0.5247297286987305 acc -> 0.79\n",
            "Batch |  681 of 3770 loss ->  0.5163624286651611 acc -> 0.79\n",
            "Batch |  682 of 3770 loss ->  0.5201368927955627 acc -> 0.79\n",
            "Batch |  683 of 3770 loss ->  0.51810622215271 acc -> 0.79\n",
            "Batch |  684 of 3770 loss ->  0.5178905725479126 acc -> 0.79\n",
            "Batch |  685 of 3770 loss ->  0.5823522210121155 acc -> 0.79\n",
            "Batch |  686 of 3770 loss ->  0.5178987979888916 acc -> 0.79\n",
            "Batch |  687 of 3770 loss ->  0.49732503294944763 acc -> 0.79\n",
            "Batch |  688 of 3770 loss ->  0.5594446659088135 acc -> 0.79\n",
            "Batch |  689 of 3770 loss ->  0.538118302822113 acc -> 0.79\n",
            "Batch |  690 of 3770 loss ->  0.5363302826881409 acc -> 0.79\n",
            "Batch |  691 of 3770 loss ->  0.5148935317993164 acc -> 0.79\n",
            "Batch |  692 of 3770 loss ->  0.5353712439537048 acc -> 0.79\n",
            "Batch |  693 of 3770 loss ->  0.5143237113952637 acc -> 0.79\n",
            "Batch |  694 of 3770 loss ->  0.5180792808532715 acc -> 0.79\n",
            "Batch |  695 of 3770 loss ->  0.5104719996452332 acc -> 0.79\n",
            "Batch |  696 of 3770 loss ->  0.5002154111862183 acc -> 0.79\n",
            "Batch |  697 of 3770 loss ->  0.5165493488311768 acc -> 0.79\n",
            "Batch |  698 of 3770 loss ->  0.5646225214004517 acc -> 0.79\n",
            "Batch |  699 of 3770 loss ->  0.5348415374755859 acc -> 0.79\n",
            "Batch |  700 of 3770 loss ->  0.5300272107124329 acc -> 0.79\n",
            "Batch |  701 of 3770 loss ->  0.5145796537399292 acc -> 0.79\n",
            "Batch |  702 of 3770 loss ->  0.5260430574417114 acc -> 0.79\n",
            "Batch |  703 of 3770 loss ->  0.5447946190834045 acc -> 0.79\n",
            "Batch |  704 of 3770 loss ->  0.5353251695632935 acc -> 0.79\n",
            "Batch |  705 of 3770 loss ->  0.5309699773788452 acc -> 0.79\n",
            "Batch |  706 of 3770 loss ->  0.5434667468070984 acc -> 0.79\n",
            "Batch |  707 of 3770 loss ->  0.5267572402954102 acc -> 0.79\n",
            "Batch |  708 of 3770 loss ->  0.5413305163383484 acc -> 0.79\n",
            "Batch |  709 of 3770 loss ->  0.5221859216690063 acc -> 0.79\n",
            "Batch |  710 of 3770 loss ->  0.5140916109085083 acc -> 0.79\n",
            "Batch |  711 of 3770 loss ->  0.5491818785667419 acc -> 0.79\n",
            "Batch |  712 of 3770 loss ->  0.5587862133979797 acc -> 0.79\n",
            "Batch |  713 of 3770 loss ->  0.5005475878715515 acc -> 0.79\n",
            "Batch |  714 of 3770 loss ->  0.5067711472511292 acc -> 0.79\n",
            "Batch |  715 of 3770 loss ->  0.5336584448814392 acc -> 0.79\n",
            "Batch |  716 of 3770 loss ->  0.49738842248916626 acc -> 0.79\n",
            "Batch |  717 of 3770 loss ->  0.4716746211051941 acc -> 0.79\n",
            "Batch |  718 of 3770 loss ->  0.5491054654121399 acc -> 0.79\n",
            "Batch |  719 of 3770 loss ->  0.5695785284042358 acc -> 0.79\n",
            "Batch |  720 of 3770 loss ->  0.5121092200279236 acc -> 0.79\n",
            "Batch |  721 of 3770 loss ->  0.5155786275863647 acc -> 0.79\n",
            "Batch |  722 of 3770 loss ->  0.5083527565002441 acc -> 0.79\n",
            "Batch |  723 of 3770 loss ->  0.5306984186172485 acc -> 0.79\n",
            "Batch |  724 of 3770 loss ->  0.5043087005615234 acc -> 0.79\n",
            "Batch |  725 of 3770 loss ->  0.5388662815093994 acc -> 0.79\n",
            "Batch |  726 of 3770 loss ->  0.5607865452766418 acc -> 0.79\n",
            "Batch |  727 of 3770 loss ->  0.5211405754089355 acc -> 0.79\n",
            "Batch |  728 of 3770 loss ->  0.5345122814178467 acc -> 0.79\n",
            "Batch |  729 of 3770 loss ->  0.5303506851196289 acc -> 0.79\n",
            "Batch |  730 of 3770 loss ->  0.534430980682373 acc -> 0.79\n",
            "Batch |  731 of 3770 loss ->  0.522102415561676 acc -> 0.79\n",
            "Batch |  732 of 3770 loss ->  0.5543349385261536 acc -> 0.8\n",
            "Batch |  733 of 3770 loss ->  0.5629324913024902 acc -> 0.8\n",
            "Batch |  734 of 3770 loss ->  0.514161229133606 acc -> 0.8\n",
            "Batch |  735 of 3770 loss ->  0.5058944225311279 acc -> 0.8\n",
            "Batch |  736 of 3770 loss ->  0.5064520835876465 acc -> 0.8\n",
            "Batch |  737 of 3770 loss ->  0.4968779981136322 acc -> 0.8\n",
            "Batch |  738 of 3770 loss ->  0.4916676878929138 acc -> 0.8\n",
            "Batch |  739 of 3770 loss ->  0.5012609958648682 acc -> 0.8\n",
            "Batch |  740 of 3770 loss ->  0.54240882396698 acc -> 0.8\n",
            "Batch |  741 of 3770 loss ->  0.49201756715774536 acc -> 0.8\n",
            "Batch |  742 of 3770 loss ->  0.5174661874771118 acc -> 0.8\n",
            "Batch |  743 of 3770 loss ->  0.5325644016265869 acc -> 0.8\n",
            "Batch |  744 of 3770 loss ->  0.5372341871261597 acc -> 0.8\n",
            "Batch |  745 of 3770 loss ->  0.5051670074462891 acc -> 0.8\n",
            "Batch |  746 of 3770 loss ->  0.5310029983520508 acc -> 0.8\n",
            "Batch |  747 of 3770 loss ->  0.484730064868927 acc -> 0.8\n",
            "Batch |  748 of 3770 loss ->  0.5219249725341797 acc -> 0.8\n",
            "Batch |  749 of 3770 loss ->  0.501002311706543 acc -> 0.8\n",
            "Batch |  750 of 3770 loss ->  0.5138943195343018 acc -> 0.8\n",
            "Batch |  751 of 3770 loss ->  0.49974650144577026 acc -> 0.8\n",
            "Batch |  752 of 3770 loss ->  0.5399625301361084 acc -> 0.8\n",
            "Batch |  753 of 3770 loss ->  0.5554534196853638 acc -> 0.8\n",
            "Batch |  754 of 3770 loss ->  0.5153356194496155 acc -> 0.8\n",
            "Batch |  755 of 3770 loss ->  0.5391947031021118 acc -> 0.8\n",
            "Batch |  756 of 3770 loss ->  0.5219299793243408 acc -> 0.8\n",
            "Batch |  757 of 3770 loss ->  0.5161657929420471 acc -> 0.8\n",
            "Batch |  758 of 3770 loss ->  0.5231178998947144 acc -> 0.8\n",
            "Batch |  759 of 3770 loss ->  0.5154708623886108 acc -> 0.8\n",
            "Batch |  760 of 3770 loss ->  0.5107965469360352 acc -> 0.8\n",
            "Batch |  761 of 3770 loss ->  0.5091691017150879 acc -> 0.8\n",
            "Batch |  762 of 3770 loss ->  0.518174946308136 acc -> 0.8\n",
            "Batch |  763 of 3770 loss ->  0.5500683784484863 acc -> 0.8\n",
            "Batch |  764 of 3770 loss ->  0.49038565158843994 acc -> 0.8\n",
            "Batch |  765 of 3770 loss ->  0.5764878988265991 acc -> 0.8\n",
            "Batch |  766 of 3770 loss ->  0.5187474489212036 acc -> 0.8\n",
            "Batch |  767 of 3770 loss ->  0.5384362936019897 acc -> 0.8\n",
            "Batch |  768 of 3770 loss ->  0.5421863794326782 acc -> 0.8\n",
            "Batch |  769 of 3770 loss ->  0.49694982171058655 acc -> 0.8\n",
            "Batch |  770 of 3770 loss ->  0.4924350380897522 acc -> 0.8\n",
            "Batch |  771 of 3770 loss ->  0.5491454601287842 acc -> 0.8\n",
            "Batch |  772 of 3770 loss ->  0.5156875848770142 acc -> 0.8\n",
            "Batch |  773 of 3770 loss ->  0.5097995400428772 acc -> 0.8\n",
            "Batch |  774 of 3770 loss ->  0.4871392846107483 acc -> 0.8\n",
            "Batch |  775 of 3770 loss ->  0.5390325784683228 acc -> 0.8\n",
            "Batch |  776 of 3770 loss ->  0.4948127865791321 acc -> 0.8\n",
            "Batch |  777 of 3770 loss ->  0.5303666591644287 acc -> 0.8\n",
            "Batch |  778 of 3770 loss ->  0.5190273523330688 acc -> 0.8\n",
            "Batch |  779 of 3770 loss ->  0.5249245762825012 acc -> 0.8\n",
            "Batch |  780 of 3770 loss ->  0.5004665851593018 acc -> 0.8\n",
            "Batch |  781 of 3770 loss ->  0.544672966003418 acc -> 0.8\n",
            "Batch |  782 of 3770 loss ->  0.5186574459075928 acc -> 0.8\n",
            "Batch |  783 of 3770 loss ->  0.541921079158783 acc -> 0.8\n",
            "Batch |  784 of 3770 loss ->  0.5093066692352295 acc -> 0.8\n",
            "Batch |  785 of 3770 loss ->  0.5178265571594238 acc -> 0.8\n",
            "Batch |  786 of 3770 loss ->  0.5359171628952026 acc -> 0.8\n",
            "Batch |  787 of 3770 loss ->  0.5112787485122681 acc -> 0.8\n",
            "Batch |  788 of 3770 loss ->  0.5612075328826904 acc -> 0.8\n",
            "Batch |  789 of 3770 loss ->  0.5128363370895386 acc -> 0.8\n",
            "Batch |  790 of 3770 loss ->  0.5249081254005432 acc -> 0.8\n",
            "Batch |  791 of 3770 loss ->  0.5309638977050781 acc -> 0.8\n",
            "Batch |  792 of 3770 loss ->  0.5055233240127563 acc -> 0.8\n",
            "Batch |  793 of 3770 loss ->  0.5223173499107361 acc -> 0.8\n",
            "Batch |  794 of 3770 loss ->  0.5299429297447205 acc -> 0.8\n",
            "Batch |  795 of 3770 loss ->  0.5160962343215942 acc -> 0.8\n",
            "Batch |  796 of 3770 loss ->  0.5162845253944397 acc -> 0.8\n",
            "Batch |  797 of 3770 loss ->  0.5510406494140625 acc -> 0.8\n",
            "Batch |  798 of 3770 loss ->  0.511347770690918 acc -> 0.8\n",
            "Batch |  799 of 3770 loss ->  0.49036383628845215 acc -> 0.8\n",
            "Batch |  800 of 3770 loss ->  0.545232892036438 acc -> 0.8\n",
            "Batch |  801 of 3770 loss ->  0.5214653015136719 acc -> 0.8\n",
            "Batch |  802 of 3770 loss ->  0.5382258892059326 acc -> 0.8\n",
            "Batch |  803 of 3770 loss ->  0.5771164894104004 acc -> 0.8\n",
            "Batch |  804 of 3770 loss ->  0.5603799223899841 acc -> 0.8\n",
            "Batch |  805 of 3770 loss ->  0.5150067210197449 acc -> 0.8\n",
            "Batch |  806 of 3770 loss ->  0.4952952265739441 acc -> 0.8\n",
            "Batch |  807 of 3770 loss ->  0.4935376048088074 acc -> 0.8\n",
            "Batch |  808 of 3770 loss ->  0.5246162414550781 acc -> 0.8\n",
            "Batch |  809 of 3770 loss ->  0.5120657682418823 acc -> 0.8\n",
            "Batch |  810 of 3770 loss ->  0.5301706790924072 acc -> 0.8\n",
            "Batch |  811 of 3770 loss ->  0.5025395750999451 acc -> 0.8\n",
            "Batch |  812 of 3770 loss ->  0.5016281604766846 acc -> 0.8\n",
            "Batch |  813 of 3770 loss ->  0.5242149829864502 acc -> 0.8\n",
            "Batch |  814 of 3770 loss ->  0.5195196866989136 acc -> 0.8\n",
            "Batch |  815 of 3770 loss ->  0.5200538635253906 acc -> 0.8\n",
            "Batch |  816 of 3770 loss ->  0.49051687121391296 acc -> 0.8\n",
            "Batch |  817 of 3770 loss ->  0.5239144563674927 acc -> 0.8\n",
            "Batch |  818 of 3770 loss ->  0.48577880859375 acc -> 0.8\n",
            "Batch |  819 of 3770 loss ->  0.5301608443260193 acc -> 0.8\n",
            "Batch |  820 of 3770 loss ->  0.4941054880619049 acc -> 0.8\n",
            "Batch |  821 of 3770 loss ->  0.5189002752304077 acc -> 0.8\n",
            "Batch |  822 of 3770 loss ->  0.5522007942199707 acc -> 0.8\n",
            "Batch |  823 of 3770 loss ->  0.5382049083709717 acc -> 0.8\n",
            "Batch |  824 of 3770 loss ->  0.4660574495792389 acc -> 0.8\n",
            "Batch |  825 of 3770 loss ->  0.4984421730041504 acc -> 0.8\n",
            "Batch |  826 of 3770 loss ->  0.5417124032974243 acc -> 0.8\n",
            "Batch |  827 of 3770 loss ->  0.5404756665229797 acc -> 0.8\n",
            "Batch |  828 of 3770 loss ->  0.5037667751312256 acc -> 0.8\n",
            "Batch |  829 of 3770 loss ->  0.5032864212989807 acc -> 0.8\n",
            "Batch |  830 of 3770 loss ->  0.4890778660774231 acc -> 0.8\n",
            "Batch |  831 of 3770 loss ->  0.4683980941772461 acc -> 0.8\n",
            "Batch |  832 of 3770 loss ->  0.5045438408851624 acc -> 0.8\n",
            "Batch |  833 of 3770 loss ->  0.4900027811527252 acc -> 0.8\n",
            "Batch |  834 of 3770 loss ->  0.53071528673172 acc -> 0.81\n",
            "Batch |  835 of 3770 loss ->  0.5148828029632568 acc -> 0.81\n",
            "Batch |  836 of 3770 loss ->  0.4996628761291504 acc -> 0.81\n",
            "Batch |  837 of 3770 loss ->  0.5194779634475708 acc -> 0.81\n",
            "Batch |  838 of 3770 loss ->  0.5285710096359253 acc -> 0.81\n",
            "Batch |  839 of 3770 loss ->  0.5185251235961914 acc -> 0.81\n",
            "Batch |  840 of 3770 loss ->  0.5259323120117188 acc -> 0.81\n",
            "Batch |  841 of 3770 loss ->  0.5387824177742004 acc -> 0.81\n",
            "Batch |  842 of 3770 loss ->  0.49478477239608765 acc -> 0.81\n",
            "Batch |  843 of 3770 loss ->  0.4963639974594116 acc -> 0.81\n",
            "Batch |  844 of 3770 loss ->  0.5436537265777588 acc -> 0.81\n",
            "Batch |  845 of 3770 loss ->  0.5354087352752686 acc -> 0.81\n",
            "Batch |  846 of 3770 loss ->  0.502138614654541 acc -> 0.81\n",
            "Batch |  847 of 3770 loss ->  0.5122247338294983 acc -> 0.81\n",
            "Batch |  848 of 3770 loss ->  0.5076937675476074 acc -> 0.81\n",
            "Batch |  849 of 3770 loss ->  0.5642634630203247 acc -> 0.81\n",
            "Batch |  850 of 3770 loss ->  0.5401719808578491 acc -> 0.81\n",
            "Batch |  851 of 3770 loss ->  0.5232341289520264 acc -> 0.81\n",
            "Batch |  852 of 3770 loss ->  0.5032477974891663 acc -> 0.81\n",
            "Batch |  853 of 3770 loss ->  0.5052534341812134 acc -> 0.81\n",
            "Batch |  854 of 3770 loss ->  0.5052279233932495 acc -> 0.81\n",
            "Batch |  855 of 3770 loss ->  0.527093768119812 acc -> 0.81\n",
            "Batch |  856 of 3770 loss ->  0.48759233951568604 acc -> 0.81\n",
            "Batch |  857 of 3770 loss ->  0.557257890701294 acc -> 0.81\n",
            "Batch |  858 of 3770 loss ->  0.5040514469146729 acc -> 0.81\n",
            "Batch |  859 of 3770 loss ->  0.46079209446907043 acc -> 0.81\n",
            "Batch |  860 of 3770 loss ->  0.49152201414108276 acc -> 0.81\n",
            "Batch |  861 of 3770 loss ->  0.47936901450157166 acc -> 0.81\n",
            "Batch |  862 of 3770 loss ->  0.5036974549293518 acc -> 0.81\n",
            "Batch |  863 of 3770 loss ->  0.4976445734500885 acc -> 0.81\n",
            "Batch |  864 of 3770 loss ->  0.5251023769378662 acc -> 0.81\n",
            "Batch |  865 of 3770 loss ->  0.4899098873138428 acc -> 0.81\n",
            "Batch |  866 of 3770 loss ->  0.5093352794647217 acc -> 0.81\n",
            "Batch |  867 of 3770 loss ->  0.5210863351821899 acc -> 0.81\n",
            "Batch |  868 of 3770 loss ->  0.487578809261322 acc -> 0.81\n",
            "Batch |  869 of 3770 loss ->  0.5012748837471008 acc -> 0.81\n",
            "Batch |  870 of 3770 loss ->  0.5538564920425415 acc -> 0.81\n",
            "Batch |  871 of 3770 loss ->  0.49696844816207886 acc -> 0.81\n",
            "Batch |  872 of 3770 loss ->  0.5202631950378418 acc -> 0.81\n",
            "Batch |  873 of 3770 loss ->  0.5268723964691162 acc -> 0.81\n",
            "Batch |  874 of 3770 loss ->  0.4852038025856018 acc -> 0.81\n",
            "Batch |  875 of 3770 loss ->  0.4650859832763672 acc -> 0.81\n",
            "Batch |  876 of 3770 loss ->  0.46835169196128845 acc -> 0.81\n",
            "Batch |  877 of 3770 loss ->  0.4792366921901703 acc -> 0.81\n",
            "Batch |  878 of 3770 loss ->  0.5163048505783081 acc -> 0.81\n",
            "Batch |  879 of 3770 loss ->  0.5034193992614746 acc -> 0.81\n",
            "Batch |  880 of 3770 loss ->  0.528538167476654 acc -> 0.81\n",
            "Batch |  881 of 3770 loss ->  0.5373573303222656 acc -> 0.81\n",
            "Batch |  882 of 3770 loss ->  0.483669638633728 acc -> 0.81\n",
            "Batch |  883 of 3770 loss ->  0.5156043767929077 acc -> 0.81\n",
            "Batch |  884 of 3770 loss ->  0.5013954043388367 acc -> 0.81\n",
            "Batch |  885 of 3770 loss ->  0.5005865693092346 acc -> 0.81\n",
            "Batch |  886 of 3770 loss ->  0.5169802904129028 acc -> 0.81\n",
            "Batch |  887 of 3770 loss ->  0.4754747748374939 acc -> 0.81\n",
            "Batch |  888 of 3770 loss ->  0.5076397657394409 acc -> 0.81\n",
            "Batch |  889 of 3770 loss ->  0.4705272912979126 acc -> 0.81\n",
            "Batch |  890 of 3770 loss ->  0.4971134662628174 acc -> 0.81\n",
            "Batch |  891 of 3770 loss ->  0.496446430683136 acc -> 0.81\n",
            "Batch |  892 of 3770 loss ->  0.5063799619674683 acc -> 0.81\n",
            "Batch |  893 of 3770 loss ->  0.4631890654563904 acc -> 0.81\n",
            "Batch |  894 of 3770 loss ->  0.5246071815490723 acc -> 0.81\n",
            "Batch |  895 of 3770 loss ->  0.5536706447601318 acc -> 0.81\n",
            "Batch |  896 of 3770 loss ->  0.5303338766098022 acc -> 0.81\n",
            "Batch |  897 of 3770 loss ->  0.4994772970676422 acc -> 0.81\n",
            "Batch |  898 of 3770 loss ->  0.46968692541122437 acc -> 0.81\n",
            "Batch |  899 of 3770 loss ->  0.5198908448219299 acc -> 0.81\n",
            "Batch |  900 of 3770 loss ->  0.5121694803237915 acc -> 0.81\n",
            "Batch |  901 of 3770 loss ->  0.46402785181999207 acc -> 0.81\n",
            "Batch |  902 of 3770 loss ->  0.5551179647445679 acc -> 0.81\n",
            "Batch |  903 of 3770 loss ->  0.5084725618362427 acc -> 0.81\n",
            "Batch |  904 of 3770 loss ->  0.5056010484695435 acc -> 0.81\n",
            "Batch |  905 of 3770 loss ->  0.5072358846664429 acc -> 0.81\n",
            "Batch |  906 of 3770 loss ->  0.4970722496509552 acc -> 0.81\n",
            "Batch |  907 of 3770 loss ->  0.48861944675445557 acc -> 0.81\n",
            "Batch |  908 of 3770 loss ->  0.5085346698760986 acc -> 0.81\n",
            "Batch |  909 of 3770 loss ->  0.5042887926101685 acc -> 0.81\n",
            "Batch |  910 of 3770 loss ->  0.4713486433029175 acc -> 0.81\n",
            "Batch |  911 of 3770 loss ->  0.48342466354370117 acc -> 0.81\n",
            "Batch |  912 of 3770 loss ->  0.5085542798042297 acc -> 0.81\n",
            "Batch |  913 of 3770 loss ->  0.49548837542533875 acc -> 0.81\n",
            "Batch |  914 of 3770 loss ->  0.5725644826889038 acc -> 0.81\n",
            "Batch |  915 of 3770 loss ->  0.5387880802154541 acc -> 0.81\n",
            "Batch |  916 of 3770 loss ->  0.5211718678474426 acc -> 0.81\n",
            "Batch |  917 of 3770 loss ->  0.4768146872520447 acc -> 0.81\n",
            "Batch |  918 of 3770 loss ->  0.49471238255500793 acc -> 0.81\n",
            "Batch |  919 of 3770 loss ->  0.4922115206718445 acc -> 0.81\n",
            "Batch |  920 of 3770 loss ->  0.5304667949676514 acc -> 0.81\n",
            "Batch |  921 of 3770 loss ->  0.4740724265575409 acc -> 0.81\n",
            "Batch |  922 of 3770 loss ->  0.47041118144989014 acc -> 0.81\n",
            "Batch |  923 of 3770 loss ->  0.45348885655403137 acc -> 0.81\n",
            "Batch |  924 of 3770 loss ->  0.510312557220459 acc -> 0.81\n",
            "Batch |  925 of 3770 loss ->  0.5191113352775574 acc -> 0.81\n",
            "Batch |  926 of 3770 loss ->  0.5208932757377625 acc -> 0.81\n",
            "Batch |  927 of 3770 loss ->  0.5301553010940552 acc -> 0.81\n",
            "Batch |  928 of 3770 loss ->  0.4888158440589905 acc -> 0.81\n",
            "Batch |  929 of 3770 loss ->  0.45775848627090454 acc -> 0.81\n",
            "Batch |  930 of 3770 loss ->  0.5515674352645874 acc -> 0.81\n",
            "Batch |  931 of 3770 loss ->  0.47951647639274597 acc -> 0.81\n",
            "Batch |  932 of 3770 loss ->  0.5349831581115723 acc -> 0.81\n",
            "Batch |  933 of 3770 loss ->  0.47899216413497925 acc -> 0.81\n",
            "Batch |  934 of 3770 loss ->  0.4982878565788269 acc -> 0.81\n",
            "Batch |  935 of 3770 loss ->  0.4831029176712036 acc -> 0.81\n",
            "Batch |  936 of 3770 loss ->  0.5130400061607361 acc -> 0.81\n",
            "Batch |  937 of 3770 loss ->  0.5123913884162903 acc -> 0.81\n",
            "Batch |  938 of 3770 loss ->  0.5237815380096436 acc -> 0.81\n",
            "Batch |  939 of 3770 loss ->  0.4988158643245697 acc -> 0.81\n",
            "Batch |  940 of 3770 loss ->  0.5097612142562866 acc -> 0.81\n",
            "Batch |  941 of 3770 loss ->  0.5078237056732178 acc -> 0.81\n",
            "Batch |  942 of 3770 loss ->  0.5213732719421387 acc -> 0.81\n",
            "Batch |  943 of 3770 loss ->  0.47073376178741455 acc -> 0.81\n",
            "Batch |  944 of 3770 loss ->  0.5124167203903198 acc -> 0.81\n",
            "Batch |  945 of 3770 loss ->  0.48409608006477356 acc -> 0.81\n",
            "Batch |  946 of 3770 loss ->  0.46718740463256836 acc -> 0.81\n",
            "Batch |  947 of 3770 loss ->  0.5071407556533813 acc -> 0.82\n",
            "Batch |  948 of 3770 loss ->  0.4783916771411896 acc -> 0.82\n",
            "Batch |  949 of 3770 loss ->  0.4792325496673584 acc -> 0.82\n",
            "Batch |  950 of 3770 loss ->  0.4800318777561188 acc -> 0.82\n",
            "Batch |  951 of 3770 loss ->  0.5533257722854614 acc -> 0.82\n",
            "Batch |  952 of 3770 loss ->  0.46459662914276123 acc -> 0.82\n",
            "Batch |  953 of 3770 loss ->  0.520905613899231 acc -> 0.82\n",
            "Batch |  954 of 3770 loss ->  0.5123692750930786 acc -> 0.82\n",
            "Batch |  955 of 3770 loss ->  0.5122876763343811 acc -> 0.82\n",
            "Batch |  956 of 3770 loss ->  0.47123682498931885 acc -> 0.82\n",
            "Batch |  957 of 3770 loss ->  0.5301125049591064 acc -> 0.82\n",
            "Batch |  958 of 3770 loss ->  0.5042049884796143 acc -> 0.82\n",
            "Batch |  959 of 3770 loss ->  0.49331364035606384 acc -> 0.82\n",
            "Batch |  960 of 3770 loss ->  0.4739381968975067 acc -> 0.82\n",
            "Batch |  961 of 3770 loss ->  0.449491024017334 acc -> 0.82\n",
            "Batch |  962 of 3770 loss ->  0.4982623755931854 acc -> 0.82\n",
            "Batch |  963 of 3770 loss ->  0.4383396506309509 acc -> 0.82\n",
            "Batch |  964 of 3770 loss ->  0.4614338278770447 acc -> 0.82\n",
            "Batch |  965 of 3770 loss ->  0.5013625621795654 acc -> 0.82\n",
            "Batch |  966 of 3770 loss ->  0.4729466140270233 acc -> 0.82\n",
            "Batch |  967 of 3770 loss ->  0.4646305441856384 acc -> 0.82\n",
            "Batch |  968 of 3770 loss ->  0.5118731260299683 acc -> 0.82\n",
            "Batch |  969 of 3770 loss ->  0.5035167932510376 acc -> 0.82\n",
            "Batch |  970 of 3770 loss ->  0.47343116998672485 acc -> 0.82\n",
            "Batch |  971 of 3770 loss ->  0.49967578053474426 acc -> 0.82\n",
            "Batch |  972 of 3770 loss ->  0.4866611957550049 acc -> 0.82\n",
            "Batch |  973 of 3770 loss ->  0.48643356561660767 acc -> 0.82\n",
            "Batch |  974 of 3770 loss ->  0.49166205525398254 acc -> 0.82\n",
            "Batch |  975 of 3770 loss ->  0.4847704768180847 acc -> 0.82\n",
            "Batch |  976 of 3770 loss ->  0.5137206315994263 acc -> 0.82\n",
            "Batch |  977 of 3770 loss ->  0.48213064670562744 acc -> 0.82\n",
            "Batch |  978 of 3770 loss ->  0.4520447552204132 acc -> 0.82\n",
            "Batch |  979 of 3770 loss ->  0.540959358215332 acc -> 0.82\n",
            "Batch |  980 of 3770 loss ->  0.47571370005607605 acc -> 0.82\n",
            "Batch |  981 of 3770 loss ->  0.46223756670951843 acc -> 0.82\n",
            "Batch |  982 of 3770 loss ->  0.5076364278793335 acc -> 0.82\n",
            "Batch |  983 of 3770 loss ->  0.46726301312446594 acc -> 0.82\n",
            "Batch |  984 of 3770 loss ->  0.4998973309993744 acc -> 0.82\n",
            "Batch |  985 of 3770 loss ->  0.507134199142456 acc -> 0.82\n",
            "Batch |  986 of 3770 loss ->  0.5048388838768005 acc -> 0.82\n",
            "Batch |  987 of 3770 loss ->  0.4639196991920471 acc -> 0.82\n",
            "Batch |  988 of 3770 loss ->  0.4916497468948364 acc -> 0.82\n",
            "Batch |  989 of 3770 loss ->  0.48842573165893555 acc -> 0.82\n",
            "Batch |  990 of 3770 loss ->  0.5071192979812622 acc -> 0.82\n",
            "Batch |  991 of 3770 loss ->  0.46494153141975403 acc -> 0.82\n",
            "Batch |  992 of 3770 loss ->  0.46525251865386963 acc -> 0.82\n",
            "Batch |  993 of 3770 loss ->  0.4707372188568115 acc -> 0.82\n",
            "Batch |  994 of 3770 loss ->  0.4888550043106079 acc -> 0.82\n",
            "Batch |  995 of 3770 loss ->  0.49479907751083374 acc -> 0.82\n",
            "Batch |  996 of 3770 loss ->  0.5137133598327637 acc -> 0.82\n",
            "Batch |  997 of 3770 loss ->  0.5203844904899597 acc -> 0.82\n",
            "Batch |  998 of 3770 loss ->  0.504097580909729 acc -> 0.82\n",
            "Batch |  999 of 3770 loss ->  0.49402734637260437 acc -> 0.82\n",
            "Batch |  1000 of 3770 loss ->  0.4935070276260376 acc -> 0.82\n",
            "Batch |  1001 of 3770 loss ->  0.4892827272415161 acc -> 0.82\n",
            "Batch |  1002 of 3770 loss ->  0.5014860033988953 acc -> 0.82\n",
            "Batch |  1003 of 3770 loss ->  0.47227030992507935 acc -> 0.82\n",
            "Batch |  1004 of 3770 loss ->  0.5406712293624878 acc -> 0.82\n",
            "Batch |  1005 of 3770 loss ->  0.47620099782943726 acc -> 0.82\n",
            "Batch |  1006 of 3770 loss ->  0.5067059993743896 acc -> 0.82\n",
            "Batch |  1007 of 3770 loss ->  0.4651898741722107 acc -> 0.82\n",
            "Batch |  1008 of 3770 loss ->  0.4650415778160095 acc -> 0.82\n",
            "Batch |  1009 of 3770 loss ->  0.5105248689651489 acc -> 0.82\n",
            "Batch |  1010 of 3770 loss ->  0.4658474624156952 acc -> 0.82\n",
            "Batch |  1011 of 3770 loss ->  0.4685143232345581 acc -> 0.82\n",
            "Batch |  1012 of 3770 loss ->  0.4952831566333771 acc -> 0.82\n",
            "Batch |  1013 of 3770 loss ->  0.4929911196231842 acc -> 0.82\n",
            "Batch |  1014 of 3770 loss ->  0.5006951093673706 acc -> 0.82\n",
            "Batch |  1015 of 3770 loss ->  0.4704076051712036 acc -> 0.82\n",
            "Batch |  1016 of 3770 loss ->  0.505462646484375 acc -> 0.82\n",
            "Batch |  1017 of 3770 loss ->  0.5251153111457825 acc -> 0.82\n",
            "Batch |  1018 of 3770 loss ->  0.48745930194854736 acc -> 0.82\n",
            "Batch |  1019 of 3770 loss ->  0.4955616891384125 acc -> 0.82\n",
            "Batch |  1020 of 3770 loss ->  0.4975162148475647 acc -> 0.82\n",
            "Batch |  1021 of 3770 loss ->  0.48351356387138367 acc -> 0.82\n",
            "Batch |  1022 of 3770 loss ->  0.45373082160949707 acc -> 0.82\n",
            "Batch |  1023 of 3770 loss ->  0.4845171272754669 acc -> 0.82\n",
            "Batch |  1024 of 3770 loss ->  0.4919603168964386 acc -> 0.82\n",
            "Batch |  1025 of 3770 loss ->  0.5024108290672302 acc -> 0.82\n",
            "Batch |  1026 of 3770 loss ->  0.5124903917312622 acc -> 0.82\n",
            "Batch |  1027 of 3770 loss ->  0.4614236056804657 acc -> 0.82\n",
            "Batch |  1028 of 3770 loss ->  0.4852072298526764 acc -> 0.82\n",
            "Batch |  1029 of 3770 loss ->  0.4813152551651001 acc -> 0.82\n",
            "Batch |  1030 of 3770 loss ->  0.48881083726882935 acc -> 0.82\n",
            "Batch |  1031 of 3770 loss ->  0.4564052224159241 acc -> 0.82\n",
            "Batch |  1032 of 3770 loss ->  0.4914472997188568 acc -> 0.82\n",
            "Batch |  1033 of 3770 loss ->  0.4820975363254547 acc -> 0.82\n",
            "Batch |  1034 of 3770 loss ->  0.49712827801704407 acc -> 0.82\n",
            "Batch |  1035 of 3770 loss ->  0.46636566519737244 acc -> 0.82\n",
            "Batch |  1036 of 3770 loss ->  0.4853403568267822 acc -> 0.82\n",
            "Batch |  1037 of 3770 loss ->  0.5170195698738098 acc -> 0.82\n",
            "Batch |  1038 of 3770 loss ->  0.48026010394096375 acc -> 0.82\n",
            "Batch |  1039 of 3770 loss ->  0.4608737826347351 acc -> 0.82\n",
            "Batch |  1040 of 3770 loss ->  0.4734244644641876 acc -> 0.82\n",
            "Batch |  1041 of 3770 loss ->  0.4976828396320343 acc -> 0.82\n",
            "Batch |  1042 of 3770 loss ->  0.49435895681381226 acc -> 0.82\n",
            "Batch |  1043 of 3770 loss ->  0.532416045665741 acc -> 0.82\n",
            "Batch |  1044 of 3770 loss ->  0.5378146767616272 acc -> 0.82\n",
            "Batch |  1045 of 3770 loss ->  0.5084596872329712 acc -> 0.82\n",
            "Batch |  1046 of 3770 loss ->  0.4576926827430725 acc -> 0.82\n",
            "Batch |  1047 of 3770 loss ->  0.46298593282699585 acc -> 0.82\n",
            "Batch |  1048 of 3770 loss ->  0.5009914636611938 acc -> 0.82\n",
            "Batch |  1049 of 3770 loss ->  0.4853721261024475 acc -> 0.82\n",
            "Batch |  1050 of 3770 loss ->  0.48899221420288086 acc -> 0.82\n",
            "Batch |  1051 of 3770 loss ->  0.4577258825302124 acc -> 0.82\n",
            "Batch |  1052 of 3770 loss ->  0.4335852861404419 acc -> 0.82\n",
            "Batch |  1053 of 3770 loss ->  0.45702415704727173 acc -> 0.82\n",
            "Batch |  1054 of 3770 loss ->  0.4815228581428528 acc -> 0.82\n",
            "Batch |  1055 of 3770 loss ->  0.48319369554519653 acc -> 0.82\n",
            "Batch |  1056 of 3770 loss ->  0.4897948205471039 acc -> 0.82\n",
            "Batch |  1057 of 3770 loss ->  0.48263365030288696 acc -> 0.82\n",
            "Batch |  1058 of 3770 loss ->  0.5288492441177368 acc -> 0.82\n",
            "Batch |  1059 of 3770 loss ->  0.4871506094932556 acc -> 0.82\n",
            "Batch |  1060 of 3770 loss ->  0.4883783757686615 acc -> 0.82\n",
            "Batch |  1061 of 3770 loss ->  0.48183751106262207 acc -> 0.83\n",
            "Batch |  1062 of 3770 loss ->  0.48232099413871765 acc -> 0.83\n",
            "Batch |  1063 of 3770 loss ->  0.5021975040435791 acc -> 0.83\n",
            "Batch |  1064 of 3770 loss ->  0.5134881734848022 acc -> 0.83\n",
            "Batch |  1065 of 3770 loss ->  0.47193893790245056 acc -> 0.83\n",
            "Batch |  1066 of 3770 loss ->  0.4816740155220032 acc -> 0.83\n",
            "Batch |  1067 of 3770 loss ->  0.5243844985961914 acc -> 0.83\n",
            "Batch |  1068 of 3770 loss ->  0.5149803161621094 acc -> 0.83\n",
            "Batch |  1069 of 3770 loss ->  0.46930521726608276 acc -> 0.83\n",
            "Batch |  1070 of 3770 loss ->  0.4901362955570221 acc -> 0.83\n",
            "Batch |  1071 of 3770 loss ->  0.5042459964752197 acc -> 0.83\n",
            "Batch |  1072 of 3770 loss ->  0.4674987196922302 acc -> 0.83\n",
            "Batch |  1073 of 3770 loss ->  0.4652649164199829 acc -> 0.83\n",
            "Batch |  1074 of 3770 loss ->  0.4826473295688629 acc -> 0.83\n",
            "Batch |  1075 of 3770 loss ->  0.4943452477455139 acc -> 0.83\n",
            "Batch |  1076 of 3770 loss ->  0.4670841693878174 acc -> 0.83\n",
            "Batch |  1077 of 3770 loss ->  0.4997733235359192 acc -> 0.83\n",
            "Batch |  1078 of 3770 loss ->  0.5327220559120178 acc -> 0.83\n",
            "Batch |  1079 of 3770 loss ->  0.4843590259552002 acc -> 0.83\n",
            "Batch |  1080 of 3770 loss ->  0.49323374032974243 acc -> 0.83\n",
            "Batch |  1081 of 3770 loss ->  0.5073955655097961 acc -> 0.83\n",
            "Batch |  1082 of 3770 loss ->  0.4968094229698181 acc -> 0.83\n",
            "Batch |  1083 of 3770 loss ->  0.4822902977466583 acc -> 0.83\n",
            "Batch |  1084 of 3770 loss ->  0.4733925461769104 acc -> 0.83\n",
            "Batch |  1085 of 3770 loss ->  0.5189346075057983 acc -> 0.83\n",
            "Batch |  1086 of 3770 loss ->  0.472009539604187 acc -> 0.83\n",
            "Batch |  1087 of 3770 loss ->  0.5233938694000244 acc -> 0.83\n",
            "Batch |  1088 of 3770 loss ->  0.45994460582733154 acc -> 0.83\n",
            "Batch |  1089 of 3770 loss ->  0.4542767405509949 acc -> 0.83\n",
            "Batch |  1090 of 3770 loss ->  0.45300188660621643 acc -> 0.83\n",
            "Batch |  1091 of 3770 loss ->  0.45267659425735474 acc -> 0.83\n",
            "Batch |  1092 of 3770 loss ->  0.4960642457008362 acc -> 0.83\n",
            "Batch |  1093 of 3770 loss ->  0.47774630784988403 acc -> 0.83\n",
            "Batch |  1094 of 3770 loss ->  0.4956640601158142 acc -> 0.83\n",
            "Batch |  1095 of 3770 loss ->  0.5019097328186035 acc -> 0.83\n",
            "Batch |  1096 of 3770 loss ->  0.4786146879196167 acc -> 0.83\n",
            "Batch |  1097 of 3770 loss ->  0.5126393437385559 acc -> 0.83\n",
            "Batch |  1098 of 3770 loss ->  0.46006518602371216 acc -> 0.83\n",
            "Batch |  1099 of 3770 loss ->  0.4648264944553375 acc -> 0.83\n",
            "Batch |  1100 of 3770 loss ->  0.4800727367401123 acc -> 0.83\n",
            "Batch |  1101 of 3770 loss ->  0.49911898374557495 acc -> 0.83\n",
            "Batch |  1102 of 3770 loss ->  0.5800270438194275 acc -> 0.83\n",
            "Batch |  1103 of 3770 loss ->  0.5083565711975098 acc -> 0.83\n",
            "Batch |  1104 of 3770 loss ->  0.47646665573120117 acc -> 0.83\n",
            "Batch |  1105 of 3770 loss ->  0.5040745139122009 acc -> 0.83\n",
            "Batch |  1106 of 3770 loss ->  0.47861945629119873 acc -> 0.83\n",
            "Batch |  1107 of 3770 loss ->  0.5059648752212524 acc -> 0.83\n",
            "Batch |  1108 of 3770 loss ->  0.46029895544052124 acc -> 0.83\n",
            "Batch |  1109 of 3770 loss ->  0.47350430488586426 acc -> 0.83\n",
            "Batch |  1110 of 3770 loss ->  0.475929319858551 acc -> 0.83\n",
            "Batch |  1111 of 3770 loss ->  0.4578152298927307 acc -> 0.83\n",
            "Batch |  1112 of 3770 loss ->  0.4962192475795746 acc -> 0.83\n",
            "Batch |  1113 of 3770 loss ->  0.4772837460041046 acc -> 0.83\n",
            "Batch |  1114 of 3770 loss ->  0.4739633798599243 acc -> 0.83\n",
            "Batch |  1115 of 3770 loss ->  0.5143308043479919 acc -> 0.83\n",
            "Batch |  1116 of 3770 loss ->  0.4817635118961334 acc -> 0.83\n",
            "Batch |  1117 of 3770 loss ->  0.46124306321144104 acc -> 0.83\n",
            "Batch |  1118 of 3770 loss ->  0.483903706073761 acc -> 0.83\n",
            "Batch |  1119 of 3770 loss ->  0.43480321764945984 acc -> 0.83\n",
            "Batch |  1120 of 3770 loss ->  0.5111492872238159 acc -> 0.83\n",
            "Batch |  1121 of 3770 loss ->  0.4533884525299072 acc -> 0.83\n",
            "Batch |  1122 of 3770 loss ->  0.5011046528816223 acc -> 0.83\n",
            "Batch |  1123 of 3770 loss ->  0.5181080102920532 acc -> 0.83\n",
            "Batch |  1124 of 3770 loss ->  0.5389370918273926 acc -> 0.83\n",
            "Batch |  1125 of 3770 loss ->  0.4803192615509033 acc -> 0.83\n",
            "Batch |  1126 of 3770 loss ->  0.4557517170906067 acc -> 0.83\n",
            "Batch |  1127 of 3770 loss ->  0.4666098356246948 acc -> 0.83\n",
            "Batch |  1128 of 3770 loss ->  0.485268771648407 acc -> 0.83\n",
            "Batch |  1129 of 3770 loss ->  0.5065656304359436 acc -> 0.83\n",
            "Batch |  1130 of 3770 loss ->  0.4702969193458557 acc -> 0.83\n",
            "Batch |  1131 of 3770 loss ->  0.4840061068534851 acc -> 0.83\n",
            "Batch |  1132 of 3770 loss ->  0.4742288589477539 acc -> 0.83\n",
            "Batch |  1133 of 3770 loss ->  0.5000625848770142 acc -> 0.83\n",
            "Batch |  1134 of 3770 loss ->  0.5484026670455933 acc -> 0.83\n",
            "Batch |  1135 of 3770 loss ->  0.48094478249549866 acc -> 0.83\n",
            "Batch |  1136 of 3770 loss ->  0.47670799493789673 acc -> 0.83\n",
            "Batch |  1137 of 3770 loss ->  0.48447155952453613 acc -> 0.83\n",
            "Batch |  1138 of 3770 loss ->  0.4865518808364868 acc -> 0.83\n",
            "Batch |  1139 of 3770 loss ->  0.4824187159538269 acc -> 0.83\n",
            "Batch |  1140 of 3770 loss ->  0.5024587512016296 acc -> 0.83\n",
            "Batch |  1141 of 3770 loss ->  0.49838414788246155 acc -> 0.83\n",
            "Batch |  1142 of 3770 loss ->  0.5535309314727783 acc -> 0.83\n",
            "Batch |  1143 of 3770 loss ->  0.45719411969184875 acc -> 0.83\n",
            "Batch |  1144 of 3770 loss ->  0.4842069745063782 acc -> 0.83\n",
            "Batch |  1145 of 3770 loss ->  0.45643794536590576 acc -> 0.83\n",
            "Batch |  1146 of 3770 loss ->  0.5313152074813843 acc -> 0.83\n",
            "Batch |  1147 of 3770 loss ->  0.47463974356651306 acc -> 0.83\n",
            "Batch |  1148 of 3770 loss ->  0.5077536106109619 acc -> 0.83\n",
            "Batch |  1149 of 3770 loss ->  0.5176600217819214 acc -> 0.83\n",
            "Batch |  1150 of 3770 loss ->  0.4807869493961334 acc -> 0.83\n",
            "Batch |  1151 of 3770 loss ->  0.4939875602722168 acc -> 0.83\n",
            "Batch |  1152 of 3770 loss ->  0.48366039991378784 acc -> 0.83\n",
            "Batch |  1153 of 3770 loss ->  0.4636874198913574 acc -> 0.83\n",
            "Batch |  1154 of 3770 loss ->  0.45043668150901794 acc -> 0.83\n",
            "Batch |  1155 of 3770 loss ->  0.514074444770813 acc -> 0.83\n",
            "Batch |  1156 of 3770 loss ->  0.4718453884124756 acc -> 0.83\n",
            "Batch |  1157 of 3770 loss ->  0.4364503026008606 acc -> 0.83\n",
            "Batch |  1158 of 3770 loss ->  0.4724676012992859 acc -> 0.83\n",
            "Batch |  1159 of 3770 loss ->  0.48139774799346924 acc -> 0.83\n",
            "Batch |  1160 of 3770 loss ->  0.4835434556007385 acc -> 0.83\n",
            "Batch |  1161 of 3770 loss ->  0.47983190417289734 acc -> 0.83\n",
            "Batch |  1162 of 3770 loss ->  0.473351389169693 acc -> 0.83\n",
            "Batch |  1163 of 3770 loss ->  0.4811740517616272 acc -> 0.83\n",
            "Batch |  1164 of 3770 loss ->  0.44401076436042786 acc -> 0.83\n",
            "Batch |  1165 of 3770 loss ->  0.4467305541038513 acc -> 0.83\n",
            "Batch |  1166 of 3770 loss ->  0.49567967653274536 acc -> 0.83\n",
            "Batch |  1167 of 3770 loss ->  0.48610129952430725 acc -> 0.83\n",
            "Batch |  1168 of 3770 loss ->  0.45837825536727905 acc -> 0.83\n",
            "Batch |  1169 of 3770 loss ->  0.48725229501724243 acc -> 0.83\n",
            "Batch |  1170 of 3770 loss ->  0.431109219789505 acc -> 0.83\n",
            "Batch |  1171 of 3770 loss ->  0.4848281741142273 acc -> 0.83\n",
            "Batch |  1172 of 3770 loss ->  0.4779960811138153 acc -> 0.83\n",
            "Batch |  1173 of 3770 loss ->  0.4768208861351013 acc -> 0.83\n",
            "Batch |  1174 of 3770 loss ->  0.49786272644996643 acc -> 0.83\n",
            "Batch |  1175 of 3770 loss ->  0.48905646800994873 acc -> 0.83\n",
            "Batch |  1176 of 3770 loss ->  0.443166047334671 acc -> 0.83\n",
            "Batch |  1177 of 3770 loss ->  0.4647088944911957 acc -> 0.83\n",
            "Batch |  1178 of 3770 loss ->  0.5191106796264648 acc -> 0.83\n",
            "Batch |  1179 of 3770 loss ->  0.44499319791793823 acc -> 0.83\n",
            "Batch |  1180 of 3770 loss ->  0.47094783186912537 acc -> 0.83\n",
            "Batch |  1181 of 3770 loss ->  0.48668885231018066 acc -> 0.83\n",
            "Batch |  1182 of 3770 loss ->  0.4635860323905945 acc -> 0.83\n",
            "Batch |  1183 of 3770 loss ->  0.4391840696334839 acc -> 0.83\n",
            "Batch |  1184 of 3770 loss ->  0.4650624990463257 acc -> 0.83\n",
            "Batch |  1185 of 3770 loss ->  0.4910905957221985 acc -> 0.83\n",
            "Batch |  1186 of 3770 loss ->  0.5250958800315857 acc -> 0.83\n",
            "Batch |  1187 of 3770 loss ->  0.4477795958518982 acc -> 0.83\n",
            "Batch |  1188 of 3770 loss ->  0.4392291307449341 acc -> 0.83\n",
            "Batch |  1189 of 3770 loss ->  0.44792741537094116 acc -> 0.83\n",
            "Batch |  1190 of 3770 loss ->  0.5592178106307983 acc -> 0.83\n",
            "Batch |  1191 of 3770 loss ->  0.486152321100235 acc -> 0.83\n",
            "Batch |  1192 of 3770 loss ->  0.47082754969596863 acc -> 0.83\n",
            "Batch |  1193 of 3770 loss ->  0.457021564245224 acc -> 0.83\n",
            "Batch |  1194 of 3770 loss ->  0.48172318935394287 acc -> 0.83\n",
            "Batch |  1195 of 3770 loss ->  0.5084317922592163 acc -> 0.83\n",
            "Batch |  1196 of 3770 loss ->  0.47367334365844727 acc -> 0.83\n",
            "Batch |  1197 of 3770 loss ->  0.4910343289375305 acc -> 0.83\n",
            "Batch |  1198 of 3770 loss ->  0.4870508909225464 acc -> 0.83\n",
            "Batch |  1199 of 3770 loss ->  0.5036638975143433 acc -> 0.83\n",
            "Batch |  1200 of 3770 loss ->  0.48083779215812683 acc -> 0.83\n",
            "Batch |  1201 of 3770 loss ->  0.4508766233921051 acc -> 0.83\n",
            "Batch |  1202 of 3770 loss ->  0.46747612953186035 acc -> 0.83\n",
            "Batch |  1203 of 3770 loss ->  0.5124683380126953 acc -> 0.83\n",
            "Batch |  1204 of 3770 loss ->  0.4985933303833008 acc -> 0.83\n",
            "Batch |  1205 of 3770 loss ->  0.49853867292404175 acc -> 0.83\n",
            "Batch |  1206 of 3770 loss ->  0.4781988859176636 acc -> 0.83\n",
            "Batch |  1207 of 3770 loss ->  0.5018084645271301 acc -> 0.83\n",
            "Batch |  1208 of 3770 loss ->  0.49855107069015503 acc -> 0.83\n",
            "Batch |  1209 of 3770 loss ->  0.4387434422969818 acc -> 0.83\n",
            "Batch |  1210 of 3770 loss ->  0.4884330928325653 acc -> 0.83\n",
            "Batch |  1211 of 3770 loss ->  0.4732537567615509 acc -> 0.83\n",
            "Batch |  1212 of 3770 loss ->  0.46233683824539185 acc -> 0.84\n",
            "Batch |  1213 of 3770 loss ->  0.4427022635936737 acc -> 0.84\n",
            "Batch |  1214 of 3770 loss ->  0.4957531690597534 acc -> 0.84\n",
            "Batch |  1215 of 3770 loss ->  0.4894435703754425 acc -> 0.84\n",
            "Batch |  1216 of 3770 loss ->  0.5284860134124756 acc -> 0.84\n",
            "Batch |  1217 of 3770 loss ->  0.519176721572876 acc -> 0.84\n",
            "Batch |  1218 of 3770 loss ->  0.47893622517585754 acc -> 0.84\n",
            "Batch |  1219 of 3770 loss ->  0.49245187640190125 acc -> 0.84\n",
            "Batch |  1220 of 3770 loss ->  0.47714149951934814 acc -> 0.84\n",
            "Batch |  1221 of 3770 loss ->  0.4806232452392578 acc -> 0.84\n",
            "Batch |  1222 of 3770 loss ->  0.4790923595428467 acc -> 0.84\n",
            "Batch |  1223 of 3770 loss ->  0.5009139776229858 acc -> 0.84\n",
            "Batch |  1224 of 3770 loss ->  0.4857485890388489 acc -> 0.84\n",
            "Batch |  1225 of 3770 loss ->  0.47618159651756287 acc -> 0.84\n",
            "Batch |  1226 of 3770 loss ->  0.4623858332633972 acc -> 0.84\n",
            "Batch |  1227 of 3770 loss ->  0.4679000973701477 acc -> 0.84\n",
            "Batch |  1228 of 3770 loss ->  0.43373844027519226 acc -> 0.84\n",
            "Batch |  1229 of 3770 loss ->  0.5123080611228943 acc -> 0.84\n",
            "Batch |  1230 of 3770 loss ->  0.45095551013946533 acc -> 0.84\n",
            "Batch |  1231 of 3770 loss ->  0.5036524534225464 acc -> 0.84\n",
            "Batch |  1232 of 3770 loss ->  0.46271824836730957 acc -> 0.84\n",
            "Batch |  1233 of 3770 loss ->  0.4603315591812134 acc -> 0.84\n",
            "Batch |  1234 of 3770 loss ->  0.4519009292125702 acc -> 0.84\n",
            "Batch |  1235 of 3770 loss ->  0.4797285199165344 acc -> 0.84\n",
            "Batch |  1236 of 3770 loss ->  0.4490050673484802 acc -> 0.84\n",
            "Batch |  1237 of 3770 loss ->  0.5114431381225586 acc -> 0.84\n",
            "Batch |  1238 of 3770 loss ->  0.46450233459472656 acc -> 0.84\n",
            "Batch |  1239 of 3770 loss ->  0.46930328011512756 acc -> 0.84\n",
            "Batch |  1240 of 3770 loss ->  0.4914191663265228 acc -> 0.84\n",
            "Batch |  1241 of 3770 loss ->  0.4724971055984497 acc -> 0.84\n",
            "Batch |  1242 of 3770 loss ->  0.4451260566711426 acc -> 0.84\n",
            "Batch |  1243 of 3770 loss ->  0.4575868844985962 acc -> 0.84\n",
            "Batch |  1244 of 3770 loss ->  0.4507191777229309 acc -> 0.84\n",
            "Batch |  1245 of 3770 loss ->  0.49576181173324585 acc -> 0.84\n",
            "Batch |  1246 of 3770 loss ->  0.47234049439430237 acc -> 0.84\n",
            "Batch |  1247 of 3770 loss ->  0.44634121656417847 acc -> 0.84\n",
            "Batch |  1248 of 3770 loss ->  0.4486709237098694 acc -> 0.84\n",
            "Batch |  1249 of 3770 loss ->  0.486824631690979 acc -> 0.84\n",
            "Batch |  1250 of 3770 loss ->  0.4629986882209778 acc -> 0.84\n",
            "Batch |  1251 of 3770 loss ->  0.4592703580856323 acc -> 0.84\n",
            "Batch |  1252 of 3770 loss ->  0.43813443183898926 acc -> 0.84\n",
            "Batch |  1253 of 3770 loss ->  0.46013763546943665 acc -> 0.84\n",
            "Batch |  1254 of 3770 loss ->  0.45665645599365234 acc -> 0.84\n",
            "Batch |  1255 of 3770 loss ->  0.46901142597198486 acc -> 0.84\n",
            "Batch |  1256 of 3770 loss ->  0.4525264799594879 acc -> 0.84\n",
            "Batch |  1257 of 3770 loss ->  0.4586194157600403 acc -> 0.84\n",
            "Batch |  1258 of 3770 loss ->  0.4491199851036072 acc -> 0.84\n",
            "Batch |  1259 of 3770 loss ->  0.4554486870765686 acc -> 0.84\n",
            "Batch |  1260 of 3770 loss ->  0.4388674795627594 acc -> 0.84\n",
            "Batch |  1261 of 3770 loss ->  0.4768451452255249 acc -> 0.84\n",
            "Batch |  1262 of 3770 loss ->  0.5150536298751831 acc -> 0.84\n",
            "Batch |  1263 of 3770 loss ->  0.4545915722846985 acc -> 0.84\n",
            "Batch |  1264 of 3770 loss ->  0.45483678579330444 acc -> 0.84\n",
            "Batch |  1265 of 3770 loss ->  0.4685507118701935 acc -> 0.84\n",
            "Batch |  1266 of 3770 loss ->  0.4894903302192688 acc -> 0.84\n",
            "Batch |  1267 of 3770 loss ->  0.474637508392334 acc -> 0.84\n",
            "Batch |  1268 of 3770 loss ->  0.42516154050827026 acc -> 0.84\n",
            "Batch |  1269 of 3770 loss ->  0.43260636925697327 acc -> 0.84\n",
            "Batch |  1270 of 3770 loss ->  0.5009324550628662 acc -> 0.84\n",
            "Batch |  1271 of 3770 loss ->  0.49214228987693787 acc -> 0.84\n",
            "Batch |  1272 of 3770 loss ->  0.4545340836048126 acc -> 0.84\n",
            "Batch |  1273 of 3770 loss ->  0.44518783688545227 acc -> 0.84\n",
            "Batch |  1274 of 3770 loss ->  0.4492582678794861 acc -> 0.84\n",
            "Batch |  1275 of 3770 loss ->  0.4931052029132843 acc -> 0.84\n",
            "Batch |  1276 of 3770 loss ->  0.4880285859107971 acc -> 0.84\n",
            "Batch |  1277 of 3770 loss ->  0.4479282796382904 acc -> 0.84\n",
            "Batch |  1278 of 3770 loss ->  0.4477497935295105 acc -> 0.84\n",
            "Batch |  1279 of 3770 loss ->  0.4676705598831177 acc -> 0.84\n",
            "Batch |  1280 of 3770 loss ->  0.4912700653076172 acc -> 0.84\n",
            "Batch |  1281 of 3770 loss ->  0.43792593479156494 acc -> 0.84\n",
            "Batch |  1282 of 3770 loss ->  0.43403464555740356 acc -> 0.84\n",
            "Batch |  1283 of 3770 loss ->  0.45326393842697144 acc -> 0.84\n",
            "Batch |  1284 of 3770 loss ->  0.4431830942630768 acc -> 0.84\n",
            "Batch |  1285 of 3770 loss ->  0.48023056983947754 acc -> 0.84\n",
            "Batch |  1286 of 3770 loss ->  0.47740668058395386 acc -> 0.84\n",
            "Batch |  1287 of 3770 loss ->  0.449179083108902 acc -> 0.84\n",
            "Batch |  1288 of 3770 loss ->  0.4768434166908264 acc -> 0.84\n",
            "Batch |  1289 of 3770 loss ->  0.45020946860313416 acc -> 0.84\n",
            "Batch |  1290 of 3770 loss ->  0.48750725388526917 acc -> 0.84\n",
            "Batch |  1291 of 3770 loss ->  0.4775255024433136 acc -> 0.84\n",
            "Batch |  1292 of 3770 loss ->  0.4731447994709015 acc -> 0.84\n",
            "Batch |  1293 of 3770 loss ->  0.46555808186531067 acc -> 0.84\n",
            "Batch |  1294 of 3770 loss ->  0.4311069846153259 acc -> 0.84\n",
            "Batch |  1295 of 3770 loss ->  0.475027859210968 acc -> 0.84\n",
            "Batch |  1296 of 3770 loss ->  0.42421239614486694 acc -> 0.84\n",
            "Batch |  1297 of 3770 loss ->  0.4638744592666626 acc -> 0.84\n",
            "Batch |  1298 of 3770 loss ->  0.44828730821609497 acc -> 0.84\n",
            "Batch |  1299 of 3770 loss ->  0.48196208477020264 acc -> 0.84\n",
            "Batch |  1300 of 3770 loss ->  0.4867669641971588 acc -> 0.84\n",
            "Batch |  1301 of 3770 loss ->  0.4968957304954529 acc -> 0.84\n",
            "Batch |  1302 of 3770 loss ->  0.4762994349002838 acc -> 0.84\n",
            "Batch |  1303 of 3770 loss ->  0.45109474658966064 acc -> 0.84\n",
            "Batch |  1304 of 3770 loss ->  0.4428141713142395 acc -> 0.84\n",
            "Batch |  1305 of 3770 loss ->  0.45641446113586426 acc -> 0.84\n",
            "Batch |  1306 of 3770 loss ->  0.48678141832351685 acc -> 0.84\n",
            "Batch |  1307 of 3770 loss ->  0.4509947597980499 acc -> 0.84\n",
            "Batch |  1308 of 3770 loss ->  0.4314534068107605 acc -> 0.84\n",
            "Batch |  1309 of 3770 loss ->  0.4916619062423706 acc -> 0.84\n",
            "Batch |  1310 of 3770 loss ->  0.4758860766887665 acc -> 0.84\n",
            "Batch |  1311 of 3770 loss ->  0.4372631311416626 acc -> 0.84\n",
            "Batch |  1312 of 3770 loss ->  0.4441104531288147 acc -> 0.84\n",
            "Batch |  1313 of 3770 loss ->  0.47254449129104614 acc -> 0.84\n",
            "Batch |  1314 of 3770 loss ->  0.4602506160736084 acc -> 0.84\n",
            "Batch |  1315 of 3770 loss ->  0.49138277769088745 acc -> 0.84\n",
            "Batch |  1316 of 3770 loss ->  0.4961986541748047 acc -> 0.84\n",
            "Batch |  1317 of 3770 loss ->  0.49655410647392273 acc -> 0.84\n",
            "Batch |  1318 of 3770 loss ->  0.46870899200439453 acc -> 0.84\n",
            "Batch |  1319 of 3770 loss ->  0.473998099565506 acc -> 0.84\n",
            "Batch |  1320 of 3770 loss ->  0.45982450246810913 acc -> 0.84\n",
            "Batch |  1321 of 3770 loss ->  0.48990121483802795 acc -> 0.84\n",
            "Batch |  1322 of 3770 loss ->  0.46086955070495605 acc -> 0.84\n",
            "Batch |  1323 of 3770 loss ->  0.478737473487854 acc -> 0.84\n",
            "Batch |  1324 of 3770 loss ->  0.46081066131591797 acc -> 0.84\n",
            "Batch |  1325 of 3770 loss ->  0.4373137354850769 acc -> 0.84\n",
            "Batch |  1326 of 3770 loss ->  0.4697672128677368 acc -> 0.84\n",
            "Batch |  1327 of 3770 loss ->  0.42769455909729004 acc -> 0.84\n",
            "Batch |  1328 of 3770 loss ->  0.47062748670578003 acc -> 0.84\n",
            "Batch |  1329 of 3770 loss ->  0.44934314489364624 acc -> 0.84\n",
            "Batch |  1330 of 3770 loss ->  0.48869985342025757 acc -> 0.84\n",
            "Batch |  1331 of 3770 loss ->  0.5324721932411194 acc -> 0.84\n",
            "Batch |  1332 of 3770 loss ->  0.4820127785205841 acc -> 0.84\n",
            "Batch |  1333 of 3770 loss ->  0.4381980001926422 acc -> 0.84\n",
            "Batch |  1334 of 3770 loss ->  0.46180111169815063 acc -> 0.84\n",
            "Batch |  1335 of 3770 loss ->  0.4390583038330078 acc -> 0.84\n",
            "Batch |  1336 of 3770 loss ->  0.4547644257545471 acc -> 0.84\n",
            "Batch |  1337 of 3770 loss ->  0.4734288454055786 acc -> 0.84\n",
            "Batch |  1338 of 3770 loss ->  0.4345998167991638 acc -> 0.84\n",
            "Batch |  1339 of 3770 loss ->  0.4602167010307312 acc -> 0.84\n",
            "Batch |  1340 of 3770 loss ->  0.46308356523513794 acc -> 0.84\n",
            "Batch |  1341 of 3770 loss ->  0.44333499670028687 acc -> 0.84\n",
            "Batch |  1342 of 3770 loss ->  0.4583764970302582 acc -> 0.84\n",
            "Batch |  1343 of 3770 loss ->  0.49604108929634094 acc -> 0.84\n",
            "Batch |  1344 of 3770 loss ->  0.5131393074989319 acc -> 0.84\n",
            "Batch |  1345 of 3770 loss ->  0.4642995297908783 acc -> 0.84\n",
            "Batch |  1346 of 3770 loss ->  0.46428823471069336 acc -> 0.84\n",
            "Batch |  1347 of 3770 loss ->  0.4750699996948242 acc -> 0.84\n",
            "Batch |  1348 of 3770 loss ->  0.4527018368244171 acc -> 0.84\n",
            "Batch |  1349 of 3770 loss ->  0.45692747831344604 acc -> 0.84\n",
            "Batch |  1350 of 3770 loss ->  0.47825077176094055 acc -> 0.84\n",
            "Batch |  1351 of 3770 loss ->  0.42989611625671387 acc -> 0.84\n",
            "Batch |  1352 of 3770 loss ->  0.4496220350265503 acc -> 0.84\n",
            "Batch |  1353 of 3770 loss ->  0.45902955532073975 acc -> 0.84\n",
            "Batch |  1354 of 3770 loss ->  0.4441673159599304 acc -> 0.84\n",
            "Batch |  1355 of 3770 loss ->  0.41632792353630066 acc -> 0.84\n",
            "Batch |  1356 of 3770 loss ->  0.4697380065917969 acc -> 0.84\n",
            "Batch |  1357 of 3770 loss ->  0.4470002353191376 acc -> 0.84\n",
            "Batch |  1358 of 3770 loss ->  0.4660898745059967 acc -> 0.84\n",
            "Batch |  1359 of 3770 loss ->  0.4165666103363037 acc -> 0.84\n",
            "Batch |  1360 of 3770 loss ->  0.4748644232749939 acc -> 0.84\n",
            "Batch |  1361 of 3770 loss ->  0.4442735016345978 acc -> 0.84\n",
            "Batch |  1362 of 3770 loss ->  0.45952391624450684 acc -> 0.84\n",
            "Batch |  1363 of 3770 loss ->  0.43376249074935913 acc -> 0.84\n",
            "Batch |  1364 of 3770 loss ->  0.4211839735507965 acc -> 0.85\n",
            "Batch |  1365 of 3770 loss ->  0.443275511264801 acc -> 0.85\n",
            "Batch |  1366 of 3770 loss ->  0.5063896179199219 acc -> 0.85\n",
            "Batch |  1367 of 3770 loss ->  0.4746621251106262 acc -> 0.85\n",
            "Batch |  1368 of 3770 loss ->  0.45802807807922363 acc -> 0.85\n",
            "Batch |  1369 of 3770 loss ->  0.4287366271018982 acc -> 0.85\n",
            "Batch |  1370 of 3770 loss ->  0.47999075055122375 acc -> 0.85\n",
            "Batch |  1371 of 3770 loss ->  0.4472901225090027 acc -> 0.85\n",
            "Batch |  1372 of 3770 loss ->  0.45500317215919495 acc -> 0.85\n",
            "Batch |  1373 of 3770 loss ->  0.4945223033428192 acc -> 0.85\n",
            "Batch |  1374 of 3770 loss ->  0.46026432514190674 acc -> 0.85\n",
            "Batch |  1375 of 3770 loss ->  0.43798553943634033 acc -> 0.85\n",
            "Batch |  1376 of 3770 loss ->  0.5013974905014038 acc -> 0.85\n",
            "Batch |  1377 of 3770 loss ->  0.511115312576294 acc -> 0.85\n",
            "Batch |  1378 of 3770 loss ->  0.4596257209777832 acc -> 0.85\n",
            "Batch |  1379 of 3770 loss ->  0.4330543279647827 acc -> 0.85\n",
            "Batch |  1380 of 3770 loss ->  0.49542808532714844 acc -> 0.85\n",
            "Batch |  1381 of 3770 loss ->  0.4774661064147949 acc -> 0.85\n",
            "Batch |  1382 of 3770 loss ->  0.4454938769340515 acc -> 0.85\n",
            "Batch |  1383 of 3770 loss ->  0.4504935145378113 acc -> 0.85\n",
            "Batch |  1384 of 3770 loss ->  0.4432089328765869 acc -> 0.85\n",
            "Batch |  1385 of 3770 loss ->  0.453948050737381 acc -> 0.85\n",
            "Batch |  1386 of 3770 loss ->  0.4430429935455322 acc -> 0.85\n",
            "Batch |  1387 of 3770 loss ->  0.4541235566139221 acc -> 0.85\n",
            "Batch |  1388 of 3770 loss ->  0.4594932496547699 acc -> 0.85\n",
            "Batch |  1389 of 3770 loss ->  0.447343111038208 acc -> 0.85\n",
            "Batch |  1390 of 3770 loss ->  0.4332926571369171 acc -> 0.85\n",
            "Batch |  1391 of 3770 loss ->  0.47798752784729004 acc -> 0.85\n",
            "Batch |  1392 of 3770 loss ->  0.4459081292152405 acc -> 0.85\n",
            "Batch |  1393 of 3770 loss ->  0.44354841113090515 acc -> 0.85\n",
            "Batch |  1394 of 3770 loss ->  0.45258307456970215 acc -> 0.85\n",
            "Batch |  1395 of 3770 loss ->  0.47111207246780396 acc -> 0.85\n",
            "Batch |  1396 of 3770 loss ->  0.44652050733566284 acc -> 0.85\n",
            "Batch |  1397 of 3770 loss ->  0.47700491547584534 acc -> 0.85\n",
            "Batch |  1398 of 3770 loss ->  0.4322132170200348 acc -> 0.85\n",
            "Batch |  1399 of 3770 loss ->  0.4308777451515198 acc -> 0.85\n",
            "Batch |  1400 of 3770 loss ->  0.44821345806121826 acc -> 0.85\n",
            "Batch |  1401 of 3770 loss ->  0.4142228364944458 acc -> 0.85\n",
            "Batch |  1402 of 3770 loss ->  0.46669334173202515 acc -> 0.85\n",
            "Batch |  1403 of 3770 loss ->  0.48765429854393005 acc -> 0.85\n",
            "Batch |  1404 of 3770 loss ->  0.4655764698982239 acc -> 0.85\n",
            "Batch |  1405 of 3770 loss ->  0.4388194978237152 acc -> 0.85\n",
            "Batch |  1406 of 3770 loss ->  0.4553103446960449 acc -> 0.85\n",
            "Batch |  1407 of 3770 loss ->  0.4390442371368408 acc -> 0.85\n",
            "Batch |  1408 of 3770 loss ->  0.4031016528606415 acc -> 0.85\n",
            "Batch |  1409 of 3770 loss ->  0.5129534006118774 acc -> 0.85\n",
            "Batch |  1410 of 3770 loss ->  0.4328038990497589 acc -> 0.85\n",
            "Batch |  1411 of 3770 loss ->  0.44647809863090515 acc -> 0.85\n",
            "Batch |  1412 of 3770 loss ->  0.43916040658950806 acc -> 0.85\n",
            "Batch |  1413 of 3770 loss ->  0.4543766379356384 acc -> 0.85\n",
            "Batch |  1414 of 3770 loss ->  0.44745945930480957 acc -> 0.85\n",
            "Batch |  1415 of 3770 loss ->  0.4826216697692871 acc -> 0.85\n",
            "Batch |  1416 of 3770 loss ->  0.4562511444091797 acc -> 0.85\n",
            "Batch |  1417 of 3770 loss ->  0.4486316740512848 acc -> 0.85\n",
            "Batch |  1418 of 3770 loss ->  0.4111020863056183 acc -> 0.85\n",
            "Batch |  1419 of 3770 loss ->  0.43052220344543457 acc -> 0.85\n",
            "Batch |  1420 of 3770 loss ->  0.47370174527168274 acc -> 0.85\n",
            "Batch |  1421 of 3770 loss ->  0.46832287311553955 acc -> 0.85\n",
            "Batch |  1422 of 3770 loss ->  0.44381892681121826 acc -> 0.85\n",
            "Batch |  1423 of 3770 loss ->  0.45721209049224854 acc -> 0.85\n",
            "Batch |  1424 of 3770 loss ->  0.43209075927734375 acc -> 0.85\n",
            "Batch |  1425 of 3770 loss ->  0.4770963191986084 acc -> 0.85\n",
            "Batch |  1426 of 3770 loss ->  0.44109559059143066 acc -> 0.85\n",
            "Batch |  1427 of 3770 loss ->  0.48543068766593933 acc -> 0.85\n",
            "Batch |  1428 of 3770 loss ->  0.43498775362968445 acc -> 0.85\n",
            "Batch |  1429 of 3770 loss ->  0.4357299506664276 acc -> 0.85\n",
            "Batch |  1430 of 3770 loss ->  0.4949065148830414 acc -> 0.85\n",
            "Batch |  1431 of 3770 loss ->  0.48036861419677734 acc -> 0.85\n",
            "Batch |  1432 of 3770 loss ->  0.43468591570854187 acc -> 0.85\n",
            "Batch |  1433 of 3770 loss ->  0.4736793041229248 acc -> 0.85\n",
            "Batch |  1434 of 3770 loss ->  0.5079016089439392 acc -> 0.85\n",
            "Batch |  1435 of 3770 loss ->  0.4786221385002136 acc -> 0.85\n",
            "Batch |  1436 of 3770 loss ->  0.4469173550605774 acc -> 0.85\n",
            "Batch |  1437 of 3770 loss ->  0.4678320586681366 acc -> 0.85\n",
            "Batch |  1438 of 3770 loss ->  0.4298934042453766 acc -> 0.85\n",
            "Batch |  1439 of 3770 loss ->  0.43618500232696533 acc -> 0.85\n",
            "Batch |  1440 of 3770 loss ->  0.4347153306007385 acc -> 0.85\n",
            "Batch |  1441 of 3770 loss ->  0.43680015206336975 acc -> 0.85\n",
            "Batch |  1442 of 3770 loss ->  0.4258362054824829 acc -> 0.85\n",
            "Batch |  1443 of 3770 loss ->  0.48488369584083557 acc -> 0.85\n",
            "Batch |  1444 of 3770 loss ->  0.4354734420776367 acc -> 0.85\n",
            "Batch |  1445 of 3770 loss ->  0.4473491311073303 acc -> 0.85\n",
            "Batch |  1446 of 3770 loss ->  0.44973286986351013 acc -> 0.85\n",
            "Batch |  1447 of 3770 loss ->  0.46237650513648987 acc -> 0.85\n",
            "Batch |  1448 of 3770 loss ->  0.43775445222854614 acc -> 0.85\n",
            "Batch |  1449 of 3770 loss ->  0.4415470361709595 acc -> 0.85\n",
            "Batch |  1450 of 3770 loss ->  0.4816391170024872 acc -> 0.85\n",
            "Batch |  1451 of 3770 loss ->  0.47097787261009216 acc -> 0.85\n",
            "Batch |  1452 of 3770 loss ->  0.42947351932525635 acc -> 0.85\n",
            "Batch |  1453 of 3770 loss ->  0.5322521328926086 acc -> 0.85\n",
            "Batch |  1454 of 3770 loss ->  0.4476851224899292 acc -> 0.85\n",
            "Batch |  1455 of 3770 loss ->  0.4519026279449463 acc -> 0.85\n",
            "Batch |  1456 of 3770 loss ->  0.44664061069488525 acc -> 0.85\n",
            "Batch |  1457 of 3770 loss ->  0.5105851292610168 acc -> 0.85\n",
            "Batch |  1458 of 3770 loss ->  0.46427205204963684 acc -> 0.85\n",
            "Batch |  1459 of 3770 loss ->  0.5337553024291992 acc -> 0.85\n",
            "Batch |  1460 of 3770 loss ->  0.40234363079071045 acc -> 0.85\n",
            "Batch |  1461 of 3770 loss ->  0.4528438448905945 acc -> 0.85\n",
            "Batch |  1462 of 3770 loss ->  0.45588719844818115 acc -> 0.85\n",
            "Batch |  1463 of 3770 loss ->  0.399189829826355 acc -> 0.85\n",
            "Batch |  1464 of 3770 loss ->  0.4338076710700989 acc -> 0.85\n",
            "Batch |  1465 of 3770 loss ->  0.4861825406551361 acc -> 0.85\n",
            "Batch |  1466 of 3770 loss ->  0.42893949151039124 acc -> 0.85\n",
            "Batch |  1467 of 3770 loss ->  0.41282907128334045 acc -> 0.85\n",
            "Batch |  1468 of 3770 loss ->  0.43652546405792236 acc -> 0.85\n",
            "Batch |  1469 of 3770 loss ->  0.4463975727558136 acc -> 0.85\n",
            "Batch |  1470 of 3770 loss ->  0.4383544325828552 acc -> 0.85\n",
            "Batch |  1471 of 3770 loss ->  0.5103538632392883 acc -> 0.85\n",
            "Batch |  1472 of 3770 loss ->  0.4543682038784027 acc -> 0.85\n",
            "Batch |  1473 of 3770 loss ->  0.4576973021030426 acc -> 0.85\n",
            "Batch |  1474 of 3770 loss ->  0.4623924493789673 acc -> 0.85\n",
            "Batch |  1475 of 3770 loss ->  0.43845894932746887 acc -> 0.85\n",
            "Batch |  1476 of 3770 loss ->  0.4346761703491211 acc -> 0.85\n",
            "Batch |  1477 of 3770 loss ->  0.4722024202346802 acc -> 0.85\n",
            "Batch |  1478 of 3770 loss ->  0.44602373242378235 acc -> 0.85\n",
            "Batch |  1479 of 3770 loss ->  0.469470739364624 acc -> 0.85\n",
            "Batch |  1480 of 3770 loss ->  0.42325010895729065 acc -> 0.85\n",
            "Batch |  1481 of 3770 loss ->  0.447305828332901 acc -> 0.85\n",
            "Batch |  1482 of 3770 loss ->  0.4251207709312439 acc -> 0.85\n",
            "Batch |  1483 of 3770 loss ->  0.5221431255340576 acc -> 0.85\n",
            "Batch |  1484 of 3770 loss ->  0.4369452893733978 acc -> 0.85\n",
            "Batch |  1485 of 3770 loss ->  0.45753008127212524 acc -> 0.85\n",
            "Batch |  1486 of 3770 loss ->  0.45753350853919983 acc -> 0.85\n",
            "Batch |  1487 of 3770 loss ->  0.4730873107910156 acc -> 0.85\n",
            "Batch |  1488 of 3770 loss ->  0.4482923448085785 acc -> 0.85\n",
            "Batch |  1489 of 3770 loss ->  0.4340307116508484 acc -> 0.85\n",
            "Batch |  1490 of 3770 loss ->  0.4105159640312195 acc -> 0.85\n",
            "Batch |  1491 of 3770 loss ->  0.43617480993270874 acc -> 0.85\n",
            "Batch |  1492 of 3770 loss ->  0.4609425365924835 acc -> 0.85\n",
            "Batch |  1493 of 3770 loss ->  0.4685233533382416 acc -> 0.85\n",
            "Batch |  1494 of 3770 loss ->  0.4587485194206238 acc -> 0.85\n",
            "Batch |  1495 of 3770 loss ->  0.49359434843063354 acc -> 0.85\n",
            "Batch |  1496 of 3770 loss ->  0.40810081362724304 acc -> 0.85\n",
            "Batch |  1497 of 3770 loss ->  0.43302083015441895 acc -> 0.85\n",
            "Batch |  1498 of 3770 loss ->  0.44862470030784607 acc -> 0.85\n",
            "Batch |  1499 of 3770 loss ->  0.4427699148654938 acc -> 0.85\n",
            "Batch |  1500 of 3770 loss ->  0.4418156147003174 acc -> 0.85\n",
            "Batch |  1501 of 3770 loss ->  0.5233948230743408 acc -> 0.85\n",
            "Batch |  1502 of 3770 loss ->  0.444055438041687 acc -> 0.85\n",
            "Batch |  1503 of 3770 loss ->  0.45312047004699707 acc -> 0.85\n",
            "Batch |  1504 of 3770 loss ->  0.48274391889572144 acc -> 0.85\n",
            "Batch |  1505 of 3770 loss ->  0.43891605734825134 acc -> 0.85\n",
            "Batch |  1506 of 3770 loss ->  0.4044936001300812 acc -> 0.85\n",
            "Batch |  1507 of 3770 loss ->  0.4301261305809021 acc -> 0.85\n",
            "Batch |  1508 of 3770 loss ->  0.4559083580970764 acc -> 0.85\n",
            "Batch |  1509 of 3770 loss ->  0.4259048402309418 acc -> 0.85\n",
            "Batch |  1510 of 3770 loss ->  0.41488581895828247 acc -> 0.85\n",
            "Batch |  1511 of 3770 loss ->  0.43297719955444336 acc -> 0.85\n",
            "Batch |  1512 of 3770 loss ->  0.41777393221855164 acc -> 0.85\n",
            "Batch |  1513 of 3770 loss ->  0.43933892250061035 acc -> 0.85\n",
            "Batch |  1514 of 3770 loss ->  0.4487464427947998 acc -> 0.85\n",
            "Batch |  1515 of 3770 loss ->  0.42884308099746704 acc -> 0.85\n",
            "Batch |  1516 of 3770 loss ->  0.46249863505363464 acc -> 0.85\n",
            "Batch |  1517 of 3770 loss ->  0.4146283268928528 acc -> 0.85\n",
            "Batch |  1518 of 3770 loss ->  0.47869372367858887 acc -> 0.85\n",
            "Batch |  1519 of 3770 loss ->  0.4654231667518616 acc -> 0.85\n",
            "Batch |  1520 of 3770 loss ->  0.4245656132698059 acc -> 0.85\n",
            "Batch |  1521 of 3770 loss ->  0.43721824884414673 acc -> 0.85\n",
            "Batch |  1522 of 3770 loss ->  0.4550926089286804 acc -> 0.85\n",
            "Batch |  1523 of 3770 loss ->  0.46920114755630493 acc -> 0.85\n",
            "Batch |  1524 of 3770 loss ->  0.45064476132392883 acc -> 0.85\n",
            "Batch |  1525 of 3770 loss ->  0.45731183886528015 acc -> 0.85\n",
            "Batch |  1526 of 3770 loss ->  0.40658140182495117 acc -> 0.85\n",
            "Batch |  1527 of 3770 loss ->  0.48826169967651367 acc -> 0.85\n",
            "Batch |  1528 of 3770 loss ->  0.4527208209037781 acc -> 0.85\n",
            "Batch |  1529 of 3770 loss ->  0.42275428771972656 acc -> 0.85\n",
            "Batch |  1530 of 3770 loss ->  0.4328722357749939 acc -> 0.85\n",
            "Batch |  1531 of 3770 loss ->  0.4950275123119354 acc -> 0.85\n",
            "Batch |  1532 of 3770 loss ->  0.42572885751724243 acc -> 0.85\n",
            "Batch |  1533 of 3770 loss ->  0.44867315888404846 acc -> 0.85\n",
            "Batch |  1534 of 3770 loss ->  0.4262115955352783 acc -> 0.85\n",
            "Batch |  1535 of 3770 loss ->  0.4841154217720032 acc -> 0.85\n",
            "Batch |  1536 of 3770 loss ->  0.40460750460624695 acc -> 0.86\n",
            "Batch |  1537 of 3770 loss ->  0.4162294864654541 acc -> 0.86\n",
            "Batch |  1538 of 3770 loss ->  0.4263358414173126 acc -> 0.86\n",
            "Batch |  1539 of 3770 loss ->  0.4345875382423401 acc -> 0.86\n",
            "Batch |  1540 of 3770 loss ->  0.4339764714241028 acc -> 0.86\n",
            "Batch |  1541 of 3770 loss ->  0.4373246133327484 acc -> 0.86\n",
            "Batch |  1542 of 3770 loss ->  0.45685434341430664 acc -> 0.86\n",
            "Batch |  1543 of 3770 loss ->  0.46505051851272583 acc -> 0.86\n",
            "Batch |  1544 of 3770 loss ->  0.40794679522514343 acc -> 0.86\n",
            "Batch |  1545 of 3770 loss ->  0.45846033096313477 acc -> 0.86\n",
            "Batch |  1546 of 3770 loss ->  0.4213416576385498 acc -> 0.86\n",
            "Batch |  1547 of 3770 loss ->  0.42352020740509033 acc -> 0.86\n",
            "Batch |  1548 of 3770 loss ->  0.424885630607605 acc -> 0.86\n",
            "Batch |  1549 of 3770 loss ->  0.4006446599960327 acc -> 0.86\n",
            "Batch |  1550 of 3770 loss ->  0.43252649903297424 acc -> 0.86\n",
            "Batch |  1551 of 3770 loss ->  0.4601314067840576 acc -> 0.86\n",
            "Batch |  1552 of 3770 loss ->  0.44606900215148926 acc -> 0.86\n",
            "Batch |  1553 of 3770 loss ->  0.45087337493896484 acc -> 0.86\n",
            "Batch |  1554 of 3770 loss ->  0.47531503438949585 acc -> 0.86\n",
            "Batch |  1555 of 3770 loss ->  0.4580388069152832 acc -> 0.86\n",
            "Batch |  1556 of 3770 loss ->  0.5128270983695984 acc -> 0.86\n",
            "Batch |  1557 of 3770 loss ->  0.44408369064331055 acc -> 0.86\n",
            "Batch |  1558 of 3770 loss ->  0.42605501413345337 acc -> 0.86\n",
            "Batch |  1559 of 3770 loss ->  0.4233282804489136 acc -> 0.86\n",
            "Batch |  1560 of 3770 loss ->  0.4222751259803772 acc -> 0.86\n",
            "Batch |  1561 of 3770 loss ->  0.44503191113471985 acc -> 0.86\n",
            "Batch |  1562 of 3770 loss ->  0.39860281348228455 acc -> 0.86\n",
            "Batch |  1563 of 3770 loss ->  0.46404165029525757 acc -> 0.86\n",
            "Batch |  1564 of 3770 loss ->  0.4379744827747345 acc -> 0.86\n",
            "Batch |  1565 of 3770 loss ->  0.441089928150177 acc -> 0.86\n",
            "Batch |  1566 of 3770 loss ->  0.42298248410224915 acc -> 0.86\n",
            "Batch |  1567 of 3770 loss ->  0.426685631275177 acc -> 0.86\n",
            "Batch |  1568 of 3770 loss ->  0.42652618885040283 acc -> 0.86\n",
            "Batch |  1569 of 3770 loss ->  0.4338049292564392 acc -> 0.86\n",
            "Batch |  1570 of 3770 loss ->  0.4776704013347626 acc -> 0.86\n",
            "Batch |  1571 of 3770 loss ->  0.5253423452377319 acc -> 0.86\n",
            "Batch |  1572 of 3770 loss ->  0.4544535279273987 acc -> 0.86\n",
            "Batch |  1573 of 3770 loss ->  0.45195719599723816 acc -> 0.86\n",
            "Batch |  1574 of 3770 loss ->  0.4140832722187042 acc -> 0.86\n",
            "Batch |  1575 of 3770 loss ->  0.46595555543899536 acc -> 0.86\n",
            "Batch |  1576 of 3770 loss ->  0.4099728763103485 acc -> 0.86\n",
            "Batch |  1577 of 3770 loss ->  0.4435177445411682 acc -> 0.86\n",
            "Batch |  1578 of 3770 loss ->  0.4427412152290344 acc -> 0.86\n",
            "Batch |  1579 of 3770 loss ->  0.4581650197505951 acc -> 0.86\n",
            "Batch |  1580 of 3770 loss ->  0.41594281792640686 acc -> 0.86\n",
            "Batch |  1581 of 3770 loss ->  0.4336233139038086 acc -> 0.86\n",
            "Batch |  1582 of 3770 loss ->  0.4517301023006439 acc -> 0.86\n",
            "Batch |  1583 of 3770 loss ->  0.4181617200374603 acc -> 0.86\n",
            "Batch |  1584 of 3770 loss ->  0.4486806392669678 acc -> 0.86\n",
            "Batch |  1585 of 3770 loss ->  0.4190700650215149 acc -> 0.86\n",
            "Batch |  1586 of 3770 loss ->  0.463680624961853 acc -> 0.86\n",
            "Batch |  1587 of 3770 loss ->  0.4081510901451111 acc -> 0.86\n",
            "Batch |  1588 of 3770 loss ->  0.42274606227874756 acc -> 0.86\n",
            "Batch |  1589 of 3770 loss ->  0.46518561244010925 acc -> 0.86\n",
            "Batch |  1590 of 3770 loss ->  0.45874619483947754 acc -> 0.86\n",
            "Batch |  1591 of 3770 loss ->  0.4467802047729492 acc -> 0.86\n",
            "Batch |  1592 of 3770 loss ->  0.37778303027153015 acc -> 0.86\n",
            "Batch |  1593 of 3770 loss ->  0.4552385210990906 acc -> 0.86\n",
            "Batch |  1594 of 3770 loss ->  0.49132710695266724 acc -> 0.86\n",
            "Batch |  1595 of 3770 loss ->  0.4523240625858307 acc -> 0.86\n",
            "Batch |  1596 of 3770 loss ->  0.4663008451461792 acc -> 0.86\n",
            "Batch |  1597 of 3770 loss ->  0.4080323576927185 acc -> 0.86\n",
            "Batch |  1598 of 3770 loss ->  0.4180527925491333 acc -> 0.86\n",
            "Batch |  1599 of 3770 loss ->  0.43435224890708923 acc -> 0.86\n",
            "Batch |  1600 of 3770 loss ->  0.41798168420791626 acc -> 0.86\n",
            "Batch |  1601 of 3770 loss ->  0.4218440651893616 acc -> 0.86\n",
            "Batch |  1602 of 3770 loss ->  0.42694783210754395 acc -> 0.86\n",
            "Batch |  1603 of 3770 loss ->  0.43570736050605774 acc -> 0.86\n",
            "Batch |  1604 of 3770 loss ->  0.43281447887420654 acc -> 0.86\n",
            "Batch |  1605 of 3770 loss ->  0.40843069553375244 acc -> 0.86\n",
            "Batch |  1606 of 3770 loss ->  0.42307859659194946 acc -> 0.86\n",
            "Batch |  1607 of 3770 loss ->  0.4326521158218384 acc -> 0.86\n",
            "Batch |  1608 of 3770 loss ->  0.46205979585647583 acc -> 0.86\n",
            "Batch |  1609 of 3770 loss ->  0.47678542137145996 acc -> 0.86\n",
            "Batch |  1610 of 3770 loss ->  0.48846641182899475 acc -> 0.86\n",
            "Batch |  1611 of 3770 loss ->  0.4148823916912079 acc -> 0.86\n",
            "Batch |  1612 of 3770 loss ->  0.4433773159980774 acc -> 0.86\n",
            "Batch |  1613 of 3770 loss ->  0.41659101843833923 acc -> 0.86\n",
            "Batch |  1614 of 3770 loss ->  0.4754869341850281 acc -> 0.86\n",
            "Batch |  1615 of 3770 loss ->  0.4703543186187744 acc -> 0.86\n",
            "Batch |  1616 of 3770 loss ->  0.38659918308258057 acc -> 0.86\n",
            "Batch |  1617 of 3770 loss ->  0.48954570293426514 acc -> 0.86\n",
            "Batch |  1618 of 3770 loss ->  0.44161224365234375 acc -> 0.86\n",
            "Batch |  1619 of 3770 loss ->  0.4806343615055084 acc -> 0.86\n",
            "Batch |  1620 of 3770 loss ->  0.41573190689086914 acc -> 0.86\n",
            "Batch |  1621 of 3770 loss ->  0.4718695282936096 acc -> 0.86\n",
            "Batch |  1622 of 3770 loss ->  0.4382976293563843 acc -> 0.86\n",
            "Batch |  1623 of 3770 loss ->  0.41122615337371826 acc -> 0.86\n",
            "Batch |  1624 of 3770 loss ->  0.39742857217788696 acc -> 0.86\n",
            "Batch |  1625 of 3770 loss ->  0.470462828874588 acc -> 0.86\n",
            "Batch |  1626 of 3770 loss ->  0.44201579689979553 acc -> 0.86\n",
            "Batch |  1627 of 3770 loss ->  0.4409371614456177 acc -> 0.86\n",
            "Batch |  1628 of 3770 loss ->  0.4036802649497986 acc -> 0.86\n",
            "Batch |  1629 of 3770 loss ->  0.4416893720626831 acc -> 0.86\n",
            "Batch |  1630 of 3770 loss ->  0.46393659710884094 acc -> 0.86\n",
            "Batch |  1631 of 3770 loss ->  0.508724570274353 acc -> 0.86\n",
            "Batch |  1632 of 3770 loss ->  0.44677644968032837 acc -> 0.86\n",
            "Batch |  1633 of 3770 loss ->  0.4472242593765259 acc -> 0.86\n",
            "Batch |  1634 of 3770 loss ->  0.4454180598258972 acc -> 0.86\n",
            "Batch |  1635 of 3770 loss ->  0.42486536502838135 acc -> 0.86\n",
            "Batch |  1636 of 3770 loss ->  0.40033596754074097 acc -> 0.86\n",
            "Batch |  1637 of 3770 loss ->  0.39603883028030396 acc -> 0.86\n",
            "Batch |  1638 of 3770 loss ->  0.40781134366989136 acc -> 0.86\n",
            "Batch |  1639 of 3770 loss ->  0.48600608110427856 acc -> 0.86\n",
            "Batch |  1640 of 3770 loss ->  0.40736204385757446 acc -> 0.86\n",
            "Batch |  1641 of 3770 loss ->  0.3931303024291992 acc -> 0.86\n",
            "Batch |  1642 of 3770 loss ->  0.4322247803211212 acc -> 0.86\n",
            "Batch |  1643 of 3770 loss ->  0.4124144911766052 acc -> 0.86\n",
            "Batch |  1644 of 3770 loss ->  0.4320570230484009 acc -> 0.86\n",
            "Batch |  1645 of 3770 loss ->  0.41170233488082886 acc -> 0.86\n",
            "Batch |  1646 of 3770 loss ->  0.45350268483161926 acc -> 0.86\n",
            "Batch |  1647 of 3770 loss ->  0.4871411919593811 acc -> 0.86\n",
            "Batch |  1648 of 3770 loss ->  0.46247634291648865 acc -> 0.86\n",
            "Batch |  1649 of 3770 loss ->  0.42260822653770447 acc -> 0.86\n",
            "Batch |  1650 of 3770 loss ->  0.4503393769264221 acc -> 0.86\n",
            "Batch |  1651 of 3770 loss ->  0.47220736742019653 acc -> 0.86\n",
            "Batch |  1652 of 3770 loss ->  0.4169904589653015 acc -> 0.86\n",
            "Batch |  1653 of 3770 loss ->  0.455876886844635 acc -> 0.86\n",
            "Batch |  1654 of 3770 loss ->  0.4089128375053406 acc -> 0.86\n",
            "Batch |  1655 of 3770 loss ->  0.47136202454566956 acc -> 0.86\n",
            "Batch |  1656 of 3770 loss ->  0.417511910200119 acc -> 0.86\n",
            "Batch |  1657 of 3770 loss ->  0.40896618366241455 acc -> 0.86\n",
            "Batch |  1658 of 3770 loss ->  0.4446747601032257 acc -> 0.86\n",
            "Batch |  1659 of 3770 loss ->  0.4430280327796936 acc -> 0.86\n",
            "Batch |  1660 of 3770 loss ->  0.4160882830619812 acc -> 0.86\n",
            "Batch |  1661 of 3770 loss ->  0.43125486373901367 acc -> 0.86\n",
            "Batch |  1662 of 3770 loss ->  0.4047010838985443 acc -> 0.86\n",
            "Batch |  1663 of 3770 loss ->  0.4372524619102478 acc -> 0.86\n",
            "Batch |  1664 of 3770 loss ->  0.45363566279411316 acc -> 0.86\n",
            "Batch |  1665 of 3770 loss ->  0.40441203117370605 acc -> 0.86\n",
            "Batch |  1666 of 3770 loss ->  0.47781816124916077 acc -> 0.86\n",
            "Batch |  1667 of 3770 loss ->  0.4741314649581909 acc -> 0.86\n",
            "Batch |  1668 of 3770 loss ->  0.42557838559150696 acc -> 0.86\n",
            "Batch |  1669 of 3770 loss ->  0.4171674847602844 acc -> 0.86\n",
            "Batch |  1670 of 3770 loss ->  0.4031752049922943 acc -> 0.86\n",
            "Batch |  1671 of 3770 loss ->  0.48355045914649963 acc -> 0.86\n",
            "Batch |  1672 of 3770 loss ->  0.41863805055618286 acc -> 0.86\n",
            "Batch |  1673 of 3770 loss ->  0.4041871428489685 acc -> 0.86\n",
            "Batch |  1674 of 3770 loss ->  0.4450446367263794 acc -> 0.86\n",
            "Batch |  1675 of 3770 loss ->  0.42116302251815796 acc -> 0.86\n",
            "Batch |  1676 of 3770 loss ->  0.42149806022644043 acc -> 0.86\n",
            "Batch |  1677 of 3770 loss ->  0.430126816034317 acc -> 0.86\n",
            "Batch |  1678 of 3770 loss ->  0.4206044375896454 acc -> 0.86\n",
            "Batch |  1679 of 3770 loss ->  0.38354599475860596 acc -> 0.86\n",
            "Batch |  1680 of 3770 loss ->  0.4053035378456116 acc -> 0.86\n",
            "Batch |  1681 of 3770 loss ->  0.42725417017936707 acc -> 0.86\n",
            "Batch |  1682 of 3770 loss ->  0.47785091400146484 acc -> 0.86\n",
            "Batch |  1683 of 3770 loss ->  0.45375001430511475 acc -> 0.86\n",
            "Batch |  1684 of 3770 loss ->  0.40249910950660706 acc -> 0.86\n",
            "Batch |  1685 of 3770 loss ->  0.41667911410331726 acc -> 0.86\n",
            "Batch |  1686 of 3770 loss ->  0.43871331214904785 acc -> 0.86\n",
            "Batch |  1687 of 3770 loss ->  0.4220619797706604 acc -> 0.86\n",
            "Batch |  1688 of 3770 loss ->  0.43509960174560547 acc -> 0.86\n",
            "Batch |  1689 of 3770 loss ->  0.4055157005786896 acc -> 0.86\n",
            "Batch |  1690 of 3770 loss ->  0.4193114638328552 acc -> 0.86\n",
            "Batch |  1691 of 3770 loss ->  0.4427071213722229 acc -> 0.86\n",
            "Batch |  1692 of 3770 loss ->  0.39685487747192383 acc -> 0.86\n",
            "Batch |  1693 of 3770 loss ->  0.4646652638912201 acc -> 0.86\n",
            "Batch |  1694 of 3770 loss ->  0.3941587507724762 acc -> 0.86\n",
            "Batch |  1695 of 3770 loss ->  0.3957178592681885 acc -> 0.86\n",
            "Batch |  1696 of 3770 loss ->  0.4248954653739929 acc -> 0.86\n",
            "Batch |  1697 of 3770 loss ->  0.4460109770298004 acc -> 0.86\n",
            "Batch |  1698 of 3770 loss ->  0.4315665364265442 acc -> 0.86\n",
            "Batch |  1699 of 3770 loss ->  0.42814934253692627 acc -> 0.86\n",
            "Batch |  1700 of 3770 loss ->  0.44607123732566833 acc -> 0.86\n",
            "Batch |  1701 of 3770 loss ->  0.466189980506897 acc -> 0.86\n",
            "Batch |  1702 of 3770 loss ->  0.4070773720741272 acc -> 0.86\n",
            "Batch |  1703 of 3770 loss ->  0.4388653635978699 acc -> 0.86\n",
            "Batch |  1704 of 3770 loss ->  0.4289095401763916 acc -> 0.86\n",
            "Batch |  1705 of 3770 loss ->  0.4020722210407257 acc -> 0.86\n",
            "Batch |  1706 of 3770 loss ->  0.44226449728012085 acc -> 0.86\n",
            "Batch |  1707 of 3770 loss ->  0.40297815203666687 acc -> 0.86\n",
            "Batch |  1708 of 3770 loss ->  0.48224854469299316 acc -> 0.86\n",
            "Batch |  1709 of 3770 loss ->  0.409932941198349 acc -> 0.86\n",
            "Batch |  1710 of 3770 loss ->  0.43290746212005615 acc -> 0.86\n",
            "Batch |  1711 of 3770 loss ->  0.41429197788238525 acc -> 0.86\n",
            "Batch |  1712 of 3770 loss ->  0.42359939217567444 acc -> 0.86\n",
            "Batch |  1713 of 3770 loss ->  0.42004790902137756 acc -> 0.86\n",
            "Batch |  1714 of 3770 loss ->  0.43200916051864624 acc -> 0.86\n",
            "Batch |  1715 of 3770 loss ->  0.4387816786766052 acc -> 0.86\n",
            "Batch |  1716 of 3770 loss ->  0.4393937587738037 acc -> 0.86\n",
            "Batch |  1717 of 3770 loss ->  0.40917444229125977 acc -> 0.86\n",
            "Batch |  1718 of 3770 loss ->  0.4736481308937073 acc -> 0.86\n",
            "Batch |  1719 of 3770 loss ->  0.4291660785675049 acc -> 0.86\n",
            "Batch |  1720 of 3770 loss ->  0.4318265914916992 acc -> 0.86\n",
            "Batch |  1721 of 3770 loss ->  0.3993591070175171 acc -> 0.86\n",
            "Batch |  1722 of 3770 loss ->  0.45708489418029785 acc -> 0.86\n",
            "Batch |  1723 of 3770 loss ->  0.44849294424057007 acc -> 0.86\n",
            "Batch |  1724 of 3770 loss ->  0.39732223749160767 acc -> 0.86\n",
            "Batch |  1725 of 3770 loss ->  0.42066675424575806 acc -> 0.86\n",
            "Batch |  1726 of 3770 loss ->  0.44890445470809937 acc -> 0.86\n",
            "Batch |  1727 of 3770 loss ->  0.3934858441352844 acc -> 0.86\n",
            "Batch |  1728 of 3770 loss ->  0.43339234590530396 acc -> 0.86\n",
            "Batch |  1729 of 3770 loss ->  0.442939817905426 acc -> 0.86\n",
            "Batch |  1730 of 3770 loss ->  0.42831775546073914 acc -> 0.86\n",
            "Batch |  1731 of 3770 loss ->  0.47256171703338623 acc -> 0.86\n",
            "Batch |  1732 of 3770 loss ->  0.4622489809989929 acc -> 0.86\n",
            "Batch |  1733 of 3770 loss ->  0.4066404700279236 acc -> 0.86\n",
            "Batch |  1734 of 3770 loss ->  0.4226357638835907 acc -> 0.86\n",
            "Batch |  1735 of 3770 loss ->  0.45985618233680725 acc -> 0.86\n",
            "Batch |  1736 of 3770 loss ->  0.42492642998695374 acc -> 0.86\n",
            "Batch |  1737 of 3770 loss ->  0.38984426856040955 acc -> 0.86\n",
            "Batch |  1738 of 3770 loss ->  0.41428428888320923 acc -> 0.87\n",
            "Batch |  1739 of 3770 loss ->  0.39944902062416077 acc -> 0.87\n",
            "Batch |  1740 of 3770 loss ->  0.39573052525520325 acc -> 0.87\n",
            "Batch |  1741 of 3770 loss ->  0.4439384937286377 acc -> 0.87\n",
            "Batch |  1742 of 3770 loss ->  0.41036832332611084 acc -> 0.87\n",
            "Batch |  1743 of 3770 loss ->  0.38582485914230347 acc -> 0.87\n",
            "Batch |  1744 of 3770 loss ->  0.41908472776412964 acc -> 0.87\n",
            "Batch |  1745 of 3770 loss ->  0.4681040048599243 acc -> 0.87\n",
            "Batch |  1746 of 3770 loss ->  0.43188029527664185 acc -> 0.87\n",
            "Batch |  1747 of 3770 loss ->  0.44068726897239685 acc -> 0.87\n",
            "Batch |  1748 of 3770 loss ->  0.41222935914993286 acc -> 0.87\n",
            "Batch |  1749 of 3770 loss ->  0.42757827043533325 acc -> 0.87\n",
            "Batch |  1750 of 3770 loss ->  0.4183085858821869 acc -> 0.87\n",
            "Batch |  1751 of 3770 loss ->  0.45702821016311646 acc -> 0.87\n",
            "Batch |  1752 of 3770 loss ->  0.4902692437171936 acc -> 0.87\n",
            "Batch |  1753 of 3770 loss ->  0.42569440603256226 acc -> 0.87\n",
            "Batch |  1754 of 3770 loss ->  0.47784629464149475 acc -> 0.87\n",
            "Batch |  1755 of 3770 loss ->  0.4050877094268799 acc -> 0.87\n",
            "Batch |  1756 of 3770 loss ->  0.43416011333465576 acc -> 0.87\n",
            "Batch |  1757 of 3770 loss ->  0.40100693702697754 acc -> 0.87\n",
            "Batch |  1758 of 3770 loss ->  0.3992305397987366 acc -> 0.87\n",
            "Batch |  1759 of 3770 loss ->  0.3957529067993164 acc -> 0.87\n",
            "Batch |  1760 of 3770 loss ->  0.4323022961616516 acc -> 0.87\n",
            "Batch |  1761 of 3770 loss ->  0.4283478558063507 acc -> 0.87\n",
            "Batch |  1762 of 3770 loss ->  0.4072648286819458 acc -> 0.87\n",
            "Batch |  1763 of 3770 loss ->  0.4490702152252197 acc -> 0.87\n",
            "Batch |  1764 of 3770 loss ->  0.4071417450904846 acc -> 0.87\n",
            "Batch |  1765 of 3770 loss ->  0.42082124948501587 acc -> 0.87\n",
            "Batch |  1766 of 3770 loss ->  0.40274766087532043 acc -> 0.87\n",
            "Batch |  1767 of 3770 loss ->  0.5151898860931396 acc -> 0.87\n",
            "Batch |  1768 of 3770 loss ->  0.4780384302139282 acc -> 0.87\n",
            "Batch |  1769 of 3770 loss ->  0.4261454939842224 acc -> 0.87\n",
            "Batch |  1770 of 3770 loss ->  0.3828030228614807 acc -> 0.87\n",
            "Batch |  1771 of 3770 loss ->  0.43992000818252563 acc -> 0.87\n",
            "Batch |  1772 of 3770 loss ->  0.40843695402145386 acc -> 0.87\n",
            "Batch |  1773 of 3770 loss ->  0.4117400050163269 acc -> 0.87\n",
            "Batch |  1774 of 3770 loss ->  0.41003096103668213 acc -> 0.87\n",
            "Batch |  1775 of 3770 loss ->  0.41092219948768616 acc -> 0.87\n",
            "Batch |  1776 of 3770 loss ->  0.39934292435646057 acc -> 0.87\n",
            "Batch |  1777 of 3770 loss ->  0.412173330783844 acc -> 0.87\n",
            "Batch |  1778 of 3770 loss ->  0.4073113799095154 acc -> 0.87\n",
            "Batch |  1779 of 3770 loss ->  0.3987925946712494 acc -> 0.87\n",
            "Batch |  1780 of 3770 loss ->  0.41493502259254456 acc -> 0.87\n",
            "Batch |  1781 of 3770 loss ->  0.47174346446990967 acc -> 0.87\n",
            "Batch |  1782 of 3770 loss ->  0.43914151191711426 acc -> 0.87\n",
            "Batch |  1783 of 3770 loss ->  0.41361933946609497 acc -> 0.87\n",
            "Batch |  1784 of 3770 loss ->  0.433574378490448 acc -> 0.87\n",
            "Batch |  1785 of 3770 loss ->  0.3796396255493164 acc -> 0.87\n",
            "Batch |  1786 of 3770 loss ->  0.40200430154800415 acc -> 0.87\n",
            "Batch |  1787 of 3770 loss ->  0.3902480900287628 acc -> 0.87\n",
            "Batch |  1788 of 3770 loss ->  0.4293610453605652 acc -> 0.87\n",
            "Batch |  1789 of 3770 loss ->  0.397555410861969 acc -> 0.87\n",
            "Batch |  1790 of 3770 loss ->  0.4166325032711029 acc -> 0.87\n",
            "Batch |  1791 of 3770 loss ->  0.392021119594574 acc -> 0.87\n",
            "Batch |  1792 of 3770 loss ->  0.3969714045524597 acc -> 0.87\n",
            "Batch |  1793 of 3770 loss ->  0.4282971918582916 acc -> 0.87\n",
            "Batch |  1794 of 3770 loss ->  0.38938581943511963 acc -> 0.87\n",
            "Batch |  1795 of 3770 loss ->  0.47686922550201416 acc -> 0.87\n",
            "Batch |  1796 of 3770 loss ->  0.3916163444519043 acc -> 0.87\n",
            "Batch |  1797 of 3770 loss ->  0.3859182596206665 acc -> 0.87\n",
            "Batch |  1798 of 3770 loss ->  0.42152613401412964 acc -> 0.87\n",
            "Batch |  1799 of 3770 loss ->  0.40467560291290283 acc -> 0.87\n",
            "Batch |  1800 of 3770 loss ->  0.3749265968799591 acc -> 0.87\n",
            "Batch |  1801 of 3770 loss ->  0.41287875175476074 acc -> 0.87\n",
            "Batch |  1802 of 3770 loss ->  0.38045209646224976 acc -> 0.87\n",
            "Batch |  1803 of 3770 loss ->  0.38864895701408386 acc -> 0.87\n",
            "Batch |  1804 of 3770 loss ->  0.41997048258781433 acc -> 0.87\n",
            "Batch |  1805 of 3770 loss ->  0.3932104706764221 acc -> 0.87\n",
            "Batch |  1806 of 3770 loss ->  0.429124116897583 acc -> 0.87\n",
            "Batch |  1807 of 3770 loss ->  0.45087653398513794 acc -> 0.87\n",
            "Batch |  1808 of 3770 loss ->  0.42629948258399963 acc -> 0.87\n",
            "Batch |  1809 of 3770 loss ->  0.4450126886367798 acc -> 0.87\n",
            "Batch |  1810 of 3770 loss ->  0.4300006628036499 acc -> 0.87\n",
            "Batch |  1811 of 3770 loss ->  0.4168611168861389 acc -> 0.87\n",
            "Batch |  1812 of 3770 loss ->  0.4876738488674164 acc -> 0.87\n",
            "Batch |  1813 of 3770 loss ->  0.40417471528053284 acc -> 0.87\n",
            "Batch |  1814 of 3770 loss ->  0.4334942698478699 acc -> 0.87\n",
            "Batch |  1815 of 3770 loss ->  0.40445709228515625 acc -> 0.87\n",
            "Batch |  1816 of 3770 loss ->  0.40240395069122314 acc -> 0.87\n",
            "Batch |  1817 of 3770 loss ->  0.39482569694519043 acc -> 0.87\n",
            "Batch |  1818 of 3770 loss ->  0.3964490294456482 acc -> 0.87\n",
            "Batch |  1819 of 3770 loss ->  0.43592533469200134 acc -> 0.87\n",
            "Batch |  1820 of 3770 loss ->  0.3845078945159912 acc -> 0.87\n",
            "Batch |  1821 of 3770 loss ->  0.4271668791770935 acc -> 0.87\n",
            "Batch |  1822 of 3770 loss ->  0.38831546902656555 acc -> 0.87\n",
            "Batch |  1823 of 3770 loss ->  0.4079750180244446 acc -> 0.87\n",
            "Batch |  1824 of 3770 loss ->  0.38912519812583923 acc -> 0.87\n",
            "Batch |  1825 of 3770 loss ->  0.41523802280426025 acc -> 0.87\n",
            "Batch |  1826 of 3770 loss ->  0.40554696321487427 acc -> 0.87\n",
            "Batch |  1827 of 3770 loss ->  0.41786497831344604 acc -> 0.87\n",
            "Batch |  1828 of 3770 loss ->  0.4115407466888428 acc -> 0.87\n",
            "Batch |  1829 of 3770 loss ->  0.3755779266357422 acc -> 0.87\n",
            "Batch |  1830 of 3770 loss ->  0.39646488428115845 acc -> 0.87\n",
            "Batch |  1831 of 3770 loss ->  0.44296762347221375 acc -> 0.87\n",
            "Batch |  1832 of 3770 loss ->  0.4148082137107849 acc -> 0.87\n",
            "Batch |  1833 of 3770 loss ->  0.4578886926174164 acc -> 0.87\n",
            "Batch |  1834 of 3770 loss ->  0.43014609813690186 acc -> 0.87\n",
            "Batch |  1835 of 3770 loss ->  0.4093403220176697 acc -> 0.87\n",
            "Batch |  1836 of 3770 loss ->  0.41908055543899536 acc -> 0.87\n",
            "Batch |  1837 of 3770 loss ->  0.41952088475227356 acc -> 0.87\n",
            "Batch |  1838 of 3770 loss ->  0.43209660053253174 acc -> 0.87\n",
            "Batch |  1839 of 3770 loss ->  0.4226458668708801 acc -> 0.87\n",
            "Batch |  1840 of 3770 loss ->  0.4889448285102844 acc -> 0.87\n",
            "Batch |  1841 of 3770 loss ->  0.5099460482597351 acc -> 0.87\n",
            "Batch |  1842 of 3770 loss ->  0.44378432631492615 acc -> 0.87\n",
            "Batch |  1843 of 3770 loss ->  0.41466736793518066 acc -> 0.87\n",
            "Batch |  1844 of 3770 loss ->  0.45458364486694336 acc -> 0.87\n",
            "Batch |  1845 of 3770 loss ->  0.3989166021347046 acc -> 0.87\n",
            "Batch |  1846 of 3770 loss ->  0.4549533724784851 acc -> 0.87\n",
            "Batch |  1847 of 3770 loss ->  0.4395331144332886 acc -> 0.87\n",
            "Batch |  1848 of 3770 loss ->  0.38025951385498047 acc -> 0.87\n",
            "Batch |  1849 of 3770 loss ->  0.4017345905303955 acc -> 0.87\n",
            "Batch |  1850 of 3770 loss ->  0.42504769563674927 acc -> 0.87\n",
            "Batch |  1851 of 3770 loss ->  0.4155083894729614 acc -> 0.87\n",
            "Batch |  1852 of 3770 loss ->  0.43622955679893494 acc -> 0.87\n",
            "Batch |  1853 of 3770 loss ->  0.38377612829208374 acc -> 0.87\n",
            "Batch |  1854 of 3770 loss ->  0.3815075755119324 acc -> 0.87\n",
            "Batch |  1855 of 3770 loss ->  0.3816494345664978 acc -> 0.87\n",
            "Batch |  1856 of 3770 loss ->  0.4249258041381836 acc -> 0.87\n",
            "Batch |  1857 of 3770 loss ->  0.4038960933685303 acc -> 0.87\n",
            "Batch |  1858 of 3770 loss ->  0.38953423500061035 acc -> 0.87\n",
            "Batch |  1859 of 3770 loss ->  0.44516798853874207 acc -> 0.87\n",
            "Batch |  1860 of 3770 loss ->  0.3958245515823364 acc -> 0.87\n",
            "Batch |  1861 of 3770 loss ->  0.4456365704536438 acc -> 0.87\n",
            "Batch |  1862 of 3770 loss ->  0.4334331452846527 acc -> 0.87\n",
            "Batch |  1863 of 3770 loss ->  0.4474835991859436 acc -> 0.87\n",
            "Batch |  1864 of 3770 loss ->  0.4001099467277527 acc -> 0.87\n",
            "Batch |  1865 of 3770 loss ->  0.38827839493751526 acc -> 0.87\n",
            "Batch |  1866 of 3770 loss ->  0.382974773645401 acc -> 0.87\n",
            "Batch |  1867 of 3770 loss ->  0.4164271056652069 acc -> 0.87\n",
            "Batch |  1868 of 3770 loss ->  0.5008677244186401 acc -> 0.87\n",
            "Batch |  1869 of 3770 loss ->  0.3999934196472168 acc -> 0.87\n",
            "Batch |  1870 of 3770 loss ->  0.4127236604690552 acc -> 0.87\n",
            "Batch |  1871 of 3770 loss ->  0.4098004102706909 acc -> 0.87\n",
            "Batch |  1872 of 3770 loss ->  0.381289005279541 acc -> 0.87\n",
            "Batch |  1873 of 3770 loss ->  0.37750834226608276 acc -> 0.87\n",
            "Batch |  1874 of 3770 loss ->  0.4215518832206726 acc -> 0.87\n",
            "Batch |  1875 of 3770 loss ->  0.39688318967819214 acc -> 0.87\n",
            "Batch |  1876 of 3770 loss ->  0.4566621780395508 acc -> 0.87\n",
            "Batch |  1877 of 3770 loss ->  0.45149940252304077 acc -> 0.87\n",
            "Batch |  1878 of 3770 loss ->  0.37907958030700684 acc -> 0.87\n",
            "Batch |  1879 of 3770 loss ->  0.4509657621383667 acc -> 0.87\n",
            "Batch |  1880 of 3770 loss ->  0.43330615758895874 acc -> 0.87\n",
            "Batch |  1881 of 3770 loss ->  0.46274590492248535 acc -> 0.87\n",
            "Batch |  1882 of 3770 loss ->  0.43605077266693115 acc -> 0.87\n",
            "Batch |  1883 of 3770 loss ->  0.4033338725566864 acc -> 0.87\n",
            "Batch |  1884 of 3770 loss ->  0.49570393562316895 acc -> 0.87\n",
            "Batch |  1885 of 3770 loss ->  0.4616784155368805 acc -> 0.87\n",
            "Batch |  1886 of 3770 loss ->  0.4065474271774292 acc -> 0.87\n",
            "Batch |  1887 of 3770 loss ->  0.39137399196624756 acc -> 0.87\n",
            "Batch |  1888 of 3770 loss ->  0.3665790557861328 acc -> 0.87\n",
            "Batch |  1889 of 3770 loss ->  0.35557907819747925 acc -> 0.87\n",
            "Batch |  1890 of 3770 loss ->  0.4346081614494324 acc -> 0.87\n",
            "Batch |  1891 of 3770 loss ->  0.43624991178512573 acc -> 0.87\n",
            "Batch |  1892 of 3770 loss ->  0.40223419666290283 acc -> 0.87\n",
            "Batch |  1893 of 3770 loss ->  0.3968660235404968 acc -> 0.87\n",
            "Batch |  1894 of 3770 loss ->  0.41297587752342224 acc -> 0.87\n",
            "Batch |  1895 of 3770 loss ->  0.38231003284454346 acc -> 0.87\n",
            "Batch |  1896 of 3770 loss ->  0.41265422105789185 acc -> 0.87\n",
            "Batch |  1897 of 3770 loss ->  0.4023315906524658 acc -> 0.87\n",
            "Batch |  1898 of 3770 loss ->  0.43174219131469727 acc -> 0.87\n",
            "Batch |  1899 of 3770 loss ->  0.4893448054790497 acc -> 0.87\n",
            "Batch |  1900 of 3770 loss ->  0.39171382784843445 acc -> 0.87\n",
            "Batch |  1901 of 3770 loss ->  0.4030316472053528 acc -> 0.87\n",
            "Batch |  1902 of 3770 loss ->  0.3878959119319916 acc -> 0.87\n",
            "Batch |  1903 of 3770 loss ->  0.4087110757827759 acc -> 0.87\n",
            "Batch |  1904 of 3770 loss ->  0.441096693277359 acc -> 0.87\n",
            "Batch |  1905 of 3770 loss ->  0.38464227318763733 acc -> 0.87\n",
            "Batch |  1906 of 3770 loss ->  0.4076278805732727 acc -> 0.87\n",
            "Batch |  1907 of 3770 loss ->  0.4027665853500366 acc -> 0.87\n",
            "Batch |  1908 of 3770 loss ->  0.4221245050430298 acc -> 0.87\n",
            "Batch |  1909 of 3770 loss ->  0.3838314414024353 acc -> 0.87\n",
            "Batch |  1910 of 3770 loss ->  0.45403623580932617 acc -> 0.87\n",
            "Batch |  1911 of 3770 loss ->  0.437595933675766 acc -> 0.87\n",
            "Batch |  1912 of 3770 loss ->  0.4089370369911194 acc -> 0.87\n",
            "Batch |  1913 of 3770 loss ->  0.4215899109840393 acc -> 0.87\n",
            "Batch |  1914 of 3770 loss ->  0.3949156105518341 acc -> 0.87\n",
            "Batch |  1915 of 3770 loss ->  0.3844980001449585 acc -> 0.87\n",
            "Batch |  1916 of 3770 loss ->  0.4221603274345398 acc -> 0.87\n",
            "Batch |  1917 of 3770 loss ->  0.40529629588127136 acc -> 0.87\n",
            "Batch |  1918 of 3770 loss ->  0.4293457865715027 acc -> 0.87\n",
            "Batch |  1919 of 3770 loss ->  0.41113775968551636 acc -> 0.87\n",
            "Batch |  1920 of 3770 loss ->  0.41968852281570435 acc -> 0.87\n",
            "Batch |  1921 of 3770 loss ->  0.4060927629470825 acc -> 0.87\n",
            "Batch |  1922 of 3770 loss ->  0.41341832280158997 acc -> 0.87\n",
            "Batch |  1923 of 3770 loss ->  0.4359431862831116 acc -> 0.87\n",
            "Batch |  1924 of 3770 loss ->  0.37850648164749146 acc -> 0.87\n",
            "Batch |  1925 of 3770 loss ->  0.40838032960891724 acc -> 0.87\n",
            "Batch |  1926 of 3770 loss ->  0.37331098318099976 acc -> 0.87\n",
            "Batch |  1927 of 3770 loss ->  0.4390612840652466 acc -> 0.87\n",
            "Batch |  1928 of 3770 loss ->  0.42148953676223755 acc -> 0.87\n",
            "Batch |  1929 of 3770 loss ->  0.4256369173526764 acc -> 0.87\n",
            "Batch |  1930 of 3770 loss ->  0.4328576624393463 acc -> 0.87\n",
            "Batch |  1931 of 3770 loss ->  0.4328681528568268 acc -> 0.87\n",
            "Batch |  1932 of 3770 loss ->  0.4329376816749573 acc -> 0.87\n",
            "Batch |  1933 of 3770 loss ->  0.46034157276153564 acc -> 0.87\n",
            "Batch |  1934 of 3770 loss ->  0.41751351952552795 acc -> 0.87\n",
            "Batch |  1935 of 3770 loss ->  0.39243724942207336 acc -> 0.87\n",
            "Batch |  1936 of 3770 loss ->  0.37831491231918335 acc -> 0.87\n",
            "Batch |  1937 of 3770 loss ->  0.36669862270355225 acc -> 0.87\n",
            "Batch |  1938 of 3770 loss ->  0.41192734241485596 acc -> 0.87\n",
            "Batch |  1939 of 3770 loss ->  0.39568379521369934 acc -> 0.87\n",
            "Batch |  1940 of 3770 loss ->  0.42105403542518616 acc -> 0.87\n",
            "Batch |  1941 of 3770 loss ->  0.4178333282470703 acc -> 0.87\n",
            "Batch |  1942 of 3770 loss ->  0.44978559017181396 acc -> 0.87\n",
            "Batch |  1943 of 3770 loss ->  0.43369653820991516 acc -> 0.87\n",
            "Batch |  1944 of 3770 loss ->  0.3830360770225525 acc -> 0.87\n",
            "Batch |  1945 of 3770 loss ->  0.3775309920310974 acc -> 0.87\n",
            "Batch |  1946 of 3770 loss ->  0.43566203117370605 acc -> 0.87\n",
            "Batch |  1947 of 3770 loss ->  0.40516605973243713 acc -> 0.87\n",
            "Batch |  1948 of 3770 loss ->  0.4275051951408386 acc -> 0.87\n",
            "Batch |  1949 of 3770 loss ->  0.4024161100387573 acc -> 0.87\n",
            "Batch |  1950 of 3770 loss ->  0.4116516411304474 acc -> 0.87\n",
            "Batch |  1951 of 3770 loss ->  0.4433932602405548 acc -> 0.87\n",
            "Batch |  1952 of 3770 loss ->  0.41095247864723206 acc -> 0.87\n",
            "Batch |  1953 of 3770 loss ->  0.3615233302116394 acc -> 0.87\n",
            "Batch |  1954 of 3770 loss ->  0.46596598625183105 acc -> 0.87\n",
            "Batch |  1955 of 3770 loss ->  0.413646399974823 acc -> 0.87\n",
            "Batch |  1956 of 3770 loss ->  0.41835451126098633 acc -> 0.87\n",
            "Batch |  1957 of 3770 loss ->  0.4037693738937378 acc -> 0.87\n",
            "Batch |  1958 of 3770 loss ->  0.38093578815460205 acc -> 0.87\n",
            "Batch |  1959 of 3770 loss ->  0.3776138722896576 acc -> 0.87\n",
            "Batch |  1960 of 3770 loss ->  0.408075749874115 acc -> 0.87\n",
            "Batch |  1961 of 3770 loss ->  0.419297456741333 acc -> 0.87\n",
            "Batch |  1962 of 3770 loss ->  0.4048748314380646 acc -> 0.88\n",
            "Batch |  1963 of 3770 loss ->  0.3881618082523346 acc -> 0.88\n",
            "Batch |  1964 of 3770 loss ->  0.3816021680831909 acc -> 0.88\n",
            "Batch |  1965 of 3770 loss ->  0.4074833393096924 acc -> 0.88\n",
            "Batch |  1966 of 3770 loss ->  0.37881070375442505 acc -> 0.88\n",
            "Batch |  1967 of 3770 loss ->  0.43038445711135864 acc -> 0.88\n",
            "Batch |  1968 of 3770 loss ->  0.3986925482749939 acc -> 0.88\n",
            "Batch |  1969 of 3770 loss ->  0.4098483920097351 acc -> 0.88\n",
            "Batch |  1970 of 3770 loss ->  0.4372715353965759 acc -> 0.88\n",
            "Batch |  1971 of 3770 loss ->  0.4268307685852051 acc -> 0.88\n",
            "Batch |  1972 of 3770 loss ->  0.3998550772666931 acc -> 0.88\n",
            "Batch |  1973 of 3770 loss ->  0.4137476682662964 acc -> 0.88\n",
            "Batch |  1974 of 3770 loss ->  0.39844298362731934 acc -> 0.88\n",
            "Batch |  1975 of 3770 loss ->  0.38554930686950684 acc -> 0.88\n",
            "Batch |  1976 of 3770 loss ->  0.3705046772956848 acc -> 0.88\n",
            "Batch |  1977 of 3770 loss ->  0.45034754276275635 acc -> 0.88\n",
            "Batch |  1978 of 3770 loss ->  0.39822107553482056 acc -> 0.88\n",
            "Batch |  1979 of 3770 loss ->  0.4153388738632202 acc -> 0.88\n",
            "Batch |  1980 of 3770 loss ->  0.47168684005737305 acc -> 0.88\n",
            "Batch |  1981 of 3770 loss ->  0.4381687641143799 acc -> 0.88\n",
            "Batch |  1982 of 3770 loss ->  0.41846439242362976 acc -> 0.88\n",
            "Batch |  1983 of 3770 loss ->  0.40713074803352356 acc -> 0.88\n",
            "Batch |  1984 of 3770 loss ->  0.41308340430259705 acc -> 0.88\n",
            "Batch |  1985 of 3770 loss ->  0.4466015696525574 acc -> 0.88\n",
            "Batch |  1986 of 3770 loss ->  0.36124539375305176 acc -> 0.88\n",
            "Batch |  1987 of 3770 loss ->  0.408574640750885 acc -> 0.88\n",
            "Batch |  1988 of 3770 loss ->  0.4516693353652954 acc -> 0.88\n",
            "Batch |  1989 of 3770 loss ->  0.45763054490089417 acc -> 0.88\n",
            "Batch |  1990 of 3770 loss ->  0.4166816473007202 acc -> 0.88\n",
            "Batch |  1991 of 3770 loss ->  0.44514375925064087 acc -> 0.88\n",
            "Batch |  1992 of 3770 loss ->  0.3692321181297302 acc -> 0.88\n",
            "Batch |  1993 of 3770 loss ->  0.3958585858345032 acc -> 0.88\n",
            "Batch |  1994 of 3770 loss ->  0.36378973722457886 acc -> 0.88\n",
            "Batch |  1995 of 3770 loss ->  0.3807287812232971 acc -> 0.88\n",
            "Batch |  1996 of 3770 loss ->  0.39646995067596436 acc -> 0.88\n",
            "Batch |  1997 of 3770 loss ->  0.48651254177093506 acc -> 0.88\n",
            "Batch |  1998 of 3770 loss ->  0.3954285681247711 acc -> 0.88\n",
            "Batch |  1999 of 3770 loss ->  0.3488304615020752 acc -> 0.88\n",
            "Batch |  2000 of 3770 loss ->  0.4116833806037903 acc -> 0.88\n",
            "Batch |  2001 of 3770 loss ->  0.3936452269554138 acc -> 0.88\n",
            "Batch |  2002 of 3770 loss ->  0.4927028715610504 acc -> 0.88\n",
            "Batch |  2003 of 3770 loss ->  0.4129600524902344 acc -> 0.88\n",
            "Batch |  2004 of 3770 loss ->  0.3743637800216675 acc -> 0.88\n",
            "Batch |  2005 of 3770 loss ->  0.4081803858280182 acc -> 0.88\n",
            "Batch |  2006 of 3770 loss ->  0.3697650730609894 acc -> 0.88\n",
            "Batch |  2007 of 3770 loss ->  0.45482516288757324 acc -> 0.88\n",
            "Batch |  2008 of 3770 loss ->  0.4067704677581787 acc -> 0.88\n",
            "Batch |  2009 of 3770 loss ->  0.4108288884162903 acc -> 0.88\n",
            "Batch |  2010 of 3770 loss ->  0.42231810092926025 acc -> 0.88\n",
            "Batch |  2011 of 3770 loss ->  0.4492313861846924 acc -> 0.88\n",
            "Batch |  2012 of 3770 loss ->  0.3866490423679352 acc -> 0.88\n",
            "Batch |  2013 of 3770 loss ->  0.4163215160369873 acc -> 0.88\n",
            "Batch |  2014 of 3770 loss ->  0.372150182723999 acc -> 0.88\n",
            "Batch |  2015 of 3770 loss ->  0.3943321406841278 acc -> 0.88\n",
            "Batch |  2016 of 3770 loss ->  0.3712177574634552 acc -> 0.88\n",
            "Batch |  2017 of 3770 loss ->  0.38030898571014404 acc -> 0.88\n",
            "Batch |  2018 of 3770 loss ->  0.38626348972320557 acc -> 0.88\n",
            "Batch |  2019 of 3770 loss ->  0.36565113067626953 acc -> 0.88\n",
            "Batch |  2020 of 3770 loss ->  0.41601645946502686 acc -> 0.88\n",
            "Batch |  2021 of 3770 loss ->  0.4032957851886749 acc -> 0.88\n",
            "Batch |  2022 of 3770 loss ->  0.4547041654586792 acc -> 0.88\n",
            "Batch |  2023 of 3770 loss ->  0.4131287932395935 acc -> 0.88\n",
            "Batch |  2024 of 3770 loss ->  0.3730992078781128 acc -> 0.88\n",
            "Batch |  2025 of 3770 loss ->  0.3889796733856201 acc -> 0.88\n",
            "Batch |  2026 of 3770 loss ->  0.3458544909954071 acc -> 0.88\n",
            "Batch |  2027 of 3770 loss ->  0.3567490577697754 acc -> 0.88\n",
            "Batch |  2028 of 3770 loss ->  0.37110310792922974 acc -> 0.88\n",
            "Batch |  2029 of 3770 loss ->  0.45297619700431824 acc -> 0.88\n",
            "Batch |  2030 of 3770 loss ->  0.41229209303855896 acc -> 0.88\n",
            "Batch |  2031 of 3770 loss ->  0.4045064449310303 acc -> 0.88\n",
            "Batch |  2032 of 3770 loss ->  0.3929058313369751 acc -> 0.88\n",
            "Batch |  2033 of 3770 loss ->  0.3620736598968506 acc -> 0.88\n",
            "Batch |  2034 of 3770 loss ->  0.37969496846199036 acc -> 0.88\n",
            "Batch |  2035 of 3770 loss ->  0.37592869997024536 acc -> 0.88\n",
            "Batch |  2036 of 3770 loss ->  0.38832706212997437 acc -> 0.88\n",
            "Batch |  2037 of 3770 loss ->  0.39918529987335205 acc -> 0.88\n",
            "Batch |  2038 of 3770 loss ->  0.39314043521881104 acc -> 0.88\n",
            "Batch |  2039 of 3770 loss ->  0.37140172719955444 acc -> 0.88\n",
            "Batch |  2040 of 3770 loss ->  0.41520750522613525 acc -> 0.88\n",
            "Batch |  2041 of 3770 loss ->  0.4741193354129791 acc -> 0.88\n",
            "Batch |  2042 of 3770 loss ->  0.3782394230365753 acc -> 0.88\n",
            "Batch |  2043 of 3770 loss ->  0.5228937864303589 acc -> 0.88\n",
            "Batch |  2044 of 3770 loss ->  0.3565261960029602 acc -> 0.88\n",
            "Batch |  2045 of 3770 loss ->  0.4498779773712158 acc -> 0.88\n",
            "Batch |  2046 of 3770 loss ->  0.3837650418281555 acc -> 0.88\n",
            "Batch |  2047 of 3770 loss ->  0.37730807065963745 acc -> 0.88\n",
            "Batch |  2048 of 3770 loss ->  0.4178832471370697 acc -> 0.88\n",
            "Batch |  2049 of 3770 loss ->  0.3944989740848541 acc -> 0.88\n",
            "Batch |  2050 of 3770 loss ->  0.39627605676651 acc -> 0.88\n",
            "Batch |  2051 of 3770 loss ->  0.40232428908348083 acc -> 0.88\n",
            "Batch |  2052 of 3770 loss ->  0.36501675844192505 acc -> 0.88\n",
            "Batch |  2053 of 3770 loss ->  0.43568915128707886 acc -> 0.88\n",
            "Batch |  2054 of 3770 loss ->  0.39861345291137695 acc -> 0.88\n",
            "Batch |  2055 of 3770 loss ->  0.4005225896835327 acc -> 0.88\n",
            "Batch |  2056 of 3770 loss ->  0.43945664167404175 acc -> 0.88\n",
            "Batch |  2057 of 3770 loss ->  0.38704073429107666 acc -> 0.88\n",
            "Batch |  2058 of 3770 loss ->  0.4830096960067749 acc -> 0.88\n",
            "Batch |  2059 of 3770 loss ->  0.4566856026649475 acc -> 0.88\n",
            "Batch |  2060 of 3770 loss ->  0.3809604048728943 acc -> 0.88\n",
            "Batch |  2061 of 3770 loss ->  0.3821723461151123 acc -> 0.88\n",
            "Batch |  2062 of 3770 loss ->  0.3782958388328552 acc -> 0.88\n",
            "Batch |  2063 of 3770 loss ->  0.3995233476161957 acc -> 0.88\n",
            "Batch |  2064 of 3770 loss ->  0.4174821972846985 acc -> 0.88\n",
            "Batch |  2065 of 3770 loss ->  0.3984432518482208 acc -> 0.88\n",
            "Batch |  2066 of 3770 loss ->  0.3824274241924286 acc -> 0.88\n",
            "Batch |  2067 of 3770 loss ->  0.4438747763633728 acc -> 0.88\n",
            "Batch |  2068 of 3770 loss ->  0.37911421060562134 acc -> 0.88\n",
            "Batch |  2069 of 3770 loss ->  0.3998640775680542 acc -> 0.88\n",
            "Batch |  2070 of 3770 loss ->  0.39936137199401855 acc -> 0.88\n",
            "Batch |  2071 of 3770 loss ->  0.35586872696876526 acc -> 0.88\n",
            "Batch |  2072 of 3770 loss ->  0.38493648171424866 acc -> 0.88\n",
            "Batch |  2073 of 3770 loss ->  0.38194453716278076 acc -> 0.88\n",
            "Batch |  2074 of 3770 loss ->  0.42045119404792786 acc -> 0.88\n",
            "Batch |  2075 of 3770 loss ->  0.38845252990722656 acc -> 0.88\n",
            "Batch |  2076 of 3770 loss ->  0.38687315583229065 acc -> 0.88\n",
            "Batch |  2077 of 3770 loss ->  0.4220609664916992 acc -> 0.88\n",
            "Batch |  2078 of 3770 loss ->  0.4057912826538086 acc -> 0.88\n",
            "Batch |  2079 of 3770 loss ->  0.4028472602367401 acc -> 0.88\n",
            "Batch |  2080 of 3770 loss ->  0.3637479543685913 acc -> 0.88\n",
            "Batch |  2081 of 3770 loss ->  0.3814009130001068 acc -> 0.88\n",
            "Batch |  2082 of 3770 loss ->  0.39097368717193604 acc -> 0.88\n",
            "Batch |  2083 of 3770 loss ->  0.37098824977874756 acc -> 0.88\n",
            "Batch |  2084 of 3770 loss ->  0.36868610978126526 acc -> 0.88\n",
            "Batch |  2085 of 3770 loss ->  0.3937930762767792 acc -> 0.88\n",
            "Batch |  2086 of 3770 loss ->  0.3764130175113678 acc -> 0.88\n",
            "Batch |  2087 of 3770 loss ->  0.3917202055454254 acc -> 0.88\n",
            "Batch |  2088 of 3770 loss ->  0.4369793236255646 acc -> 0.88\n",
            "Batch |  2089 of 3770 loss ->  0.43353813886642456 acc -> 0.88\n",
            "Batch |  2090 of 3770 loss ->  0.4755263328552246 acc -> 0.88\n",
            "Batch |  2091 of 3770 loss ->  0.39249446988105774 acc -> 0.88\n",
            "Batch |  2092 of 3770 loss ->  0.382773756980896 acc -> 0.88\n",
            "Batch |  2093 of 3770 loss ->  0.4239916205406189 acc -> 0.88\n",
            "Batch |  2094 of 3770 loss ->  0.42294955253601074 acc -> 0.88\n",
            "Batch |  2095 of 3770 loss ->  0.4134749472141266 acc -> 0.88\n",
            "Batch |  2096 of 3770 loss ->  0.3651278614997864 acc -> 0.88\n",
            "Batch |  2097 of 3770 loss ->  0.40553826093673706 acc -> 0.88\n",
            "Batch |  2098 of 3770 loss ->  0.4291209280490875 acc -> 0.88\n",
            "Batch |  2099 of 3770 loss ->  0.4242681860923767 acc -> 0.88\n",
            "Batch |  2100 of 3770 loss ->  0.4024416208267212 acc -> 0.88\n",
            "Batch |  2101 of 3770 loss ->  0.36994853615760803 acc -> 0.88\n",
            "Batch |  2102 of 3770 loss ->  0.4617597460746765 acc -> 0.88\n",
            "Batch |  2103 of 3770 loss ->  0.39592015743255615 acc -> 0.88\n",
            "Batch |  2104 of 3770 loss ->  0.36386388540267944 acc -> 0.88\n",
            "Batch |  2105 of 3770 loss ->  0.3684391379356384 acc -> 0.88\n",
            "Batch |  2106 of 3770 loss ->  0.44466304779052734 acc -> 0.88\n",
            "Batch |  2107 of 3770 loss ->  0.37505877017974854 acc -> 0.88\n",
            "Batch |  2108 of 3770 loss ->  0.40323448181152344 acc -> 0.88\n",
            "Batch |  2109 of 3770 loss ->  0.48044419288635254 acc -> 0.88\n",
            "Batch |  2110 of 3770 loss ->  0.39138585329055786 acc -> 0.88\n",
            "Batch |  2111 of 3770 loss ->  0.3959433138370514 acc -> 0.88\n",
            "Batch |  2112 of 3770 loss ->  0.38314521312713623 acc -> 0.88\n",
            "Batch |  2113 of 3770 loss ->  0.3789914548397064 acc -> 0.88\n",
            "Batch |  2114 of 3770 loss ->  0.3737587332725525 acc -> 0.88\n",
            "Batch |  2115 of 3770 loss ->  0.370478093624115 acc -> 0.88\n",
            "Batch |  2116 of 3770 loss ->  0.4061223268508911 acc -> 0.88\n",
            "Batch |  2117 of 3770 loss ->  0.4257667660713196 acc -> 0.88\n",
            "Batch |  2118 of 3770 loss ->  0.40068960189819336 acc -> 0.88\n",
            "Batch |  2119 of 3770 loss ->  0.3817504644393921 acc -> 0.88\n",
            "Batch |  2120 of 3770 loss ->  0.4312160313129425 acc -> 0.88\n",
            "Batch |  2121 of 3770 loss ->  0.3736506998538971 acc -> 0.88\n",
            "Batch |  2122 of 3770 loss ->  0.36898094415664673 acc -> 0.88\n",
            "Batch |  2123 of 3770 loss ->  0.3923271596431732 acc -> 0.88\n",
            "Batch |  2124 of 3770 loss ->  0.4031234085559845 acc -> 0.88\n",
            "Batch |  2125 of 3770 loss ->  0.3849751949310303 acc -> 0.88\n",
            "Batch |  2126 of 3770 loss ->  0.3570483922958374 acc -> 0.88\n",
            "Batch |  2127 of 3770 loss ->  0.36615610122680664 acc -> 0.88\n",
            "Batch |  2128 of 3770 loss ->  0.47411906719207764 acc -> 0.88\n",
            "Batch |  2129 of 3770 loss ->  0.36651989817619324 acc -> 0.88\n",
            "Batch |  2130 of 3770 loss ->  0.37763020396232605 acc -> 0.88\n",
            "Batch |  2131 of 3770 loss ->  0.3634797930717468 acc -> 0.88\n",
            "Batch |  2132 of 3770 loss ->  0.39578503370285034 acc -> 0.88\n",
            "Batch |  2133 of 3770 loss ->  0.3979783356189728 acc -> 0.88\n",
            "Batch |  2134 of 3770 loss ->  0.36783069372177124 acc -> 0.88\n",
            "Batch |  2135 of 3770 loss ->  0.37171247601509094 acc -> 0.88\n",
            "Batch |  2136 of 3770 loss ->  0.3956170976161957 acc -> 0.88\n",
            "Batch |  2137 of 3770 loss ->  0.36734598875045776 acc -> 0.88\n",
            "Batch |  2138 of 3770 loss ->  0.4014321565628052 acc -> 0.88\n",
            "Batch |  2139 of 3770 loss ->  0.4765588045120239 acc -> 0.88\n",
            "Batch |  2140 of 3770 loss ->  0.352928102016449 acc -> 0.88\n",
            "Batch |  2141 of 3770 loss ->  0.3897310495376587 acc -> 0.88\n",
            "Batch |  2142 of 3770 loss ->  0.4302826523780823 acc -> 0.88\n",
            "Batch |  2143 of 3770 loss ->  0.4019681215286255 acc -> 0.88\n",
            "Batch |  2144 of 3770 loss ->  0.3835068345069885 acc -> 0.88\n",
            "Batch |  2145 of 3770 loss ->  0.4109630584716797 acc -> 0.88\n",
            "Batch |  2146 of 3770 loss ->  0.41864120960235596 acc -> 0.88\n",
            "Batch |  2147 of 3770 loss ->  0.415157675743103 acc -> 0.88\n",
            "Batch |  2148 of 3770 loss ->  0.40555083751678467 acc -> 0.88\n",
            "Batch |  2149 of 3770 loss ->  0.42263901233673096 acc -> 0.88\n",
            "Batch |  2150 of 3770 loss ->  0.37297213077545166 acc -> 0.88\n",
            "Batch |  2151 of 3770 loss ->  0.37278711795806885 acc -> 0.88\n",
            "Batch |  2152 of 3770 loss ->  0.3977310359477997 acc -> 0.88\n",
            "Batch |  2153 of 3770 loss ->  0.3696936070919037 acc -> 0.88\n",
            "Batch |  2154 of 3770 loss ->  0.39596807956695557 acc -> 0.88\n",
            "Batch |  2155 of 3770 loss ->  0.343418151140213 acc -> 0.88\n",
            "Batch |  2156 of 3770 loss ->  0.39501136541366577 acc -> 0.88\n",
            "Batch |  2157 of 3770 loss ->  0.3586680293083191 acc -> 0.88\n",
            "Batch |  2158 of 3770 loss ->  0.3637840151786804 acc -> 0.88\n",
            "Batch |  2159 of 3770 loss ->  0.38784754276275635 acc -> 0.88\n",
            "Batch |  2160 of 3770 loss ->  0.4075792133808136 acc -> 0.88\n",
            "Batch |  2161 of 3770 loss ->  0.37836790084838867 acc -> 0.88\n",
            "Batch |  2162 of 3770 loss ->  0.39268893003463745 acc -> 0.88\n",
            "Batch |  2163 of 3770 loss ->  0.39153969287872314 acc -> 0.88\n",
            "Batch |  2164 of 3770 loss ->  0.36971473693847656 acc -> 0.88\n",
            "Batch |  2165 of 3770 loss ->  0.4170600175857544 acc -> 0.88\n",
            "Batch |  2166 of 3770 loss ->  0.40388837456703186 acc -> 0.88\n",
            "Batch |  2167 of 3770 loss ->  0.4297946095466614 acc -> 0.88\n",
            "Batch |  2168 of 3770 loss ->  0.3627498745918274 acc -> 0.88\n",
            "Batch |  2169 of 3770 loss ->  0.3780595064163208 acc -> 0.88\n",
            "Batch |  2170 of 3770 loss ->  0.4852015972137451 acc -> 0.88\n",
            "Batch |  2171 of 3770 loss ->  0.42157289385795593 acc -> 0.88\n",
            "Batch |  2172 of 3770 loss ->  0.4226980209350586 acc -> 0.88\n",
            "Batch |  2173 of 3770 loss ->  0.4054916501045227 acc -> 0.88\n",
            "Batch |  2174 of 3770 loss ->  0.3624322712421417 acc -> 0.88\n",
            "Batch |  2175 of 3770 loss ->  0.3585341274738312 acc -> 0.88\n",
            "Batch |  2176 of 3770 loss ->  0.36853787302970886 acc -> 0.88\n",
            "Batch |  2177 of 3770 loss ->  0.3517143726348877 acc -> 0.88\n",
            "Batch |  2178 of 3770 loss ->  0.3813372254371643 acc -> 0.88\n",
            "Batch |  2179 of 3770 loss ->  0.40090417861938477 acc -> 0.88\n",
            "Batch |  2180 of 3770 loss ->  0.3981776535511017 acc -> 0.88\n",
            "Batch |  2181 of 3770 loss ->  0.3813927173614502 acc -> 0.88\n",
            "Batch |  2182 of 3770 loss ->  0.36168307065963745 acc -> 0.88\n",
            "Batch |  2183 of 3770 loss ->  0.38255465030670166 acc -> 0.88\n",
            "Batch |  2184 of 3770 loss ->  0.39571359753608704 acc -> 0.88\n",
            "Batch |  2185 of 3770 loss ->  0.4346059560775757 acc -> 0.88\n",
            "Batch |  2186 of 3770 loss ->  0.3712598979473114 acc -> 0.88\n",
            "Batch |  2187 of 3770 loss ->  0.377574622631073 acc -> 0.88\n",
            "Batch |  2188 of 3770 loss ->  0.41111209988594055 acc -> 0.88\n",
            "Batch |  2189 of 3770 loss ->  0.38261425495147705 acc -> 0.88\n",
            "Batch |  2190 of 3770 loss ->  0.3780542314052582 acc -> 0.88\n",
            "Batch |  2191 of 3770 loss ->  0.40291666984558105 acc -> 0.88\n",
            "Batch |  2192 of 3770 loss ->  0.44672727584838867 acc -> 0.88\n",
            "Batch |  2193 of 3770 loss ->  0.36205074191093445 acc -> 0.88\n",
            "Batch |  2194 of 3770 loss ->  0.4710637331008911 acc -> 0.88\n",
            "Batch |  2195 of 3770 loss ->  0.4583454132080078 acc -> 0.88\n",
            "Batch |  2196 of 3770 loss ->  0.3732171952724457 acc -> 0.88\n",
            "Batch |  2197 of 3770 loss ->  0.3924504816532135 acc -> 0.88\n",
            "Batch |  2198 of 3770 loss ->  0.34587278962135315 acc -> 0.88\n",
            "Batch |  2199 of 3770 loss ->  0.349418044090271 acc -> 0.88\n",
            "Batch |  2200 of 3770 loss ->  0.4842117726802826 acc -> 0.88\n",
            "Batch |  2201 of 3770 loss ->  0.3969799876213074 acc -> 0.88\n",
            "Batch |  2202 of 3770 loss ->  0.371773362159729 acc -> 0.88\n",
            "Batch |  2203 of 3770 loss ->  0.42583054304122925 acc -> 0.88\n",
            "Batch |  2204 of 3770 loss ->  0.3451559543609619 acc -> 0.88\n",
            "Batch |  2205 of 3770 loss ->  0.3806411325931549 acc -> 0.88\n",
            "Batch |  2206 of 3770 loss ->  0.37680691480636597 acc -> 0.88\n",
            "Batch |  2207 of 3770 loss ->  0.38316109776496887 acc -> 0.88\n",
            "Batch |  2208 of 3770 loss ->  0.35993438959121704 acc -> 0.88\n",
            "Batch |  2209 of 3770 loss ->  0.38105741143226624 acc -> 0.88\n",
            "Batch |  2210 of 3770 loss ->  0.3747425079345703 acc -> 0.88\n",
            "Batch |  2211 of 3770 loss ->  0.34688815474510193 acc -> 0.88\n",
            "Batch |  2212 of 3770 loss ->  0.38454198837280273 acc -> 0.88\n",
            "Batch |  2213 of 3770 loss ->  0.37092867493629456 acc -> 0.88\n",
            "Batch |  2214 of 3770 loss ->  0.45777153968811035 acc -> 0.88\n",
            "Batch |  2215 of 3770 loss ->  0.3684147000312805 acc -> 0.88\n",
            "Batch |  2216 of 3770 loss ->  0.3704485297203064 acc -> 0.88\n",
            "Batch |  2217 of 3770 loss ->  0.3799542784690857 acc -> 0.88\n",
            "Batch |  2218 of 3770 loss ->  0.3465113043785095 acc -> 0.88\n",
            "Batch |  2219 of 3770 loss ->  0.36969929933547974 acc -> 0.88\n",
            "Batch |  2220 of 3770 loss ->  0.4493812918663025 acc -> 0.88\n",
            "Batch |  2221 of 3770 loss ->  0.36137518286705017 acc -> 0.88\n",
            "Batch |  2222 of 3770 loss ->  0.35914409160614014 acc -> 0.88\n",
            "Batch |  2223 of 3770 loss ->  0.38755184412002563 acc -> 0.88\n",
            "Batch |  2224 of 3770 loss ->  0.4388795495033264 acc -> 0.88\n",
            "Batch |  2225 of 3770 loss ->  0.32231730222702026 acc -> 0.88\n",
            "Batch |  2226 of 3770 loss ->  0.3951963782310486 acc -> 0.88\n",
            "Batch |  2227 of 3770 loss ->  0.4020861089229584 acc -> 0.88\n",
            "Batch |  2228 of 3770 loss ->  0.4187042713165283 acc -> 0.88\n",
            "Batch |  2229 of 3770 loss ->  0.3780345320701599 acc -> 0.88\n",
            "Batch |  2230 of 3770 loss ->  0.38977381587028503 acc -> 0.88\n",
            "Batch |  2231 of 3770 loss ->  0.40995892882347107 acc -> 0.88\n",
            "Batch |  2232 of 3770 loss ->  0.37730807065963745 acc -> 0.88\n",
            "Batch |  2233 of 3770 loss ->  0.4373786449432373 acc -> 0.88\n",
            "Batch |  2234 of 3770 loss ->  0.46641331911087036 acc -> 0.88\n",
            "Batch |  2235 of 3770 loss ->  0.3570247292518616 acc -> 0.88\n",
            "Batch |  2236 of 3770 loss ->  0.4225956201553345 acc -> 0.88\n",
            "Batch |  2237 of 3770 loss ->  0.37221139669418335 acc -> 0.88\n",
            "Batch |  2238 of 3770 loss ->  0.36293119192123413 acc -> 0.88\n",
            "Batch |  2239 of 3770 loss ->  0.3884182274341583 acc -> 0.88\n",
            "Batch |  2240 of 3770 loss ->  0.3512336015701294 acc -> 0.88\n",
            "Batch |  2241 of 3770 loss ->  0.34725284576416016 acc -> 0.88\n",
            "Batch |  2242 of 3770 loss ->  0.40620332956314087 acc -> 0.88\n",
            "Batch |  2243 of 3770 loss ->  0.36371269822120667 acc -> 0.88\n",
            "Batch |  2244 of 3770 loss ->  0.3815709352493286 acc -> 0.89\n",
            "Batch |  2245 of 3770 loss ->  0.3548448383808136 acc -> 0.89\n",
            "Batch |  2246 of 3770 loss ->  0.3664633631706238 acc -> 0.89\n",
            "Batch |  2247 of 3770 loss ->  0.3796570301055908 acc -> 0.89\n",
            "Batch |  2248 of 3770 loss ->  0.38696205615997314 acc -> 0.89\n",
            "Batch |  2249 of 3770 loss ->  0.3572634756565094 acc -> 0.89\n",
            "Batch |  2250 of 3770 loss ->  0.3935897946357727 acc -> 0.89\n",
            "Batch |  2251 of 3770 loss ->  0.3626592755317688 acc -> 0.89\n",
            "Batch |  2252 of 3770 loss ->  0.40304034948349 acc -> 0.89\n",
            "Batch |  2253 of 3770 loss ->  0.3493875563144684 acc -> 0.89\n",
            "Batch |  2254 of 3770 loss ->  0.36877450346946716 acc -> 0.89\n",
            "Batch |  2255 of 3770 loss ->  0.391387939453125 acc -> 0.89\n",
            "Batch |  2256 of 3770 loss ->  0.3801463544368744 acc -> 0.89\n",
            "Batch |  2257 of 3770 loss ->  0.33630305528640747 acc -> 0.89\n",
            "Batch |  2258 of 3770 loss ->  0.35789501667022705 acc -> 0.89\n",
            "Batch |  2259 of 3770 loss ->  0.4176664650440216 acc -> 0.89\n",
            "Batch |  2260 of 3770 loss ->  0.3688091039657593 acc -> 0.89\n",
            "Batch |  2261 of 3770 loss ->  0.3833632469177246 acc -> 0.89\n",
            "Batch |  2262 of 3770 loss ->  0.4735317528247833 acc -> 0.89\n",
            "Batch |  2263 of 3770 loss ->  0.41161876916885376 acc -> 0.89\n",
            "Batch |  2264 of 3770 loss ->  0.3916185200214386 acc -> 0.89\n",
            "Batch |  2265 of 3770 loss ->  0.3592512011528015 acc -> 0.89\n",
            "Batch |  2266 of 3770 loss ->  0.35850730538368225 acc -> 0.89\n",
            "Batch |  2267 of 3770 loss ->  0.390802264213562 acc -> 0.89\n",
            "Batch |  2268 of 3770 loss ->  0.4234336316585541 acc -> 0.89\n",
            "Batch |  2269 of 3770 loss ->  0.3917163014411926 acc -> 0.89\n",
            "Batch |  2270 of 3770 loss ->  0.39164066314697266 acc -> 0.89\n",
            "Batch |  2271 of 3770 loss ->  0.43719977140426636 acc -> 0.89\n",
            "Batch |  2272 of 3770 loss ->  0.40745896100997925 acc -> 0.89\n",
            "Batch |  2273 of 3770 loss ->  0.3912884294986725 acc -> 0.89\n",
            "Batch |  2274 of 3770 loss ->  0.419927179813385 acc -> 0.89\n",
            "Batch |  2275 of 3770 loss ->  0.3807936906814575 acc -> 0.89\n",
            "Batch |  2276 of 3770 loss ->  0.4040730893611908 acc -> 0.89\n",
            "Batch |  2277 of 3770 loss ->  0.3770539462566376 acc -> 0.89\n",
            "Batch |  2278 of 3770 loss ->  0.44352495670318604 acc -> 0.89\n",
            "Batch |  2279 of 3770 loss ->  0.3951539695262909 acc -> 0.89\n",
            "Batch |  2280 of 3770 loss ->  0.4391568899154663 acc -> 0.89\n",
            "Batch |  2281 of 3770 loss ->  0.37803471088409424 acc -> 0.89\n",
            "Batch |  2282 of 3770 loss ->  0.4009866714477539 acc -> 0.89\n",
            "Batch |  2283 of 3770 loss ->  0.39037227630615234 acc -> 0.89\n",
            "Batch |  2284 of 3770 loss ->  0.37252077460289 acc -> 0.89\n",
            "Batch |  2285 of 3770 loss ->  0.3697736859321594 acc -> 0.89\n",
            "Batch |  2286 of 3770 loss ->  0.3656402826309204 acc -> 0.89\n",
            "Batch |  2287 of 3770 loss ->  0.3639206886291504 acc -> 0.89\n",
            "Batch |  2288 of 3770 loss ->  0.34322530031204224 acc -> 0.89\n",
            "Batch |  2289 of 3770 loss ->  0.41240155696868896 acc -> 0.89\n",
            "Batch |  2290 of 3770 loss ->  0.34911566972732544 acc -> 0.89\n",
            "Batch |  2291 of 3770 loss ->  0.3696972131729126 acc -> 0.89\n",
            "Batch |  2292 of 3770 loss ->  0.3637288808822632 acc -> 0.89\n",
            "Batch |  2293 of 3770 loss ->  0.3681045174598694 acc -> 0.89\n",
            "Batch |  2294 of 3770 loss ->  0.33877018094062805 acc -> 0.89\n",
            "Batch |  2295 of 3770 loss ->  0.3638192415237427 acc -> 0.89\n",
            "Batch |  2296 of 3770 loss ->  0.36275044083595276 acc -> 0.89\n",
            "Batch |  2297 of 3770 loss ->  0.3743176758289337 acc -> 0.89\n",
            "Batch |  2298 of 3770 loss ->  0.3304416537284851 acc -> 0.89\n",
            "Batch |  2299 of 3770 loss ->  0.3415718972682953 acc -> 0.89\n",
            "Batch |  2300 of 3770 loss ->  0.374295711517334 acc -> 0.89\n",
            "Batch |  2301 of 3770 loss ->  0.4249469041824341 acc -> 0.89\n",
            "Batch |  2302 of 3770 loss ->  0.41289326548576355 acc -> 0.89\n",
            "Batch |  2303 of 3770 loss ->  0.454447865486145 acc -> 0.89\n",
            "Batch |  2304 of 3770 loss ->  0.371804416179657 acc -> 0.89\n",
            "Batch |  2305 of 3770 loss ->  0.35137939453125 acc -> 0.89\n",
            "Batch |  2306 of 3770 loss ->  0.37019869685173035 acc -> 0.89\n",
            "Batch |  2307 of 3770 loss ->  0.3483479619026184 acc -> 0.89\n",
            "Batch |  2308 of 3770 loss ->  0.3649139404296875 acc -> 0.89\n",
            "Batch |  2309 of 3770 loss ->  0.39309561252593994 acc -> 0.89\n",
            "Batch |  2310 of 3770 loss ->  0.36691826581954956 acc -> 0.89\n",
            "Batch |  2311 of 3770 loss ->  0.4224596917629242 acc -> 0.89\n",
            "Batch |  2312 of 3770 loss ->  0.37294381856918335 acc -> 0.89\n",
            "Batch |  2313 of 3770 loss ->  0.3529273569583893 acc -> 0.89\n",
            "Batch |  2314 of 3770 loss ->  0.4119694232940674 acc -> 0.89\n",
            "Batch |  2315 of 3770 loss ->  0.3215949535369873 acc -> 0.89\n",
            "Batch |  2316 of 3770 loss ->  0.3843267560005188 acc -> 0.89\n",
            "Batch |  2317 of 3770 loss ->  0.3739168345928192 acc -> 0.89\n",
            "Batch |  2318 of 3770 loss ->  0.37185221910476685 acc -> 0.89\n",
            "Batch |  2319 of 3770 loss ->  0.3993324637413025 acc -> 0.89\n",
            "Batch |  2320 of 3770 loss ->  0.3517698645591736 acc -> 0.89\n",
            "Batch |  2321 of 3770 loss ->  0.3576224446296692 acc -> 0.89\n",
            "Batch |  2322 of 3770 loss ->  0.36647987365722656 acc -> 0.89\n",
            "Batch |  2323 of 3770 loss ->  0.41902753710746765 acc -> 0.89\n",
            "Batch |  2324 of 3770 loss ->  0.3578276038169861 acc -> 0.89\n",
            "Batch |  2325 of 3770 loss ->  0.40607205033302307 acc -> 0.89\n",
            "Batch |  2326 of 3770 loss ->  0.48327338695526123 acc -> 0.89\n",
            "Batch |  2327 of 3770 loss ->  0.3628581166267395 acc -> 0.89\n",
            "Batch |  2328 of 3770 loss ->  0.43354952335357666 acc -> 0.89\n",
            "Batch |  2329 of 3770 loss ->  0.3919304609298706 acc -> 0.89\n",
            "Batch |  2330 of 3770 loss ->  0.37560558319091797 acc -> 0.89\n",
            "Batch |  2331 of 3770 loss ->  0.3802981376647949 acc -> 0.89\n",
            "Batch |  2332 of 3770 loss ->  0.3736046254634857 acc -> 0.89\n",
            "Batch |  2333 of 3770 loss ->  0.43028441071510315 acc -> 0.89\n",
            "Batch |  2334 of 3770 loss ->  0.38053736090660095 acc -> 0.89\n",
            "Batch |  2335 of 3770 loss ->  0.42389947175979614 acc -> 0.89\n",
            "Batch |  2336 of 3770 loss ->  0.39523953199386597 acc -> 0.89\n",
            "Batch |  2337 of 3770 loss ->  0.39472851157188416 acc -> 0.89\n",
            "Batch |  2338 of 3770 loss ->  0.35320404171943665 acc -> 0.89\n",
            "Batch |  2339 of 3770 loss ->  0.3570501208305359 acc -> 0.89\n",
            "Batch |  2340 of 3770 loss ->  0.38567548990249634 acc -> 0.89\n",
            "Batch |  2341 of 3770 loss ->  0.3563597798347473 acc -> 0.89\n",
            "Batch |  2342 of 3770 loss ->  0.3912854790687561 acc -> 0.89\n",
            "Batch |  2343 of 3770 loss ->  0.39843636751174927 acc -> 0.89\n",
            "Batch |  2344 of 3770 loss ->  0.3573988676071167 acc -> 0.89\n",
            "Batch |  2345 of 3770 loss ->  0.38483691215515137 acc -> 0.89\n",
            "Batch |  2346 of 3770 loss ->  0.46591097116470337 acc -> 0.89\n",
            "Batch |  2347 of 3770 loss ->  0.3343324065208435 acc -> 0.89\n",
            "Batch |  2348 of 3770 loss ->  0.4295770525932312 acc -> 0.89\n",
            "Batch |  2349 of 3770 loss ->  0.38655829429626465 acc -> 0.89\n",
            "Batch |  2350 of 3770 loss ->  0.38023364543914795 acc -> 0.89\n",
            "Batch |  2351 of 3770 loss ->  0.37879616022109985 acc -> 0.89\n",
            "Batch |  2352 of 3770 loss ->  0.3517988920211792 acc -> 0.89\n",
            "Batch |  2353 of 3770 loss ->  0.366731733083725 acc -> 0.89\n",
            "Batch |  2354 of 3770 loss ->  0.4115843176841736 acc -> 0.89\n",
            "Batch |  2355 of 3770 loss ->  0.4167096018791199 acc -> 0.89\n",
            "Batch |  2356 of 3770 loss ->  0.40104299783706665 acc -> 0.89\n",
            "Batch |  2357 of 3770 loss ->  0.3700011670589447 acc -> 0.89\n",
            "Batch |  2358 of 3770 loss ->  0.36360523104667664 acc -> 0.89\n",
            "Batch |  2359 of 3770 loss ->  0.3608085513114929 acc -> 0.89\n",
            "Batch |  2360 of 3770 loss ->  0.3693869411945343 acc -> 0.89\n",
            "Batch |  2361 of 3770 loss ->  0.4171218276023865 acc -> 0.89\n",
            "Batch |  2362 of 3770 loss ->  0.34791460633277893 acc -> 0.89\n",
            "Batch |  2363 of 3770 loss ->  0.3709665834903717 acc -> 0.89\n",
            "Batch |  2364 of 3770 loss ->  0.3965700566768646 acc -> 0.89\n",
            "Batch |  2365 of 3770 loss ->  0.3759867548942566 acc -> 0.89\n",
            "Batch |  2366 of 3770 loss ->  0.377849280834198 acc -> 0.89\n",
            "Batch |  2367 of 3770 loss ->  0.38317787647247314 acc -> 0.89\n",
            "Batch |  2368 of 3770 loss ->  0.3701626658439636 acc -> 0.89\n",
            "Batch |  2369 of 3770 loss ->  0.3755808174610138 acc -> 0.89\n",
            "Batch |  2370 of 3770 loss ->  0.3728513717651367 acc -> 0.89\n",
            "Batch |  2371 of 3770 loss ->  0.4006657004356384 acc -> 0.89\n",
            "Batch |  2372 of 3770 loss ->  0.3932589292526245 acc -> 0.89\n",
            "Batch |  2373 of 3770 loss ->  0.3906841278076172 acc -> 0.89\n",
            "Batch |  2374 of 3770 loss ->  0.3749237060546875 acc -> 0.89\n",
            "Batch |  2375 of 3770 loss ->  0.3620234727859497 acc -> 0.89\n",
            "Batch |  2376 of 3770 loss ->  0.34829217195510864 acc -> 0.89\n",
            "Batch |  2377 of 3770 loss ->  0.45526838302612305 acc -> 0.89\n",
            "Batch |  2378 of 3770 loss ->  0.3143100142478943 acc -> 0.89\n",
            "Batch |  2379 of 3770 loss ->  0.3885003924369812 acc -> 0.89\n",
            "Batch |  2380 of 3770 loss ->  0.355330228805542 acc -> 0.89\n",
            "Batch |  2381 of 3770 loss ->  0.3853601813316345 acc -> 0.89\n",
            "Batch |  2382 of 3770 loss ->  0.38438206911087036 acc -> 0.89\n",
            "Batch |  2383 of 3770 loss ->  0.45820510387420654 acc -> 0.89\n",
            "Batch |  2384 of 3770 loss ->  0.3398924469947815 acc -> 0.89\n",
            "Batch |  2385 of 3770 loss ->  0.39625903964042664 acc -> 0.89\n",
            "Batch |  2386 of 3770 loss ->  0.37177416682243347 acc -> 0.89\n",
            "Batch |  2387 of 3770 loss ->  0.3832617700099945 acc -> 0.89\n",
            "Batch |  2388 of 3770 loss ->  0.35868069529533386 acc -> 0.89\n",
            "Batch |  2389 of 3770 loss ->  0.516913652420044 acc -> 0.89\n",
            "Batch |  2390 of 3770 loss ->  0.3725252151489258 acc -> 0.89\n",
            "Batch |  2391 of 3770 loss ->  0.3729468584060669 acc -> 0.89\n",
            "Batch |  2392 of 3770 loss ->  0.4691906273365021 acc -> 0.89\n",
            "Batch |  2393 of 3770 loss ->  0.37296342849731445 acc -> 0.89\n",
            "Batch |  2394 of 3770 loss ->  0.3281671404838562 acc -> 0.89\n",
            "Batch |  2395 of 3770 loss ->  0.3778797388076782 acc -> 0.89\n",
            "Batch |  2396 of 3770 loss ->  0.32227373123168945 acc -> 0.89\n",
            "Batch |  2397 of 3770 loss ->  0.3456961512565613 acc -> 0.89\n",
            "Batch |  2398 of 3770 loss ->  0.3427883982658386 acc -> 0.89\n",
            "Batch |  2399 of 3770 loss ->  0.3442405164241791 acc -> 0.89\n",
            "Batch |  2400 of 3770 loss ->  0.34441086649894714 acc -> 0.89\n",
            "Batch |  2401 of 3770 loss ->  0.37051427364349365 acc -> 0.89\n",
            "Batch |  2402 of 3770 loss ->  0.37737494707107544 acc -> 0.89\n",
            "Batch |  2403 of 3770 loss ->  0.33517563343048096 acc -> 0.89\n",
            "Batch |  2404 of 3770 loss ->  0.4045112729072571 acc -> 0.89\n",
            "Batch |  2405 of 3770 loss ->  0.4044988453388214 acc -> 0.89\n",
            "Batch |  2406 of 3770 loss ->  0.3611828684806824 acc -> 0.89\n",
            "Batch |  2407 of 3770 loss ->  0.3318711519241333 acc -> 0.89\n",
            "Batch |  2408 of 3770 loss ->  0.3568401336669922 acc -> 0.89\n",
            "Batch |  2409 of 3770 loss ->  0.39540520310401917 acc -> 0.89\n",
            "Batch |  2410 of 3770 loss ->  0.33610692620277405 acc -> 0.89\n",
            "Batch |  2411 of 3770 loss ->  0.4240598678588867 acc -> 0.89\n",
            "Batch |  2412 of 3770 loss ->  0.3973437547683716 acc -> 0.89\n",
            "Batch |  2413 of 3770 loss ->  0.394019216299057 acc -> 0.89\n",
            "Batch |  2414 of 3770 loss ->  0.36573100090026855 acc -> 0.89\n",
            "Batch |  2415 of 3770 loss ->  0.39630985260009766 acc -> 0.89\n",
            "Batch |  2416 of 3770 loss ->  0.3366985023021698 acc -> 0.89\n",
            "Batch |  2417 of 3770 loss ->  0.3930494785308838 acc -> 0.89\n",
            "Batch |  2418 of 3770 loss ->  0.34336012601852417 acc -> 0.89\n",
            "Batch |  2419 of 3770 loss ->  0.37359818816185 acc -> 0.89\n",
            "Batch |  2420 of 3770 loss ->  0.35676366090774536 acc -> 0.89\n",
            "Batch |  2421 of 3770 loss ->  0.4510941505432129 acc -> 0.89\n",
            "Batch |  2422 of 3770 loss ->  0.3682123124599457 acc -> 0.89\n",
            "Batch |  2423 of 3770 loss ->  0.3756459355354309 acc -> 0.89\n",
            "Batch |  2424 of 3770 loss ->  0.3819516897201538 acc -> 0.89\n",
            "Batch |  2425 of 3770 loss ->  0.3972548246383667 acc -> 0.89\n",
            "Batch |  2426 of 3770 loss ->  0.3958737254142761 acc -> 0.89\n",
            "Batch |  2427 of 3770 loss ->  0.4713539779186249 acc -> 0.89\n",
            "Batch |  2428 of 3770 loss ->  0.40076345205307007 acc -> 0.89\n",
            "Batch |  2429 of 3770 loss ->  0.4116128087043762 acc -> 0.89\n",
            "Batch |  2430 of 3770 loss ->  0.3512050211429596 acc -> 0.89\n",
            "Batch |  2431 of 3770 loss ->  0.3640206456184387 acc -> 0.89\n",
            "Batch |  2432 of 3770 loss ->  0.40149959921836853 acc -> 0.89\n",
            "Batch |  2433 of 3770 loss ->  0.35791611671447754 acc -> 0.89\n",
            "Batch |  2434 of 3770 loss ->  0.3892243802547455 acc -> 0.89\n",
            "Batch |  2435 of 3770 loss ->  0.3724343180656433 acc -> 0.89\n",
            "Batch |  2436 of 3770 loss ->  0.3275080919265747 acc -> 0.89\n",
            "Batch |  2437 of 3770 loss ->  0.3797423243522644 acc -> 0.89\n",
            "Batch |  2438 of 3770 loss ->  0.3649542033672333 acc -> 0.89\n",
            "Batch |  2439 of 3770 loss ->  0.3720254898071289 acc -> 0.89\n",
            "Batch |  2440 of 3770 loss ->  0.32236239314079285 acc -> 0.89\n",
            "Batch |  2441 of 3770 loss ->  0.33318811655044556 acc -> 0.89\n",
            "Batch |  2442 of 3770 loss ->  0.4417343735694885 acc -> 0.89\n",
            "Batch |  2443 of 3770 loss ->  0.3940753936767578 acc -> 0.89\n",
            "Batch |  2444 of 3770 loss ->  0.38963961601257324 acc -> 0.89\n",
            "Batch |  2445 of 3770 loss ->  0.3450963497161865 acc -> 0.89\n",
            "Batch |  2446 of 3770 loss ->  0.37255215644836426 acc -> 0.89\n",
            "Batch |  2447 of 3770 loss ->  0.3350674510002136 acc -> 0.89\n",
            "Batch |  2448 of 3770 loss ->  0.4241609573364258 acc -> 0.89\n",
            "Batch |  2449 of 3770 loss ->  0.39499759674072266 acc -> 0.89\n",
            "Batch |  2450 of 3770 loss ->  0.4725240468978882 acc -> 0.89\n",
            "Batch |  2451 of 3770 loss ->  0.38190561532974243 acc -> 0.89\n",
            "Batch |  2452 of 3770 loss ->  0.32494503259658813 acc -> 0.89\n",
            "Batch |  2453 of 3770 loss ->  0.3951866030693054 acc -> 0.89\n",
            "Batch |  2454 of 3770 loss ->  0.4033595621585846 acc -> 0.89\n",
            "Batch |  2455 of 3770 loss ->  0.3784298896789551 acc -> 0.89\n",
            "Batch |  2456 of 3770 loss ->  0.39163312315940857 acc -> 0.89\n",
            "Batch |  2457 of 3770 loss ->  0.3912457823753357 acc -> 0.89\n",
            "Batch |  2458 of 3770 loss ->  0.4391779601573944 acc -> 0.89\n",
            "Batch |  2459 of 3770 loss ->  0.41240647435188293 acc -> 0.89\n",
            "Batch |  2460 of 3770 loss ->  0.35060250759124756 acc -> 0.89\n",
            "Batch |  2461 of 3770 loss ->  0.3546537756919861 acc -> 0.89\n",
            "Batch |  2462 of 3770 loss ->  0.3385489284992218 acc -> 0.89\n",
            "Batch |  2463 of 3770 loss ->  0.4107605218887329 acc -> 0.89\n",
            "Batch |  2464 of 3770 loss ->  0.38977187871932983 acc -> 0.89\n",
            "Batch |  2465 of 3770 loss ->  0.39177870750427246 acc -> 0.89\n",
            "Batch |  2466 of 3770 loss ->  0.3965821862220764 acc -> 0.89\n",
            "Batch |  2467 of 3770 loss ->  0.3632402718067169 acc -> 0.89\n",
            "Batch |  2468 of 3770 loss ->  0.3156090974807739 acc -> 0.89\n",
            "Batch |  2469 of 3770 loss ->  0.32939091324806213 acc -> 0.89\n",
            "Batch |  2470 of 3770 loss ->  0.34001654386520386 acc -> 0.89\n",
            "Batch |  2471 of 3770 loss ->  0.37325161695480347 acc -> 0.89\n",
            "Batch |  2472 of 3770 loss ->  0.3758535087108612 acc -> 0.89\n",
            "Batch |  2473 of 3770 loss ->  0.42434030771255493 acc -> 0.89\n",
            "Batch |  2474 of 3770 loss ->  0.3625980019569397 acc -> 0.89\n",
            "Batch |  2475 of 3770 loss ->  0.3813445568084717 acc -> 0.89\n",
            "Batch |  2476 of 3770 loss ->  0.4167596101760864 acc -> 0.89\n",
            "Batch |  2477 of 3770 loss ->  0.3745594620704651 acc -> 0.89\n",
            "Batch |  2478 of 3770 loss ->  0.3628501296043396 acc -> 0.89\n",
            "Batch |  2479 of 3770 loss ->  0.3531220257282257 acc -> 0.89\n",
            "Batch |  2480 of 3770 loss ->  0.3754662573337555 acc -> 0.89\n",
            "Batch |  2481 of 3770 loss ->  0.34840789437294006 acc -> 0.89\n",
            "Batch |  2482 of 3770 loss ->  0.39685821533203125 acc -> 0.89\n",
            "Batch |  2483 of 3770 loss ->  0.36290618777275085 acc -> 0.89\n",
            "Batch |  2484 of 3770 loss ->  0.35901665687561035 acc -> 0.89\n",
            "Batch |  2485 of 3770 loss ->  0.37070298194885254 acc -> 0.89\n",
            "Batch |  2486 of 3770 loss ->  0.33370494842529297 acc -> 0.89\n",
            "Batch |  2487 of 3770 loss ->  0.44542813301086426 acc -> 0.89\n",
            "Batch |  2488 of 3770 loss ->  0.3846769332885742 acc -> 0.89\n",
            "Batch |  2489 of 3770 loss ->  0.35106539726257324 acc -> 0.89\n",
            "Batch |  2490 of 3770 loss ->  0.33501988649368286 acc -> 0.89\n",
            "Batch |  2491 of 3770 loss ->  0.3518689274787903 acc -> 0.89\n",
            "Batch |  2492 of 3770 loss ->  0.4751070439815521 acc -> 0.89\n",
            "Batch |  2493 of 3770 loss ->  0.3368436098098755 acc -> 0.89\n",
            "Batch |  2494 of 3770 loss ->  0.43293121457099915 acc -> 0.89\n",
            "Batch |  2495 of 3770 loss ->  0.36387795209884644 acc -> 0.89\n",
            "Batch |  2496 of 3770 loss ->  0.3579353094100952 acc -> 0.89\n",
            "Batch |  2497 of 3770 loss ->  0.3850621283054352 acc -> 0.89\n",
            "Batch |  2498 of 3770 loss ->  0.38317736983299255 acc -> 0.89\n",
            "Batch |  2499 of 3770 loss ->  0.3855193853378296 acc -> 0.89\n",
            "Batch |  2500 of 3770 loss ->  0.37287476658821106 acc -> 0.89\n",
            "Batch |  2501 of 3770 loss ->  0.36141306161880493 acc -> 0.89\n",
            "Batch |  2502 of 3770 loss ->  0.37855932116508484 acc -> 0.89\n",
            "Batch |  2503 of 3770 loss ->  0.3783249855041504 acc -> 0.89\n",
            "Batch |  2504 of 3770 loss ->  0.3715108335018158 acc -> 0.89\n",
            "Batch |  2505 of 3770 loss ->  0.3364051580429077 acc -> 0.89\n",
            "Batch |  2506 of 3770 loss ->  0.36642664670944214 acc -> 0.89\n",
            "Batch |  2507 of 3770 loss ->  0.34336143732070923 acc -> 0.89\n",
            "Batch |  2508 of 3770 loss ->  0.3738401532173157 acc -> 0.89\n",
            "Batch |  2509 of 3770 loss ->  0.4034281373023987 acc -> 0.89\n",
            "Batch |  2510 of 3770 loss ->  0.38848239183425903 acc -> 0.89\n",
            "Batch |  2511 of 3770 loss ->  0.33839452266693115 acc -> 0.89\n",
            "Batch |  2512 of 3770 loss ->  0.35842570662498474 acc -> 0.89\n",
            "Batch |  2513 of 3770 loss ->  0.34974348545074463 acc -> 0.89\n",
            "Batch |  2514 of 3770 loss ->  0.33991706371307373 acc -> 0.89\n",
            "Batch |  2515 of 3770 loss ->  0.3399087190628052 acc -> 0.89\n",
            "Batch |  2516 of 3770 loss ->  0.35500019788742065 acc -> 0.89\n",
            "Batch |  2517 of 3770 loss ->  0.3640313148498535 acc -> 0.89\n",
            "Batch |  2518 of 3770 loss ->  0.4088418483734131 acc -> 0.89\n",
            "Batch |  2519 of 3770 loss ->  0.43129003047943115 acc -> 0.89\n",
            "Batch |  2520 of 3770 loss ->  0.3520342707633972 acc -> 0.89\n",
            "Batch |  2521 of 3770 loss ->  0.336723268032074 acc -> 0.89\n",
            "Batch |  2522 of 3770 loss ->  0.4142884612083435 acc -> 0.89\n",
            "Batch |  2523 of 3770 loss ->  0.347391277551651 acc -> 0.89\n",
            "Batch |  2524 of 3770 loss ->  0.3177124857902527 acc -> 0.89\n",
            "Batch |  2525 of 3770 loss ->  0.327980637550354 acc -> 0.89\n",
            "Batch |  2526 of 3770 loss ->  0.326194167137146 acc -> 0.89\n",
            "Batch |  2527 of 3770 loss ->  0.375518798828125 acc -> 0.89\n",
            "Batch |  2528 of 3770 loss ->  0.36589890718460083 acc -> 0.89\n",
            "Batch |  2529 of 3770 loss ->  0.33202439546585083 acc -> 0.89\n",
            "Batch |  2530 of 3770 loss ->  0.3856622874736786 acc -> 0.89\n",
            "Batch |  2531 of 3770 loss ->  0.3558509647846222 acc -> 0.89\n",
            "Batch |  2532 of 3770 loss ->  0.4276658296585083 acc -> 0.89\n",
            "Batch |  2533 of 3770 loss ->  0.34985339641571045 acc -> 0.89\n",
            "Batch |  2534 of 3770 loss ->  0.371390163898468 acc -> 0.89\n",
            "Batch |  2535 of 3770 loss ->  0.3293507993221283 acc -> 0.89\n",
            "Batch |  2536 of 3770 loss ->  0.3983727693557739 acc -> 0.89\n",
            "Batch |  2537 of 3770 loss ->  0.32750657200813293 acc -> 0.89\n",
            "Batch |  2538 of 3770 loss ->  0.36918455362319946 acc -> 0.89\n",
            "Batch |  2539 of 3770 loss ->  0.35725539922714233 acc -> 0.89\n",
            "Batch |  2540 of 3770 loss ->  0.401339054107666 acc -> 0.89\n",
            "Batch |  2541 of 3770 loss ->  0.378160297870636 acc -> 0.89\n",
            "Batch |  2542 of 3770 loss ->  0.3464725911617279 acc -> 0.89\n",
            "Batch |  2543 of 3770 loss ->  0.38806527853012085 acc -> 0.89\n",
            "Batch |  2544 of 3770 loss ->  0.4107682406902313 acc -> 0.89\n",
            "Batch |  2545 of 3770 loss ->  0.3687802851200104 acc -> 0.89\n",
            "Batch |  2546 of 3770 loss ->  0.4481517970561981 acc -> 0.89\n",
            "Batch |  2547 of 3770 loss ->  0.3755509853363037 acc -> 0.89\n",
            "Batch |  2548 of 3770 loss ->  0.3658459186553955 acc -> 0.89\n",
            "Batch |  2549 of 3770 loss ->  0.39500272274017334 acc -> 0.89\n",
            "Batch |  2550 of 3770 loss ->  0.3513443171977997 acc -> 0.89\n",
            "Batch |  2551 of 3770 loss ->  0.4706880450248718 acc -> 0.89\n",
            "Batch |  2552 of 3770 loss ->  0.39749687910079956 acc -> 0.89\n",
            "Batch |  2553 of 3770 loss ->  0.3467070758342743 acc -> 0.89\n",
            "Batch |  2554 of 3770 loss ->  0.3805513083934784 acc -> 0.89\n",
            "Batch |  2555 of 3770 loss ->  0.38282519578933716 acc -> 0.89\n",
            "Batch |  2556 of 3770 loss ->  0.3909953534603119 acc -> 0.89\n",
            "Batch |  2557 of 3770 loss ->  0.38571417331695557 acc -> 0.89\n",
            "Batch |  2558 of 3770 loss ->  0.3483663499355316 acc -> 0.89\n",
            "Batch |  2559 of 3770 loss ->  0.35035693645477295 acc -> 0.89\n",
            "Batch |  2560 of 3770 loss ->  0.3517642021179199 acc -> 0.89\n",
            "Batch |  2561 of 3770 loss ->  0.3853415846824646 acc -> 0.89\n",
            "Batch |  2562 of 3770 loss ->  0.3403037190437317 acc -> 0.89\n",
            "Batch |  2563 of 3770 loss ->  0.3433108925819397 acc -> 0.89\n",
            "Batch |  2564 of 3770 loss ->  0.3490496575832367 acc -> 0.89\n",
            "Batch |  2565 of 3770 loss ->  0.3691835403442383 acc -> 0.89\n",
            "Batch |  2566 of 3770 loss ->  0.3517823815345764 acc -> 0.89\n",
            "Batch |  2567 of 3770 loss ->  0.3931710720062256 acc -> 0.89\n",
            "Batch |  2568 of 3770 loss ->  0.3794042766094208 acc -> 0.89\n",
            "Batch |  2569 of 3770 loss ->  0.3527368903160095 acc -> 0.89\n",
            "Batch |  2570 of 3770 loss ->  0.3510846495628357 acc -> 0.89\n",
            "Batch |  2571 of 3770 loss ->  0.410331666469574 acc -> 0.89\n",
            "Batch |  2572 of 3770 loss ->  0.31932199001312256 acc -> 0.89\n",
            "Batch |  2573 of 3770 loss ->  0.44105011224746704 acc -> 0.89\n",
            "Batch |  2574 of 3770 loss ->  0.4395695626735687 acc -> 0.89\n",
            "Batch |  2575 of 3770 loss ->  0.35168394446372986 acc -> 0.89\n",
            "Batch |  2576 of 3770 loss ->  0.37418895959854126 acc -> 0.89\n",
            "Batch |  2577 of 3770 loss ->  0.3995082676410675 acc -> 0.89\n",
            "Batch |  2578 of 3770 loss ->  0.37233737111091614 acc -> 0.89\n",
            "Batch |  2579 of 3770 loss ->  0.4049144685268402 acc -> 0.89\n",
            "Batch |  2580 of 3770 loss ->  0.37084001302719116 acc -> 0.89\n",
            "Batch |  2581 of 3770 loss ->  0.3165237605571747 acc -> 0.89\n",
            "Batch |  2582 of 3770 loss ->  0.37522315979003906 acc -> 0.89\n",
            "Batch |  2583 of 3770 loss ->  0.43746280670166016 acc -> 0.89\n",
            "Batch |  2584 of 3770 loss ->  0.4437364339828491 acc -> 0.89\n",
            "Batch |  2585 of 3770 loss ->  0.3761206269264221 acc -> 0.89\n",
            "Batch |  2586 of 3770 loss ->  0.3570941090583801 acc -> 0.89\n",
            "Batch |  2587 of 3770 loss ->  0.3670024871826172 acc -> 0.89\n",
            "Batch |  2588 of 3770 loss ->  0.38240164518356323 acc -> 0.89\n",
            "Batch |  2589 of 3770 loss ->  0.3243308365345001 acc -> 0.89\n",
            "Batch |  2590 of 3770 loss ->  0.3770866394042969 acc -> 0.89\n",
            "Batch |  2591 of 3770 loss ->  0.3704296350479126 acc -> 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmgGvUiTU0f5"
      },
      "source": [
        "target = torch.empty(3, dtype=torch.long).random_(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uucrh1H1xxm"
      },
      "source": [
        "target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M6j_dkB1ydV"
      },
      "source": [
        "semmb = semmb.to(device)\n",
        "prob = model(semmb, semmb) \n",
        "prob.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm-efRRk2kaT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}